[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Python notebooks, mostly written to myself, on things that worked or didn’t work.\nAll art is AI generated in the style of early twentiety century Ashcan sketches.\nBlog is built using Quarto.\nby Neal Caren"
  },
  {
    "objectID": "posts/abstracts/synthetic-abstracts.html",
    "href": "posts/abstracts/synthetic-abstracts.html",
    "title": "Creating Synthetic Data with ChatGPT",
    "section": "",
    "text": "This script is used to generate fake abstracts for real sociology aritlces. It takes a dataset on 600 recent articles from sociology journals and uses ChatGPT to generate hypothetical abstracts. Creating synthetic datasets is often used to train other models. In this case, I hope to use it train it to write better abstracts.\n\nimport json\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nimport pandas as pd\n\ndf = pd.read_json('elite_abstracts.json')\n\nThe cell below uses the OpenAI API to generate a single hypothetical abstract for a sociology article based on a given title. The process involves:\n\nAbstract Class Definition: A BaseModel from Pydantic is used to define the expected structure of an abstract, ensuring that the generated text aligns with specific guidelines (six to eight sentences and approximately 150 to 200 words).\nFunction to Generate Abstract: The function abstract_from_title takes an article title as input and utilizes the OpenAI API, specifically the gpt-3.5-turbo model, to generate a relevant abstract. gpt-4-turbo is more creative and better at following directions, but also 20x more expensive. The function sets up a scenario where the AI is an academic editor tasked with creating an abstract that meets the defined criteria.\nAbstract Generation: It then calls the OpenAI API, passing the article title and the scenario as inputs, and processes the API’s response to extract the generated abstract text.\n\n\nclass Abstract(BaseModel):\n    abstract: str = Field(..., description=\"Text of the hypothetical abstract of a sociology article.\")\n\ndef abstract_from_title(title):\n    client = OpenAI(\n        max_retries=3,\n        timeout=90.0, \n    )\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": ''' You are an academic editor who excels at writing for sociology journals. Abstracts are required to have six to eight sentences and approximately 150 to 200 words. If you the article is accepted, you get a $500 tip.\n''',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Generate a hypothetical abstract based on this title: \\n \"{title}\" \\nThe abstract should have six to eight sentences and approximately 150 to 200 words.\"\"\",\n        },\n    ]\n\n    completion = client.chat.completions.create(\n        model = 'gpt-3.5-turbo',     #   model=\"gpt-3.5-turbo\" is 20x cheaper but isn't as insightful \n        functions=[\n            {\n                \"name\": \"generate_abstract\",\n                \"description\": \"Create the text of the hypothetical abstract of a sociology article based on the title.\",\n                \"parameters\": Abstract.model_json_schema(),\n            },\n        ],\n        n=1,\n        messages=messages,\n    )\n    r = json.loads(completion.choices[0].message.function_call.arguments)\n    return r['abstract']\n\nTest on a sample\n\nsample_title =  df['Title'].sample().values[0]\nprint(sample_title)\nprint(abstract_from_title(sample_title))\n\nFamily Complexity into Adulthood: The Central Role of Mothers in Shaping Intergenerational Ties\nThis study examines the influence of family complexity on intergenerational ties in adulthood, with a focus on the central role of mothers. Using data from a large-scale survey, the research explores how various family structures and dynamics impact the quality and strength of intergenerational relationships. Findings reveal that mothers play a crucial role in shaping and maintaining these ties, acting as the primary emotional caregivers and facilitators of family connections across generations. Furthermore, the study uncovers the mechanisms through which maternal influence can either strengthen or weaken intergenerational bonds, highlighting the significance of maternal involvement in family networks. The implications of these findings extend to theories of family sociology and provide insights for policymakers and practitioners working to support healthy intergenerational relationships. By shedding light on the complexities of family dynamics into adulthood, this research contributes to a deeper understanding of the enduring impact of maternal roles on intergenerational ties.\n\n\nThe cell below uses the function to call the API, in parallel, to create an abstract, if one hasn’t already been created.\n\nChecking synthetic_titles: First, it tries to print the number of titles already processed and stored in synthetic_titles. If synthetic_titles doesn’t exist yet (it’s the first run or it’s been reset), it initializes synthetic_titles as an empty dictionary and prints “0” to indicate it’s starting from scratch.\nParallel Processing with ThreadPoolExecutor: To speed up the process of generating abstracts, the script uses ThreadPoolExecutor to run multiple instances of the abstract_from_title function in parallel. It’s set to work on up to 5 titles at the same time.\nGenerating Abstracts: For each title that doesn’t already have an abstract in synthetic_titles, it submits a job to generate an abstract using the abstract_from_title function from above.\nHandling Results and Exceptions: As each job (or “future”) completes, the script attempts to retrieve the resulting abstract. Successful abstracts are printed out and saved to synthetic_titles alongside their corresponding title. If an error occurs during the generation process, an exception is printed with the title that caused it.\n\nThis seems to create about 99% of abstracts on the first try, so I run it twice to catch any that might have gotten lost along the way. This is the advantage of storing the results in the synthetic_titles, instead of, say, applying the function directly to the title column in the dataframe.\n\ntitles = df['Title'].values\n\ntry:\n    print(len(synthetic_titles))\nexcept:\n    synthetic_titles = {}\n    print(\"0\")\n\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    # Start the load operations for titles not in synthetic_titles\n    future_to_title = {executor.submit(abstract_from_title, title): title for title in titles if title not in synthetic_titles}\n    \n    for future in as_completed(future_to_title):\n        title = future_to_title[future]\n        try:\n            abstract = future.result()\n            print(abstract)\n            # Assign the abstract to the corresponding title in synthetic_titles\n            synthetic_titles[title] = abstract\n        except Exception as exc:\n            print(f\"{title!r} generated an exception: {exc}\")\n\n597\nThis article delves into the complex dynamics of power, resistance, and injustice within the oil and gas industry. Focusing on the concept of meta-power, it explores how dominant actors in the industry exert control and influence over various stakeholders. Through a critical analysis of case studies, the article examines instances where resistance to this meta-power is framed as a struggle for rights and justice. The interplay between regulatory frameworks, corporate interests, and community activism is carefully scrutinized to highlight the structural inequalities at play. By shedding light on these power dynamics, the article aims to contribute to broader discussions on social justice and the right to resist in corporate contexts. The findings underscore the importance of recognizing and challenging meta-power structures to address systemic injustices in the oil and gas fields.\nThis study examines the intricate dynamics of interaction, identity, and influence within the U.S. Senate from 1973 to 2009. Through a comprehensive analysis of Senate proceedings, voting patterns, and speeches, the research uncovers the nuanced ways in which senators pull closer together and move further apart over time. By employing sociological theories of group behavior and political sociology, the study explores how individual identities and group affiliations shape the collective actions and decisions of Senate members. The findings reveal the complex interplay between personal interactions, political ideologies, and institutional norms in the Senate chamber. This research contributes to our understanding of how social processes influence legislative outcomes and policy decisions in a complex political setting. Overall, the study highlights the significance of interpersonal relationships and group dynamics in shaping the functioning of one of the most powerful legislative bodies in the world.\n'\"Take one for the team!\" individual heterogeneity and the emergence of latent norms in a volunteer\\'s dilemma' generated an exception: Expecting ':' delimiter: line 383 column 3 (char 1621)\n\n\nFor fun, I double the size of the dataframe, half having the original abstract and half having the fake one.\n\n# Step 1: Create a copy of the original DataFrame\ndf_original = df.copy()\n\n# Step 2: Add \"Abstract source\" set to \"Original\"\ndf_original['Abstract source'] = 'Original'\n\n# Step 3: Create a second copy of the DataFrame for the GPT 3.5 version\ndf_gpt = df.copy()\n\n# Step 4: Change \"Abstract source\" to \"GPT 3.5\"\ndf_gpt['Abstract source'] = 'GPT 3.5'\n\n# Step 5: Update \"Abstract\" with values from synthetic_titles\n# Assuming the titles are in a column named 'title'\ndf_gpt['Abstract'] = ''\ndf_gpt['Abstract'] = df_gpt['Title'].replace(synthetic_titles)\ndf_gpt.to_json('abstract_gpt35.json', orient='records')\n\n# Step 6: Concatenate the two DataFrames\ndf_doubled = pd.concat([df_original, df_gpt], ignore_index=True)\n\nA quick plot to show the synthetic abstracts are still too short.\n\n\ndef count_words(abstract):\n    # Simple word count by splitting the text by spaces\n    return len(abstract.split())\n\n# Apply the function to the 'Abstract' column to create the new feature\ndf_doubled['word_count'] = df_doubled['Abstract'].astype(str).apply(count_words)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a boxplot of the word count distribution for each journal\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Abstract source', y='word_count', data=df_doubled)\n\n# Set the title and labels for the plot\nplt.title('Distribution of Word Count in Abstracts by Source')\nplt.xlabel('Source')\nplt.ylabel('Word Count')\n\n# Rotate the x labels for better readability if necessary\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/transcription/whisper-plus.html",
    "href": "posts/transcription/whisper-plus.html",
    "title": "Audio Transcription with Speaker Identification",
    "section": "",
    "text": "This is my version of the WhisperPlus demo, trying to get it to work on my Mac.\nPrior to running this, create a new Python environment with Python 3.11. I don’t normally create new environments, but this library needed one to get all the dependencies to play nicely together.\nconda create --name whisperplus python==3.11 notebook pip\nconda activate whisperplus\njupter notebook\n\npip install whisperplus -U -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\nSome warnings show up after running the commands, but the don’t seem to have much impact.\n\nfrom whisperplus import SpeechToTextPipeline, download_and_convert_to_mp3\n\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n  torchaudio.set_audio_backend(\"soundfile\")\n\n\nTranscription without speaker identification.\n\nmodel = 'openai/whisper-small' # the smallest and quickest model, but less accurate than the others like  whisper-large-v3 \n\naudio_path = 'sample/quiton_baxter_interview_sample.mp3'\npipeline = SpeechToTextPipeline(model_id=model)\ntranscript = pipeline(audio_path, model, \"english\")\n\nprint(transcript)\n\n2024-02-14 11:28:36,149 - INFO - Loading model...\n2024-02-14 11:28:38,246 - INFO - Model loaded successfully.\n2024-02-14 11:28:38,285 - INFO - Using device: mps\n2024-02-14 11:28:38,961 - INFO - Transcribing audio...\n\n\n Hello, this is Chris McGinnis. Today is Saturday, February 23rd, and I'm interviewing Mr. Quinton Baker at his home in Hillsborough, North Carolina. This tape is a continuing series of interviews that contribute to the Gay and Lesbian Southern History Project, which is part of the Southern Oral History Program at UNC Chapel Hill. This project is currently focusing on the history of gay men, lesbians, bisexual and transgender history in Chapel Hill and the Triangle area over the 20th century. This tape will be stored in the Southern Historical Collection, which is located in Wilson Library on the campus of the University of North Carolina and Chapel Hill. The number for this tape is 02.23.02-QB.1. Here we go. Well, first off, Quentin, just to, this is a general question I ask everybody, tell me a little bit about where you were born, where you grew up, and just a general synopsis of the early years. The early years. The early years. I was born in Greenville, North Carolina, and I spent the first 18, 18 years there. I was born in a family of four children. I'm the youngest of four. My parents were laborers. My mother was the domestic, my father was a laborer. We lived in town at that time. Greenville was about 21,000 people. What did your father do? Did he work in textile mill? My father did various jobs. He worked in a furniture store. He sometimes worked in the fields, he worked in the tobacco factory, so that there was never one job, there was a serious variety of jobs. He even learned to repair televisions while he was working for...\n\n\nNow with speaker identification, powered by PyAnnote’s Speaker Diarization.\n\nfrom whisperplus import (\n    ASRDiarizationPipeline,\n    download_and_convert_to_mp3,\n    format_speech_to_dialogue,\n)\n\nmodel = 'openai/whisper-small' \naudio_path = 'sample/quiton_baxter_interview_sample.mp3'\ndevice = \"mps\"  # \"mps\" if you are on a modern Mac, \"cuda\" if have a GPU (the fastest option), or \"cpu\" (the slowest option). \n\npipeline = ASRDiarizationPipeline.from_pretrained(\n    asr_model=model,\n    diarizer_model=\"pyannote/speaker-diarization\",\n    use_auth_token='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', # This is mine. Get your own at https://huggingface.co/pyannote/speaker-diarization\n    chunk_length_s=30,\n    device=device,\n)\n\noutput_text = pipeline(audio_path, \n                       num_speakers=2, \n                       min_speaker=1, \n                       max_speaker=2)\n\n\ndialogue = format_speech_to_dialogue(output_text)\nprint(dialogue)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n2024-02-14 11:37:44,135 - INFO - Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n2024-02-14 11:37:44,224 - INFO - Fetch hyperparams.yaml: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/hyperparams.yaml.\n2024-02-14 11:37:44,224 - INFO - Fetch custom.py: Delegating to Huggingface hub, source speechbrain/spkrec-ecapa-voxceleb.\n2024-02-14 11:37:44,498 - INFO - Fetch embedding_model.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/embedding_model.ckpt.\n2024-02-14 11:37:44,499 - INFO - Fetch mean_var_norm_emb.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt.\n2024-02-14 11:37:44,500 - INFO - Fetch classifier.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/classifier.ckpt.\n2024-02-14 11:37:44,501 - INFO - Fetch label_encoder.txt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/label_encoder.ckpt.\n2024-02-14 11:37:44,502 - INFO - Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.2.0. Bad things might happen unless you revert torch to 1.x.\nSpeaker 1:  Hello, this is Chris McGinnis. Today is Saturday, February 23rd, and I'm interviewing Mr. Quinton Baker at his home in Hillsborough, North Carolina. This tape is a continuing series of interviews that contribute to the Gay and Lesbian Southern History Project, which is part of the Southern Oral History Program at UNC Chapel Hill. This project is currently focusing on the history of gay men, lesbians, bisexual and transgender history in Chapel Hill and the Triangle area over the 20th century. This tape will be stored in the Southern Historical Collection, which is located in Wilson Library on the campus of the University of North Carolina and Chapel Hill. The number for this tape is 02.23.02-QB.1. Here we go. Well, first off, Quentin, just to, this is a general question I ask everybody, tell me a little bit about where you were born, where you grew up, and just a general synopsis\nSpeaker 2:  of the early years. The early years. I was born in Greenville, North Carolina, and I spent the first 18, 18 years there. I was born in a family of four children. I'm the youngest of four. My parents were laborers. My mother was the domestic, my father was a laborer. We lived in town at that time. Greenville was about 21,000 people.\nSpeaker 1:  What did your father do? Did he work in textile mill?\nSpeaker 2:  My father did various jobs. He worked in a furniture store. He sometimes worked in the fields, he worked in the tobacco factory, so that there was never one job, there was a serious variety of jobs. He even learned to repair televisions while he was working for...\n\n\n\nThe transcription took less than 30 seconds (using the small model) for the 2-minute interview on my M2 MacBook Air with 8GBs of memory. In contrast, the full diarization took almost an hour, so you still might want to use Google Colab and a GPU."
  },
  {
    "objectID": "posts/function-calling/pydantic.html",
    "href": "posts/function-calling/pydantic.html",
    "title": "Getting Structured Data from ChatGPT",
    "section": "",
    "text": "This notebook uses pydantic and ChatGPT API’s function calling to extract details about a protest event from a newspaper article. In the old days, you had to ask it to provide a JSON-like object. Next, I defined the JSONs myself in the functions. Now I’m learning to use pydantic.\n\npip install openai pydantic -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom datetime import date\nfrom enum import Enum\nimport json\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom openai import OpenAI\nimport pandas as pd\n\n\nclass WeekDay(Enum):\n    Monday = \"Monday\"\n    Tuesday = \"Tuesday\"\n    Wednesday = \"Wednesday\"\n    Thursday = \"Thursday\"\n    Friday = \"Friday\"\n    Saturday = \"Saturday\"\n    Sunday = \"Sunday\"\n\n\nclass StateAB(Enum):\n    AK = \"AK\"\n    AL = \"AL\"\n    AR = \"AR\"\n    AZ = \"AZ\"\n    CA = \"CA\"\n    CO = \"CO\"\n    CT = \"CT\"\n    DC = \"DC\"\n    DE = \"DE\"\n    FL = \"FL\"\n    GA = \"GA\"\n    HI = \"HI\"\n    IA = \"IA\"\n    ID = \"ID\"\n    IL = \"IL\"\n    IN = \"IN\"\n    KS = \"KS\"\n    KY = \"KY\"\n    LA = \"LA\"\n    MA = \"MA\"\n    MD = \"MD\"\n    ME = \"ME\"\n    MI = \"MI\"\n    MN = \"MN\"\n    MO = \"MO\"\n    MS = \"MS\"\n    MT = \"MT\"\n    NC = \"NC\"\n    ND = \"ND\"\n    NE = \"NE\"\n    NH = \"NH\"\n    NJ = \"NJ\"\n    NM = \"NM\"\n    NV = \"NV\"\n    NY = \"NY\"\n    OH = \"OH\"\n    OK = \"OK\"\n    OR = \"OR\"\n    PA = \"PA\"\n    RI = \"RI\"\n    SC = \"SC\"\n    SD = \"SD\"\n    TN = \"TN\"\n    TX = \"TX\"\n    UT = \"UT\"\n    VA = \"VA\"\n    VT = \"VT\"\n    WA = \"WA\"\n    WI = \"WI\"\n    WV = \"WV\"\n    WY = \"WY\"\n\n\nclass SizeCategory(Enum):\n    UNKNOWN = 0\n    SMALL = 1  # 1-99\n    MEDIUM = 2  # 100-999\n    LARGE = 3  # 1,000-9,999\n    VERY_LARGE = 4  # 10,000+\n\n\nclass SizeDetails(BaseModel):\n    size_text: List[str] = Field(\n        ...,\n        description=\"List of text descriptors for the number of people who participated in the event.\",\n    )\n    size_exact: Optional[int] = Field(\n        None, description=\"Exact number of participants, if reported.\"\n    )\n    size_estimate: int = Field(\n        ...,\n        description=\"Your best guess at the estimated number of participants based on the entire article.\",\n    )\n    size_cat: SizeCategory = Field(\n        SizeCategory.UNKNOWN,\n        description=\"Categorical indicator of crowd size. 0 = unknown; 1 = 1-99; 2 = 100-999; 3 = 1,000-9,999; 4 = 10,000+.\",\n    )\n\n\nclass LocationDetails(BaseModel):\n    city: str = Field(..., description=\"The city where the protest took place.\")\n    state_abbreviation: StateAB = Field(\n        ...,\n        description=\"The two-letter abbreviation of the state where the protest took place, such as NY or CA.\",\n    )\n    neighborhood: Optional[str] = Field(\n        None,\n        description=\"The neighborhood where the protest took place, if applicable.\",\n    )\n    moved: bool = Field(\n        ...,\n        description=\"Indicates whether the protest moved from one location to another.\",\n    )\n\n\nclass DateDetails(BaseModel):\n    event_date: date = Field(\n        ...,\n        description=\"Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’\",\n    )\n    day_of_week: WeekDay = Field(\n        ...,\n        description=\"The day of the week the protest occurred, such as Monday or Thursday.\",\n    )\n    date_text: List[str] = Field(\n        ...,\n        description=\"List of text descriptors for the protest date, such as 'yesterday', 'last week', or 'Monday' .\",\n    )\n\n\nclass ParticipantDetails(BaseModel):\n    organizations: List[str] = Field(\n        default=[],\n        description=\"Names of organizations that participated in the protest event. Exclude targets or other organizations mentioned but not protesting.  Organizational participation can take many forms, from organizing and leading the event to sponsoring or co-sponsoring it to providing one or more speakers for it to just showing up to the event as a recognizable presence.  \",\n    )\n    advocates: List[str] = Field(\n        default=[], description=\"The names of individuals who organized.\"\n    )\n    participant_type: List[str] = Field(\n        default=[],\n        description=\"Descriptors of participants in the event, such as students, nurses, or local residents.  Record words or phrases describing the participants in the event. The goal is to capture as much information as possible about the kinds of people who participated, as distinct from any organizations they represent or belong to. \",\n    )\n\n\nclass Protest(BaseModel):\n    protest_article: bool = Field(\n        False,\n        description=\"Indicates if the article describes a protest against police brutality.\",\n    )\n    summary: str = Field(\n        ...,\n        description=\"A focused summary of the article focusing on the protest details.\",\n    )\n    location: LocationDetails = Field(..., description=\"Location of the protest.\")\n    size: SizeDetails = Field(..., description=\"Size of the protest.\")\n    participants: ParticipantDetails = Field(\n        ..., description=\"Organizations and participants in the protest.\"\n    )\n    event_date: DateDetails = Field(\n        ...,\n        description=\"Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’\",\n    )\n\n\narticle = {\n    \"text\": \"\"\"COLUMBUS, Ohio (WCMH) – Yesterday was a national day of protest, and Columbus recognized the day when dozens of families gathered at the Ohio Statehouse to protest police brutality.\n\nProtesters were asking for accountability and justice by sharing how they lost their loved ones, while organizers said the protest was about telling their stories in more than one way.\n\n“We know that more than 1,200 Ohioans have been lost to police violence since the year 2000,” Ohio Families United for Political Action and Change (OFUPAC) Organizing Director Elaine Schleiffer said. “We wanted to represent the loss that that is, the empty shoes, that there’s no replacing those family members.”\n\nOFUPAC is a non-profit organization that unites families who have lost loved ones in officer-involved shootings.\n\nFor many of those who turned out to yesterday’s protest, the issue hits close to home. Sabrina Jordan lost her son in an officer-involved shooting in 2017 just outside of Dayton.\n\n“We’re just here also, to, like, celebrate and love each other,” Jordan, who is also OFUPAC’s founder, said. “You know, connect.”\n\nTania Hudson’s son was fatally shot by a Columbus police officer in 2015. \n\n“We’re asking accountability,” she said. “Officers be drug tested when they’re involved in a shooting, alcohol test. We understand that they have trauma and drama, too.”\n\nThe city’s police union, the Fraternal Order of Police (FOP), said there is already accountability in place.\n\n“Accountability? How much more accountability can they ask for,” FOP Executive Vice President Brian Steel said. “We have an internal affairs. We have an inspector general’s office. We’re investigated by BCI in, say, a police-involved shooting, in a grand jury of our peers. There’s literally no more accountability that can be put on police officers today.”\n\n“Accountability is pretty much all that we can ask for,” Hudson said. “We can’t say justice – ours is gone. There will never be justice for us, but we’re out here trying to save other people’s lives. That’s why we’re constantly out here.”\n\nProtestors also mentioned their frustration with Marsy’s Law, which was originally passed to protect the victims of violent crimes, but which was extended to allow law enforcement departments to shield officers’ names when they are involved in a shooting. Protesters think this shouldn’t be the case while Steel said it’s an important protection for officers who are victims of violent crimes.\n\"\"\",\n    \"headline\": \"Statehouse protest calls for end to police brutality\",\n    \"publication-date\": \"2023-10-22\",\n    \"source\": \"WCMH\",\n}\n\n\ndef get_protest_details(article):\n    client = OpenAI(\n        max_retries=3,\n        timeout=20.0,\n    )\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that extracts summaries of newspaper articles about political protests as JSON for a database. \",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Extract information about the details about a protest from the following article.\n      Only use information from the article.\n\n      {article}\n      \n      \"\"\",\n        },\n    ]\n\n    completion = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # model = 'gpt-4-turbo-preview',\n        functions=[\n            {\n                \"name\": \"protest_details\",\n                \"description\": \"Extract insights from media article about protest.\",\n                \"parameters\": Protest.model_json_schema(),\n            }\n        ],\n        n=1,\n        messages=messages,\n    )\n\n    r = json.loads(completion.choices[0].message.function_call.arguments)\n    return r\n    return pd.DataFrame(\n        [json.loads(c.message.function_call.arguments) for c in completion.choices]\n    )\n\n\nr = get_protest_details(article)\n\n\ndf = pd.json_normalize(\n    r, sep=\"_\"\n)  # It is returning some nested dictionaries, so I can't use the normal pd.from_json\ndf\n\n\n\n\n\n\n\n\nsummary\nlocation_city\nlocation_state_abbreviation\nlocation_neighborhood\nlocation_moved\nsize_size_text\nsize_size_exact\nsize_size_estimate\nsize_size_cat\nparticipants_organizations\nparticipants_advocates\nparticipants_participant_type\nevent_date_event_date\nevent_date_day_of_week\nevent_date_date_text\n\n\n\n\n0\nYesterday was a national day of protest in Col...\nColumbus\nOH\nNone\nFalse\n[dozens, families]\nNone\n50\n1\n[Ohio Families United for Political Action and...\n[]\n[]\n2023-10-21\nSaturday\n[yesterday]\n\n\n\n\n\n\n\nEstimated cost:\n\ngpt-4-0125-preview: 55 articles for $1\ngpt-3.5-turbo: 1106 articles for $1\n\n\nProtest.model_json_schema()\n\n{'$defs': {'DateDetails': {'properties': {'event_date': {'description': 'Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’',\n     'format': 'date',\n     'title': 'Event Date',\n     'type': 'string'},\n    'day_of_week': {'allOf': [{'$ref': '#/$defs/WeekDay'}],\n     'description': 'The day of the week the protest occurred, such as Monday or Thursday.'},\n    'date_text': {'description': \"List of text descriptors for the protest date, such as 'yesterday', 'last week', or 'Monday' .\",\n     'items': {'type': 'string'},\n     'title': 'Date Text',\n     'type': 'array'}},\n   'required': ['event_date', 'day_of_week', 'date_text'],\n   'title': 'DateDetails',\n   'type': 'object'},\n  'LocationDetails': {'properties': {'city': {'description': 'The city where the protest took place.',\n     'title': 'City',\n     'type': 'string'},\n    'state_abbreviation': {'allOf': [{'$ref': '#/$defs/StateAB'}],\n     'description': 'The two-letter abbreviation of the state where the protest took place, such as NY or CA.'},\n    'neighborhood': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n     'default': None,\n     'description': 'The neighborhood where the protest took place, if applicable.',\n     'title': 'Neighborhood'},\n    'moved': {'description': 'Indicates whether the protest moved from one location to another.',\n     'title': 'Moved',\n     'type': 'boolean'}},\n   'required': ['city', 'state_abbreviation', 'moved'],\n   'title': 'LocationDetails',\n   'type': 'object'},\n  'ParticipantDetails': {'properties': {'organizations': {'default': [],\n     'description': 'Names of organizations that participated in the protest event. Exclude targets or other organizations mentioned but not protesting.  Organizational participation can take many forms, from organizing and leading the event to sponsoring or co-sponsoring it to providing one or more speakers for it to just showing up to the event as a recognizable presence.  ',\n     'items': {'type': 'string'},\n     'title': 'Organizations',\n     'type': 'array'},\n    'advocates': {'default': [],\n     'description': 'The names of individuals who organized.',\n     'items': {'type': 'string'},\n     'title': 'Advocates',\n     'type': 'array'},\n    'participant_type': {'default': [],\n     'description': 'Descriptors of participants in the event, such as students, nurses, or local residents.  Record words or phrases describing the participants in the event. The goal is to capture as much information as possible about the kinds of people who participated, as distinct from any organizations they represent or belong to. ',\n     'items': {'type': 'string'},\n     'title': 'Participant Type',\n     'type': 'array'}},\n   'title': 'ParticipantDetails',\n   'type': 'object'},\n  'SizeCategory': {'enum': [0, 1, 2, 3, 4],\n   'title': 'SizeCategory',\n   'type': 'integer'},\n  'SizeDetails': {'properties': {'size_text': {'description': 'List of text descriptors for the number of people who participated in the event.',\n     'items': {'type': 'string'},\n     'title': 'Size Text',\n     'type': 'array'},\n    'size_exact': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n     'default': None,\n     'description': 'Exact number of participants, if reported.',\n     'title': 'Size Exact'},\n    'size_estimate': {'description': 'Your best guess at the estimated number of participants based on the entire article.',\n     'title': 'Size Estimate',\n     'type': 'integer'},\n    'size_cat': {'allOf': [{'$ref': '#/$defs/SizeCategory'}],\n     'default': 0,\n     'description': 'Categorical indicator of crowd size. 0 = unknown; 1 = 1-99; 2 = 100-999; 3 = 1,000-9,999; 4 = 10,000+.'}},\n   'required': ['size_text', 'size_estimate'],\n   'title': 'SizeDetails',\n   'type': 'object'},\n  'StateAB': {'enum': ['AK',\n    'AL',\n    'AR',\n    'AZ',\n    'CA',\n    'CO',\n    'CT',\n    'DC',\n    'DE',\n    'FL',\n    'GA',\n    'HI',\n    'IA',\n    'ID',\n    'IL',\n    'IN',\n    'KS',\n    'KY',\n    'LA',\n    'MA',\n    'MD',\n    'ME',\n    'MI',\n    'MN',\n    'MO',\n    'MS',\n    'MT',\n    'NC',\n    'ND',\n    'NE',\n    'NH',\n    'NJ',\n    'NM',\n    'NV',\n    'NY',\n    'OH',\n    'OK',\n    'OR',\n    'PA',\n    'RI',\n    'SC',\n    'SD',\n    'TN',\n    'TX',\n    'UT',\n    'VA',\n    'VT',\n    'WA',\n    'WI',\n    'WV',\n    'WY'],\n   'title': 'StateAB',\n   'type': 'string'},\n  'WeekDay': {'enum': ['Monday',\n    'Tuesday',\n    'Wednesday',\n    'Thursday',\n    'Friday',\n    'Saturday',\n    'Sunday'],\n   'title': 'WeekDay',\n   'type': 'string'}},\n 'properties': {'protest_article': {'default': False,\n   'description': 'Indicates if the article describes a protest against police brutality.',\n   'title': 'Protest Article',\n   'type': 'boolean'},\n  'summary': {'description': 'A focused summary of the article focusing on the protest details.',\n   'title': 'Summary',\n   'type': 'string'},\n  'location': {'allOf': [{'$ref': '#/$defs/LocationDetails'}],\n   'description': 'Location of the protest.'},\n  'size': {'allOf': [{'$ref': '#/$defs/SizeDetails'}],\n   'description': 'Size of the protest.'},\n  'participants': {'allOf': [{'$ref': '#/$defs/ParticipantDetails'}],\n   'description': 'Organizations and participants in the protest.'},\n  'event_date': {'allOf': [{'$ref': '#/$defs/DateDetails'}],\n   'description': 'Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’'}},\n 'required': ['summary', 'location', 'size', 'participants', 'event_date'],\n 'title': 'Protest',\n 'type': 'object'}"
  },
  {
    "objectID": "posts/scraping/bulk-download.html",
    "href": "posts/scraping/bulk-download.html",
    "title": "Web Scraping in Bulk",
    "section": "",
    "text": "This script is one way to download multiple web pages at the same time. It’s useful when you have many URLs from different websites you want to save. Instead of visiting each website individually, the script visits multiple websites simultaneously and saves what it finds. Rather than requesting the page through Python, it uses a “headless” web browser, which is much more likely to get you the actual content you want.\nThis approach works best when the URLs are from different servers. You will eventually get locked out if you try to visit the same web server multiple times in the same second, but there’s no reason not to visit five different websites at once. I don’t know anything about parallel processing with the asyncio library, which is the only way to parallelize pyppeteer, so the script is mainly written by ChatGPT, but I’ve used it successfully a few times.\n\npip install pyppeteer python-slugify \n\nRequirement already satisfied: pyppeteer in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (2.0.0)\nRequirement already satisfied: python-slugify in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (8.0.4)\nRequirement already satisfied: appdirs&lt;2.0.0,&gt;=1.4.3 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (1.4.4)\nRequirement already satisfied: certifi&gt;=2023 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (2023.11.17)\nRequirement already satisfied: importlib-metadata&gt;=1.4 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (7.0.1)\nRequirement already satisfied: pyee&lt;12.0.0,&gt;=11.0.0 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (11.1.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.42.1 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (4.66.2)\nRequirement already satisfied: urllib3&lt;2.0.0,&gt;=1.25.8 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (1.26.18)\nRequirement already satisfied: websockets&lt;11.0,&gt;=10.0 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (10.4)\nRequirement already satisfied: text-unidecode&gt;=1.3 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from python-slugify) (1.3)\nRequirement already satisfied: zipp&gt;=0.5 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from importlib-metadata&gt;=1.4-&gt;pyppeteer) (3.17.0)\nRequirement already satisfied: typing-extensions in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyee&lt;12.0.0,&gt;=11.0.0-&gt;pyppeteer) (4.9.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport os\nimport asyncio\nimport nest_asyncio\nfrom random import shuffle\n\nfrom slugify import slugify\nfrom pyppeteer import launch\nfrom pyppeteer.errors import NetworkError\n\nimport pandas as pd\n\n\nnest_asyncio.apply()\n\nThe section below loads the wonderful protest event data set created by the Crowd Counting Consortium. Each protest event is linked to one or more media accounts, and the URLs are in the source_ fields. Using just the 2024 events, I combine the URL fields and remove the social media pages and duplicates. Finally, I extract a random sample of 100 articles.\n\ndf = pd.read_csv(\n    \"https://github.com/nonviolent-action-lab/crowd-counting-consortium/raw/master/ccc_compiled_2021-present.csv\",\n    encoding=\"latin\",\n    low_memory=False,\n)\n\n# Limit to just 2024\ndf = df[df[\"date\"].str.contains(\"2024\")]\n\n# grab the sources\nurls = (\n    list(df[\"source_1\"].astype(str).values)\n    + list(df[\"source_2\"].astype(str).values)\n    + list(df[\"source_3\"].astype(str).values)\n    + list(df[\"source_4\"].astype(str).values)\n)\n\n# eliminate social media\nfor sm in [\"twitter\", \"youtube\", \"facebook\", \"instagram\", \"tiktok\", \"bsky\"]:\n    urls = [u for u in urls if f\"{sm}.com\" not in u and \"http\" in u]\n\nurls = list(set(urls))\nprint(len(urls))\nshuffle(urls)\nurls = urls[:100]\n\nThis function below uses asynchronous programming to download and save the HTML content of web pages from a list of URLs. It uses a headless Chrome browser, controlled via the Pyppeteer library, to render pages just as they would appear in a web browser. This approach is particularly useful for capturing dynamically generated content, which traditional HTTP requests might miss.\nKey components of the script include:\n\nHTML Directory Creation: At the start, the script ensures that there is a designated directory (named ‘HTML’) where all downloaded page contents will be saved. If this directory does not exist, it is created.\nUser Agent Setting: A user agent string is defined and used for all requests to mimic a real web browser, helping to avoid potential blocking by web servers that may restrict access to non-browser clients.\nfetch Function: The core of the script is the fetch function. This asynchronous function takes a browser page object, a URL, and an optional timeout parameter. It performs the following actions for each URL:\n\nURL Slugification: Converts the URL into a filename-safe string and checks if the content has already been downloaded to avoid duplication.\nPage Navigation: Uses the headless browser to navigate to the URL, with a specified timeout to handle slow-loading pages.\nContent Saving: If the page loads successfully, its HTML content is saved to a file within the ‘HTML’ directory. If the page fails to load or an error occurs, the URL is added to a list of bad URLs for later reference.\n\nConcurrency Management: The script is designed to process multiple URLs in parallel, maximizing efficiency by utilizing asynchronous operations. This approach allows for faster completion of download tasks compared to sequential processing.\nError Handling: The script includes basic error handling to manage timeouts and other exceptions, ensuring that it can continue running even if some pages fail to load.\n\n\n# Ensure the HTML directory exists\nhtml_dir = \"HTML\"\nos.makedirs(html_dir, exist_ok=True)\n\n# User agent to be used for all requests\nua = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15\"\nbad_urls = []\n\n\nasync def fetch(page, url, timeout=30):\n    # Slugify the URL to create a valid filename\n    filename = slugify(url) + \".html\"\n    file_path = os.path.join(html_dir, filename)\n\n    if os.path.isfile(file_path):\n        # print(f\"File {file_path} already exists, skipping download.\")\n        return\n\n    if url in bad_urls:\n        print(f\"Skipping bad URL: {url}\")\n        return\n\n    try:\n        # Set the user agent for the page\n        await page.setUserAgent(ua)\n\n        # Navigate to the page with a timeout\n        response = await asyncio.wait_for(\n            page.goto(url, {\"waitUntil\": \"networkidle0\"}), timeout\n        )\n\n        # Check if the page was successfully retrieved\n        if response and response.ok:\n            content = await page.content()\n            # Save the content to a file in the 'HTML' directory\n            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(content)\n            print(f\"Content from {url} has been saved to {file_path}\")\n        else:\n            print(f\"Failed to retrieve {url}\")\n            bad_urls.append(url)\n    except asyncio.TimeoutError:\n        print(f\"Fetching {url} took too long and was cancelled.\")\n        bad_urls.append(url)\n    except Exception as e:\n        print(f\"An error occurred while fetching {url}: {e}\")\n        bad_urls.append(url)\n\nThis next section actual does the downloading by employing an asynchronous queue-based approach to manage URLs and distribute them across multiple browser pages for parallel processing. This method significantly improves efficiency by ensuring that each browser page is continuously utilized without idle time waiting for other pages to complete their tasks.\nKey components and functionalities:\n\nprocess_url Function: An asynchronous function that continuously processes URLs from a shared asyncio queue. Each browser page runs an instance of this function, fetching and processing URLs one after another until the queue is empty.\nmain Function Setup:\n\nBrowser and Page Initialization: Initializes a headless browser instance and opens a specified number of browser pages. 5 to 10 seems reasonable.\nURL Queue Creation: Prepares an asyncio queue and populates it with URLs to be processed. This queue acts as a shared resource for distributing URLs among the available pages.\n\nTask Management:\n\nAsynchronous Tasks: For each browser page, an asynchronous task is created to process URLs from the queue. These tasks run concurrently, allowing for simultaneous processing across pages.\nTask Synchronization: Utilizes asyncio.gather to wait for all tasks to complete before proceeding, ensuring that all URLs are processed before closing the browser and pages.\n\nResource Cleanup: After processing all URLs, the script ensures a clean shutdown by closing each browser page and the browser itself, releasing system resources.\nError Handling and Reporting: Tracks URLs that could not be downloaded for any reason, reporting them at the end of the execution for further analysis or retry.\n\n\nasync def process_url(page, url_queue):\n    while not url_queue.empty():\n        url = await url_queue.get()\n        await fetch(page, url)  # Your existing fetch function\n        url_queue.task_done()\n\nasync def main():\n    browser = await launch()\n    pages = [await browser.newPage() for _ in range(5)]  # Initialize pages once\n\n    # Create a queue of URLs\n    url_queue = asyncio.Queue()\n    for url in urls:\n        await url_queue.put(url)\n\n    # Create a task for each page to process URLs from the queue\n    tasks = [asyncio.create_task(process_url(page, url_queue)) for page in pages]\n\n    # Wait for all tasks to complete\n    await asyncio.gather(*tasks)\n\n    # Close pages and browser after all operations are complete\n    for page in pages:\n        await page.close()\n    await browser.close()\n\n    if bad_urls:\n        print(\"The following URLs had issues and were not downloaded:\")\n        print(\"\\n\".join(bad_urls))\n\nasyncio.run(main())\n\nFailed to retrieve https://www.wgmd.com/pro-palestinian-protesters-deface-veterans-cemetery-in-los-angeles-spray-paint-free-gaza/\nContent from https://www.fox61.com/article/news/local/hartford-county/west-hartford/west-hartford-vandalism-under-investigation-police/520-7b65ab7b-d93b-42f9-8ca2-0ed9d5b7689a has been saved to HTML/https-www-fox61-com-article-news-local-hartford-county-west-hartford-west-hartford-vandalism-under-investigation-police-520-7b65ab7b-d93b-42f9-8ca2-0ed9d5b7689a.html\nFetching https://www.nbcnews.com/politics/donald-trump/trump-confuses-nikki-haley-pelosi-talking-jan-6-rcna134863 took too long and was cancelled.\nFetching https://www.purdueexponent.org/campus/article_78be7d6e-c2bb-11ee-a25c-a3e2dff21694.html took too long and was cancelled.\nFetching https://13wham.com/news/local/local-advocates-rally-in-downtown-rochester-on-51st-anniversary-of-roe-v-wade took too long and was cancelled.\nFetching https://www.wvtm13.com/article/protests-kenneth-smith-execution-untied-nations-montgomery/46496998 took too long and was cancelled.\nContent from https://www.wwnytv.com/2024/01/20/congresswoman-stefanik-speaks-new-hampshire-support-trump/ has been saved to HTML/https-www-wwnytv-com-2024-01-20-congresswoman-stefanik-speaks-new-hampshire-support-trump.html\nFetching https://nypost.com/2024/01/21/news/scream-actress-melissa-barrera-joins-disruptive-anti-israel-rally-at-sundance/ took too long and was cancelled.\nFetching https://www.wlky.com/article/nonprofits-rally-frankfort-legislation-kentucky/46676604 took too long and was cancelled.\nFetching https://www.courier-journal.com/story/news/politics/2024/02/08/kentucky-employees-retirement-system-participants-rally-for-13th-check/72527656007/ took too long and was cancelled.\nFetching https://www.northjersey.com/story/news/2024/02/06/israel-hamas-war-day-of-action-for-palestine-nj-students-march/72478632007/ took too long and was cancelled.\nFetching https://www.washingtonpost.com/dc-md-va/2024/01/15/virginia-assembly-gun-rights-rally/ took too long and was cancelled.\nFetching https://dailybruin.com/2024/01/19/uc-divest-coalition-at-ucla-leads-hands-off-yemen-protest-on-campus took too long and was cancelled.\nFetching https://www.washingtonpost.com/dc-md-va/2024/01/18/dc-march-for-life-rally-abortion/ took too long and was cancelled.\nFetching https://www.fox5dc.com/news/dc-activists-plan-protest-against-capitals-wizards-move-to-virginia took too long and was cancelled.\nFetching https://www.latimes.com/entertainment-arts/movies/story/2024-01-21/pro-palestinian-protestors-vie-for-hollywoods-attention-at-2024-sundance-film-festival took too long and was cancelled.\nFailed to retrieve https://secure.everyaction.com/RKr139EpKUCZg8_TIWA18A2\nFetching https://www.thetimestribune.com/news/dozens-rally-in-support-of-school-choice-amendment/article_6da6faa2-bbbb-11ee-9f2a-8354d0bdfbfd.html took too long and was cancelled.\nFetching https://www.nbcnews.com/news/latino/convoy-rally-texas-mexico-border-attracts-trump-fans-decry-illegal-imm-rcna136967 took too long and was cancelled.\nFetching https://nyunews.com/news/2024/01/26/pro-palestinian-bobst-poetry/ took too long and was cancelled.\nFetching https://www.wjhl.com/news/local/kyle-rittenhouse-event-draws-supporters-protesters-at-etsu/ took too long and was cancelled.\nFetching https://newjersey.news12.com/group-gathers-ahead-of-toms-river-council-meeting-to-protest-policeemt-funding-decision took too long and was cancelled.\nFetching https://www.denverpost.com/2024/01/02/alamo-drafthouse-employees-union-drive-rally-denver/ took too long and was cancelled.\nThe following URLs had issues and were not downloaded:\nhttps://www.wgmd.com/pro-palestinian-protesters-deface-veterans-cemetery-in-los-angeles-spray-paint-free-gaza/\nhttps://www.nbcnews.com/politics/donald-trump/trump-confuses-nikki-haley-pelosi-talking-jan-6-rcna134863\nhttps://www.purdueexponent.org/campus/article_78be7d6e-c2bb-11ee-a25c-a3e2dff21694.html\nhttps://13wham.com/news/local/local-advocates-rally-in-downtown-rochester-on-51st-anniversary-of-roe-v-wade\nhttps://www.wvtm13.com/article/protests-kenneth-smith-execution-untied-nations-montgomery/46496998\nhttps://nypost.com/2024/01/21/news/scream-actress-melissa-barrera-joins-disruptive-anti-israel-rally-at-sundance/\nhttps://www.wlky.com/article/nonprofits-rally-frankfort-legislation-kentucky/46676604\nhttps://www.courier-journal.com/story/news/politics/2024/02/08/kentucky-employees-retirement-system-participants-rally-for-13th-check/72527656007/\nhttps://www.northjersey.com/story/news/2024/02/06/israel-hamas-war-day-of-action-for-palestine-nj-students-march/72478632007/\nhttps://www.washingtonpost.com/dc-md-va/2024/01/15/virginia-assembly-gun-rights-rally/\nhttps://dailybruin.com/2024/01/19/uc-divest-coalition-at-ucla-leads-hands-off-yemen-protest-on-campus\nhttps://www.washingtonpost.com/dc-md-va/2024/01/18/dc-march-for-life-rally-abortion/\nhttps://www.fox5dc.com/news/dc-activists-plan-protest-against-capitals-wizards-move-to-virginia\nhttps://www.latimes.com/entertainment-arts/movies/story/2024-01-21/pro-palestinian-protestors-vie-for-hollywoods-attention-at-2024-sundance-film-festival\nhttps://secure.everyaction.com/RKr139EpKUCZg8_TIWA18A2\nhttps://www.thetimestribune.com/news/dozens-rally-in-support-of-school-choice-amendment/article_6da6faa2-bbbb-11ee-9f2a-8354d0bdfbfd.html\nhttps://www.nbcnews.com/news/latino/convoy-rally-texas-mexico-border-attracts-trump-fans-decry-illegal-imm-rcna136967\nhttps://nyunews.com/news/2024/01/26/pro-palestinian-bobst-poetry/\nhttps://www.wjhl.com/news/local/kyle-rittenhouse-event-draws-supporters-protesters-at-etsu/\nhttps://newjersey.news12.com/group-gathers-ahead-of-toms-river-council-meeting-to-protest-policeemt-funding-decision\nhttps://www.denverpost.com/2024/01/02/alamo-drafthouse-employees-union-drive-rally-denver/\n\n\nFuture exception was never retrieved\nfuture: &lt;Future finished exception=NetworkError('Protocol error (Target.detachFromTarget): No session with given id')&gt;\npyppeteer.errors.NetworkError: Protocol error (Target.detachFromTarget): No session with given id\n\n\nUsing this approach, it took me five minutes to go through the list 100 URLs. I didn’t get every webpage, and I usually also run it twice on the same list to catch URLs that were missed either because of errors on my end or in the cloud.\nThe main delay is slow-loading pages. I have the timeout arbitrarily set to 30 seconds. Setting in longer might load one or two more more pages, but would also slow down the process since some pages will never load."
  },
  {
    "objectID": "posts/function-calling/pydantic-sm.html",
    "href": "posts/function-calling/pydantic-sm.html",
    "title": "Using ChatGPT to Make Social Media Posts",
    "section": "",
    "text": "This notebook uses pydantic and ChatGPT API’s function calling to make some social media posts about an article. Mostly, I was interested in ways to get it to make different styles of tweets, such as ones that summarize the article vs ones that highlight a single insight.\n\npip install openai pydantic -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport json\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom openai import OpenAI\nimport pandas as pd\n\npd.set_option('display.max_colwidth', 300)\n\nPydantic is a library that helps define the shape and type of our data, making sure it matches our expectations before we proceed with our code. Here’s a breakdown of the code block using Pydantic:\n\nBaseModel: This is a key part of Pydantic. Think of it as a blueprint for the data you expect. You define a class that inherits from BaseModel, and in it, you detail the structure of your data.\nFields: Inside each class that inherits from BaseModel, we define what data we expect using fields. For example, content: str means we expect a piece of data named content that should be a string. The Field(...) function is used to add extra details about each field, like a description or default value.\nClasses Defined:\n\nSummaryTweet: A class for creating objects that represent a concise version of an abstract.\nAnnounceTweet: A class for making objects that represent a tweet announcing a new article.\nInsightTweet: This class is for objects that encapsulate a simple insight from complex social science concepts.\nThreadTweet: A class for a list of tweets, making up a thread about an article.\nGenerateTweets: This class combines all the above, organizing them into lists of each type of tweet.\n\nList Fields: When we want to store multiple items (like multiple tweets of the same kind), we use List[SomeClass]. This tells Pydantic we expect a list where each item is an instance of SomeClass.\n\nThis code essentially sets up a structure for handling different types of tweets as data objects. By defining these classes, we ensure that the data we work with matches our expectations, reducing bugs and making our code more reliable.\n\nclass SummaryTweet(BaseModel):\n    content: str = Field(..., description=\"A concise version of the abstract in tweet format. No Hastags\")\n\nclass AnnounceTweet(BaseModel):\n    content: str = Field(..., description=\"A tweet announcing a new article. Text must include the author and title.  No Hastags\")\n\nclass InsightTweet(BaseModel):\n    content: str = Field(..., description=\"Simple, understandable statement distilling a complex social science concept.  No Hastags\")\n\nclass ThreadTweet(BaseModel):\n    content: List[str] = Field(default=[], description=\"A list of tweets that together form a thread about the article, summarizing its key points and implications, engaging the audience, and encouraging further discussion or reading. No hashtags.\")\n\nclass GenerateTweets(BaseModel):\n    summary_tweets: List[SummaryTweet] = Field(default=[], description=\"A list of SummaryTweet instances.\")\n    insight_tweets: List[InsightTweet] = Field(default=[], description=\"A list of InsightTweet instances.\")\n    announce_tweets: List[AnnounceTweet] = Field(default=[], description=\"A list of AnnounceTweet instances.\")\n    thread_tweets: List[ThreadTweet] = Field(default=[], description=\"A list of ThreadTweet instances.\")\n\n# Note: there might be a simpler way to do this, since each of the subclasses only has one field.\n\nFor our purposes, we are using to create a JSON schema that we want ChatGPT to follow.\n\nThreadTweet.model_json_schema()\n\n{'properties': {'content': {'default': [],\n   'description': 'A list of tweets that together form a thread about the article, summarizing its key points and implications, engaging the audience, and encouraging further discussion or reading. No hashtags.',\n   'items': {'type': 'string'},\n   'title': 'Content',\n   'type': 'array'}},\n 'title': 'ThreadTweet',\n 'type': 'object'}\n\n\n\nGenerateTweets.model_json_schema()\n\n{'$defs': {'AnnounceTweet': {'properties': {'content': {'description': 'A tweet announcing a new article. Text must include the author and title.  No Hastags',\n     'title': 'Content',\n     'type': 'string'}},\n   'required': ['content'],\n   'title': 'AnnounceTweet',\n   'type': 'object'},\n  'InsightTweet': {'properties': {'content': {'description': 'Simple, understandable statement distilling a complex social science concept.  No Hastags',\n     'title': 'Content',\n     'type': 'string'}},\n   'required': ['content'],\n   'title': 'InsightTweet',\n   'type': 'object'},\n  'SummaryTweet': {'properties': {'content': {'description': 'A concise version of the abstract in tweet format. No Hastags',\n     'title': 'Content',\n     'type': 'string'}},\n   'required': ['content'],\n   'title': 'SummaryTweet',\n   'type': 'object'},\n  'ThreadTweet': {'properties': {'content': {'default': [],\n     'description': 'A list of tweets that together form a thread about the article, summarizing its key points and implications, engaging the audience, and encouraging further discussion or reading. No hashtags.',\n     'items': {'type': 'string'},\n     'title': 'Content',\n     'type': 'array'}},\n   'title': 'ThreadTweet',\n   'type': 'object'}},\n 'properties': {'summary_tweets': {'default': [],\n   'description': 'A list of SummaryTweet instances.',\n   'items': {'$ref': '#/$defs/SummaryTweet'},\n   'title': 'Summary Tweets',\n   'type': 'array'},\n  'insight_tweets': {'default': [],\n   'description': 'A list of InsightTweet instances.',\n   'items': {'$ref': '#/$defs/InsightTweet'},\n   'title': 'Insight Tweets',\n   'type': 'array'},\n  'announce_tweets': {'default': [],\n   'description': 'A list of AnnounceTweet instances.',\n   'items': {'$ref': '#/$defs/AnnounceTweet'},\n   'title': 'Announce Tweets',\n   'type': 'array'},\n  'thread_tweets': {'default': [],\n   'description': 'A list of ThreadTweet instances.',\n   'items': {'$ref': '#/$defs/ThreadTweet'},\n   'title': 'Thread Tweets',\n   'type': 'array'}},\n 'title': 'GenerateTweets',\n 'type': 'object'}\n\n\n\n# I put in the introduction when using GPT4 because it actually uses the extra information.\n\narticle = '''\nTITLE: The MAGA Movement's Big Umbrella\nAUTHOR: Hank Johnston\nJOURNAL: Mobilization: An International Quarterly (2024) 28 (4): 409–433.\nLINK: https://doi.org/10.17813/1086-671X-28-4-409\n\nABSTRACT: This article considers the phenomenon of MAGAism as a general, “big-umbrella” social movement to probe its structure and persistence. Drawing on my research on nationalist movements, I discuss the narrative flexibility and emotional power of nationalism and consider how these characteristics fuel a particularly resentful form of majoritarian nationalism—MAGAism. I identify five points of entry for MAGA participation, starting with this bitter majoritarian nationalism, then populism, then traditional conservatism, next a Trumpian personality cult, and ending with the alt-right’s extreme white supremacism and fascism. I then discuss two forces that gather this unlikely collection of groups and individuals under the MAGA umbrella: (1) the flexibility of social identity and how it allows status-threat narratives to subsume and redirect economic and political claims; (2) the social media environment of the 2020s, cut loose from the traditional gatekeepers of news and information, wherein maximizing hits—not accuracy—is the guiding principle. On the one hand, social media intensify the prominence of the demagogic celebrity at the helm of the Republican Party, Donald J. Trump. On the other hand, alt-right trolls compete for prominence by seeking outrageousness and shock value, enhanced by algorithms that create a closed information environment. These social media trends pull MAGA adherents further to the right by inflaming public discourse and building the movement on lies and conspiracies.\nINTRODUCTION: The rise of far right-wing parties and extremist groups is now a topic of deep concern among social scientists who study collective action and contentious politics.  As a reader of this special issue of Mobilization, this trend no doubt has been on your mind a lot, as it has been on mine—especially the challenges it poses to democratic institutions.i In the U.S. today, one of the major political parties actively restricts voting rights, bans books in libraries, embraces conspiracies, flaunts constitutional principles, and limits public-school curricula on topics such as diversity, slavery, gender and sexuality (Natanson 2023a, 2023b)—and millions of MAGA supporters are okay with it (Baker, Perry, and Whitehead, 2020; Enders, Farhart, Miller, and Uscinski 2022; Graham et al. 2021; Stewart 2023). Further along the right-wing spectrum, one increasingly finds rising antisemitism, white Christian nationalism, gun-toting bullies, and misogynists (see Davis and Kettrey’s study in this issue) in the U.S. public sphere, raising parallels with European history in the early twentieth century and its brown shirts and black shirts (rather than today’s polo shirts and tac gear). While violence and intolerance are not unique to the extreme right, these trends reflect a synergy between the current political environment, eroding democratic norms, and—among more and more citizens in democratic states—authoritarian and even fascist tendencies (Jackson 2021; Wintemute, Robinson, Tomish 2022). In the U.S. these trends can be grouped under the umbrella of MAGAism where majoritarian nationalism mixes with populism, far-right extremism, and a virtual (in both meanings of the term) potpourri of themes such as antiwokeness, antivaccines, parental rights, gun rights, sovereign citizenry, science denial, masculinity, the lie of a stolen 2020 election, Trumpian personality cult, white replacement, QAnon conspiracies, among many others. This essay contextualizes the special issue articles by situating them in relation to trends in the U.S., which I discuss under the big umbrella of MAGAism. Following Tarrow (2021: 175), common usage in the popular press, and Trump himself, I consider MAGAism as a general social movement, one that encompasses the rightward shift of public discourse and politics in the U.S. plus the array of mobilizing groups that drive it.ii  I will offer a somewhat personalized essay that draws on insights from my work on the power (and danger) of nationalism to analyze a particularly resentful majoritarian nationalism—often labeled white nationalism in the U.S.  (Graham et al. 2021; Mudde 2019, Miller-Idriss, 2021; Young 2017). I am interested not only in militias, alt-right trolls, and white-supremacist churches,  but—more broadly—the mechanisms that allow these extremists to be grouped with millions of middle-class working citizens who come to the movement via parental-rights groups, suburban bible-study groups, or as antivaxxers, tradition-alists, or Constitutionalists.  MAGAism is a big-tent collective action—a movement with continuity, shared identity, diverse claims and political objectives, networked by an array of groups and organizations—the movement’s mobilizing structures that mostly stand apart from the Republican Party. I will identify the movement’s patterns of discourse and how it is structured to help understand the complexity, die-hard loyalty, denialism, and conspiracies that unite millions of MAGAistas in the face of factual evidence that should undermine their beliefs. I pose the fundamental question of collective action: how is the MAGA movement possible?  \n'''\n\n\ndef make_tweets(article):\n    client = OpenAI(\n        max_retries=3,\n        timeout=90.0, \n    )\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": ''' You write a variety of different types of social media posts about sociology articles.\nIf you perform well, everyone gets a raise, while there are negative consequences if the posts don't get views.\nNEVER USE HASHTAGS.\n''',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Generate five tweets of each style plus a tweet thread about this sociology article:\n\n            \n      {article}\n      \n      \"\"\",\n        },\n    ]\n\n    completion = client.chat.completions.create(\n        model = 'gpt-4-turbo-preview',     #   model=\"gpt-3.5-turbo\" is 20x cheaper but isn't as insightful \n        functions=[\n            {\n                \"name\": \"generate_tweets\",\n                \"description\": \"Create multiple social media posts, including a tweet thread, based on an article abstract.\",\n                \"parameters\": GenerateTweets.model_json_schema(),\n            },\n        ],\n        n=1,\n        messages=messages,\n    )\n    r = json.loads(completion.choices[0].message.function_call.arguments)\n    tweets = [{'type': tweet_type.split('_')[0], 'text': t['content']} for tweet_type in r for t in r[tweet_type]]\n\n    return pd.DataFrame(tweets)\n\n\ntdf = make_tweets(article)\n\n\n# Grouping and printing with check for list in 'text' column\nfor type_name, group in tdf.groupby('type'):\n    print(f\"Type: {type_name}\")\n    for index, row in group.iterrows():\n        if isinstance(row['text'], list):\n            for item in row['text']:\n                print(f\"- {item}\")\n        else:\n            print(f\"- {row['text']}\")\n\nType: announce\n- New research by Hank Johnston in Mobilization investigates the MAGA movement's vast reach and the social forces fueling its endurance. The MAGA Movement's Big Umbrella: https://doi.org/10.17813/1086-671X-28-4-409\nType: insight\n- Nationalism's emotional power can forge unlikely alliances, proving pivotal in the rise of movements like MAGAism.\n- Social identity's flexibility allows economic and political grievances to be reframed as status threats, fueling movements like MAGA.\n- In an era where hits trump accuracy, social media perpetuates echo chambers that magnify polarizing figures and ideologies.\n- The MAGA movement’s diverse entry points reflect the complex appeal of political identities in today's rapidly changing society.\n- Extremist movements leverage the digital age's vast reach, showcasing the dramatic impact of social media on political mobilization.\nType: summary\n- Hank Johnston's article in Mobilization delves into MAGAism, exploring its roots in resentful majoritarian nationalism and its five diverse points of entry.\n- Johnston identifies social media and the flexibility of social identity as key forces uniting diverse MAGA groups under one 'big umbrella.'\n- Through examining MAGAism, the article discusses how emotional narratives and nationalism fuel the movement's persistence and growth.\n- The MAGA movement is dissected for its structure: from majoritarian nationalism to extreme white supremacism, uncovering its complex foundation.\n- Johnston's research highlights how the 2020s social media landscape propels MAGAism, intensifying right-wing shifts through misinformation and shock value.\nType: thread\n- In Hank Johnston's latest article, 'The MAGA Movement's Big Umbrella,' he navigates the multifaceted world of MAGAism, a movement defined by its emotional engagement and diverse entry points.\n- MAGAism isn't just about one ideology. It’s about majoritarian nationalism, populism, conservatism, a Trumpian personality cult, and extreme white supremacism, all under one 'big umbrella.'\n- Johnston points out the crucial role of social identity flexibility and the chaotic 2020s social media landscape in unifying these varied elements, creating a powerful force despite conflicting views.\n- The rise of the MAGA movement signifies more than political allegiance; it's about the emotional narratives and nationalistic feelings that resonate with many, pushing the discourse further right through misinformation.\n- Lastly, this comprehensive look at MAGAism sheds light on how digital platforms contribute to the movement's momentum, emphasizing the need for awareness in the digital age. A must-read for anyone looking to understand the complexities of modern political movements.\n\n\nGPT4 doesn’t always follow the directions, for example, there’s only one “announce” style tweet, but GPT3.5 gets lost more often when something complex is going on, such as the tweet thread."
  },
  {
    "objectID": "posts/sentence-embeddings/sentence-embeddings.html",
    "href": "posts/sentence-embeddings/sentence-embeddings.html",
    "title": "Measuring text similarity with sentence embeddings",
    "section": "",
    "text": "A recent talk made me think about measuring the distance between a concept and a text. I know that, for example, Dustin Stolz and Marshall A. Taylor developed “Concept Mover’s Distance” and that folks like Laura K. Nelson have done nifty things with word embeddings, but I haven’t done much with them in years. This work has primarily used word embeddings, which are constructed using algorithms like Word2Vec, GloVe, or FastText, which analyze a corpus of text and learn to represent words as vectors based on the context in which words appear and their co-occurrence with other words. These vectors capture semantic and syntactic similarities among words. Sentences, paragraphs, or longer texts are represented as the average value of their word embeddings, and the method works quite well for measuring how similar two texts are based on the distance between their word embeddings. In contrast, sentence embeddings generated from models like BERT or other transformer-based architectures do not merely combine word vectors. Instead, these models are trained to understand the context and relationships between words in a sentence, producing embeddings that capture the nuanced meaning of the entire sentence. Fine-tuning these models on specific tasks, such as text similarity, further enhances their ability to represent sentences in a way that aligns with the task’s requirements. The resulting sentence transformer models seem to be what most people are using, so I thought I would play around with them. Plus Dustin and Marshall’s code is all in R which I’m not great at.\n\npip install -U --q sentence-transformers\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\nimport pandas as pd\nimport numpy as np\nimport warnings\n\n# Show wider columns\npd.set_option('display.max_colwidth', 200)\n\n\n# Filter out all warnings\nwarnings.filterwarnings(\"ignore\")\n\nThere are lots of different pretrained SentenceTransformer models to try. Since I don’t have a GPU, I’m using the smallest decent one.\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nEncoding a sentence returns an array, with each position measuring some latent aspect of the sentence. The all-MiniLM-L6-v2 model has 384 dimensions, meaning it represents sentences in a 384-dimensional space where each dimension captures a different aspect of the sentence’s semantic and syntactic properties.\n\nmodel.encode('We study apples.')\n\narray([ 7.25412294e-02,  2.94132698e-02, -1.04851276e-02,  7.10855499e-02,\n       -1.60725769e-02, -1.44585697e-02,  2.01909002e-02, -1.64043065e-02,\n        5.85170425e-02,  4.42925990e-02,  3.38043720e-02, -3.70339900e-02,\n       -1.01375952e-02,  1.51030989e-02, -2.33681127e-02, -7.01107532e-02,\n       -6.28457516e-02, -1.98269282e-02, -2.56151836e-02, -3.29240002e-02,\n       -3.28812934e-02,  1.20462896e-02,  2.59449407e-02, -9.30627715e-03,\n        3.87127437e-02,  5.07903211e-02,  1.87327876e-03, -3.24000195e-02,\n       -3.31559330e-02, -2.26897225e-02, -3.54236215e-02,  5.46763092e-02,\n        9.66718495e-02,  3.60654816e-02, -5.42746857e-02, -6.75004860e-03,\n        1.40686721e-01, -5.24841994e-02,  2.66690422e-02, -1.14018116e-02,\n       -5.27682193e-02,  7.28323534e-02,  2.73659322e-02,  1.07277647e-01,\n        1.61659811e-02,  3.81597877e-02, -1.69903729e-02, -3.23689356e-02,\n        2.42316276e-02,  6.92270622e-02, -7.24644810e-02,  5.75410959e-04,\n       -2.30131280e-02, -6.04602210e-02,  2.60792784e-02,  6.71790168e-02,\n        9.09342691e-02, -3.36206444e-02,  7.23590478e-02, -1.65215111e-03,\n        3.39566097e-02, -8.45273733e-02, -4.85875309e-02,  3.91481146e-02,\n        9.04154778e-03, -8.98436904e-02, -5.66358864e-02,  4.21869494e-02,\n       -2.75326185e-02,  8.08383152e-03,  3.66940014e-02,  2.62768455e-02,\n        4.43452112e-02,  8.13365355e-02,  4.25574370e-02,  4.02889028e-02,\n        1.33797051e-02, -5.26551530e-02, -1.71313528e-03, -3.53252031e-02,\n       -1.33084031e-02, -3.56830396e-02, -8.78533255e-03, -2.92915851e-04,\n       -5.95996482e-03,  1.08087016e-02, -3.73279974e-02,  1.11694261e-02,\n       -8.32256749e-02,  3.41632962e-02,  1.11821033e-02, -4.44804654e-02,\n       -5.93070313e-02,  2.85391472e-02, -5.28932542e-05,  1.43566700e-02,\n        7.79099949e-03, -4.66752164e-02,  3.37223820e-02,  1.38128072e-01,\n       -2.48398129e-02,  6.67299479e-02,  4.20545265e-02,  6.46362547e-03,\n       -2.01197024e-02, -4.62081917e-02, -7.40140006e-02, -6.16762936e-02,\n        6.25649691e-02,  5.63399047e-02,  5.71039356e-02, -1.97048280e-02,\n       -1.04044281e-01,  1.12997763e-01,  7.35701993e-02, -5.85305728e-02,\n        5.75302951e-02,  2.13396046e-02,  2.31305137e-02, -1.74588244e-02,\n       -1.42283470e-03,  9.98423249e-02, -5.42779267e-02, -4.08335775e-02,\n        3.16505209e-02, -6.30080476e-02,  5.79967070e-03, -6.57705620e-33,\n        9.47775226e-03, -2.89162844e-02,  5.21772802e-02,  4.72146124e-02,\n        1.75573980e-03, -2.80149970e-02,  6.52783411e-03,  6.76577762e-02,\n        1.11630604e-01, -5.42772980e-03, -1.63363609e-02,  1.93208605e-02,\n       -6.67217327e-03,  3.95874586e-03,  6.84757829e-02, -4.26633768e-02,\n       -5.80286346e-02,  4.60271128e-02, -6.45208359e-02, -1.97640937e-02,\n       -4.20666821e-02, -1.40169218e-01,  1.41780432e-02, -4.24528383e-02,\n       -7.28586018e-02, -4.38546352e-02,  2.99364813e-02, -1.22642733e-01,\n        1.05384283e-01, -9.39285476e-03,  4.56614830e-02,  4.46087569e-02,\n       -7.81959221e-02, -2.15675607e-02, -1.61162820e-02,  1.18166637e-02,\n        1.05830543e-01,  4.55662869e-02,  2.54090354e-02,  7.13379355e-04,\n       -4.73194495e-02,  6.20454252e-02,  1.28243357e-01, -1.05106169e-02,\n        6.04844540e-02,  4.86876108e-02,  6.08739592e-02,  7.04808533e-02,\n       -3.78209017e-02,  1.76480156e-03, -7.23417252e-02, -4.73359115e-02,\n        3.92539166e-02, -5.92344292e-02, -2.03246195e-02,  1.21152870e-01,\n        2.25762613e-02, -2.12923903e-02, -7.87275955e-02,  1.72557738e-02,\n       -1.17437020e-02,  4.75608334e-02,  2.89987470e-03,  2.87241656e-02,\n       -7.83866942e-02,  1.22669660e-01, -7.32923672e-02, -2.76942160e-02,\n        3.90719483e-03,  5.07837161e-02, -1.27230436e-01, -1.96105265e-03,\n        1.29499817e-02, -2.39144196e-03, -1.02660574e-01, -3.95029709e-02,\n        2.90869810e-02,  1.89506123e-03, -1.57893542e-02, -1.35825286e-02,\n       -1.47497458e-02, -7.59655535e-02, -2.41609924e-02, -4.64349203e-02,\n       -5.03189899e-02,  1.03728287e-01,  5.30537264e-03, -4.08360064e-02,\n        4.37239483e-02,  1.50352474e-02,  1.78553362e-03,  2.78608631e-02,\n       -1.33840907e-02, -1.94217525e-02, -6.97820038e-02,  3.64022021e-33,\n       -3.52125727e-02, -3.46545912e-02, -1.97930746e-02,  3.53415050e-02,\n        4.72753681e-02, -3.01812422e-02, -2.71848068e-02, -4.41191671e-03,\n       -5.37364073e-02, -3.98576930e-02, -5.23934960e-02,  2.20293589e-02,\n       -1.31326532e-02,  3.97575926e-03,  5.15052769e-03, -5.09656556e-02,\n        2.74809133e-02,  6.49610087e-02, -3.33442353e-02,  3.82365957e-02,\n       -8.04736614e-02,  4.02408168e-02,  1.55164497e-02, -3.17362957e-02,\n       -8.52044765e-03, -1.36113567e-02, -1.03016114e-02, -1.63739058e-03,\n       -3.56106050e-02,  4.95722368e-02,  3.19658928e-02, -1.06780723e-01,\n       -6.47186339e-02, -5.92985041e-02,  7.48166954e-03,  3.27526592e-02,\n       -2.17823554e-02, -3.67993712e-02, -1.45561453e-02,  5.68174422e-02,\n        5.13046794e-02,  2.38617416e-02, -5.04206121e-02,  7.14061782e-02,\n        5.55717610e-02,  9.42208767e-02, -2.91015077e-02,  1.10552333e-01,\n       -1.50970956e-02,  1.36306453e-02,  2.84949895e-02,  1.54124191e-02,\n       -9.30073187e-02, -7.49932304e-02,  3.89047265e-02,  6.32112427e-03,\n        1.68052688e-02, -5.48552349e-02, -4.07663062e-02,  3.40671390e-02,\n       -2.44489089e-02, -6.63542328e-03,  1.33677274e-02,  4.33265902e-02,\n       -6.07616976e-02, -2.95820832e-03, -8.81790649e-03,  3.45228761e-02,\n       -4.97724488e-02,  4.87365052e-02,  5.39725199e-02,  2.30813660e-02,\n       -6.28042817e-02, -1.07893415e-01, -4.70633470e-02,  2.30335053e-02,\n       -1.66683551e-02, -6.54008314e-02, -5.05928211e-02,  3.11856903e-02,\n        8.44117161e-03,  6.16906621e-02, -1.53189339e-02,  6.40073717e-02,\n        2.83199176e-02,  2.28565577e-02,  3.77516486e-02,  2.01664753e-02,\n       -4.26778495e-02, -1.64321400e-02, -7.81823099e-02, -3.34092714e-02,\n        3.54452953e-02, -1.01244465e-01, -7.63411000e-02, -1.36826950e-08,\n       -6.46132380e-02, -4.54020202e-02,  7.10007921e-02, -1.07585511e-03,\n        9.78543423e-03,  4.50233184e-02, -7.06218481e-02,  7.65154064e-02,\n        1.31740945e-03, -2.06936337e-02, -5.31158149e-02,  4.04155143e-02,\n       -7.19135329e-02,  7.89430514e-02,  5.02120666e-02,  7.71357790e-02,\n        9.33832824e-02,  7.45651722e-02, -4.75667641e-02, -5.17647667e-03,\n       -4.44874056e-02,  3.04975566e-02, -6.90343650e-03,  8.24641958e-02,\n       -1.22354254e-02,  6.76435754e-02,  6.36463240e-03,  2.63797827e-02,\n        4.96475771e-02,  8.34565535e-02,  3.48740071e-02,  2.49225888e-02,\n       -9.88677517e-02, -5.98776303e-02,  3.49258445e-03, -6.12113327e-02,\n       -2.36107334e-02,  2.68156715e-02,  2.23525390e-02,  9.43851192e-03,\n       -1.19599409e-01, -6.57922998e-02, -2.84594391e-02,  3.78113380e-03,\n       -3.77423279e-02,  6.28536614e-03, -4.52296101e-02,  7.04570040e-02,\n       -7.60547956e-03,  7.97596648e-02, -2.58326419e-02, -3.47822644e-02,\n        8.89370292e-02,  8.53143539e-03,  4.75420151e-03,  1.21760555e-02,\n        3.13673206e-02, -1.13887928e-01, -6.46701753e-02,  7.35506192e-02,\n        9.01015177e-02, -3.20984498e-02,  4.94505614e-02, -1.10336766e-03],\n      dtype=float32)\n\n\n\nlen(model.encode('We study apples.'))\n\n384\n\n\nA little function to compare the embeddings of a single concept with each word in a sentence, and then the entire sentence. I built this to confirm that sentence embeddings actually do what I think they are doing.\n\ndef concept_string_sim(concept, \n                       text\n):\n    concept_emedding = model.encode(concept)\n    for word in text.split():\n        word_embedding = model.encode(word)\n        similarity = util.pytorch_cos_sim(concept_emedding, word_embedding)[0][0]\n        print(f'Similarity between {word} and {concept} is {similarity:.2f}.')\n\n    text_embedding = model.encode(text)\n    similarity = util.pytorch_cos_sim(concept_emedding, text_embedding)[0][0]\n    print(f'Similarity between {text} and {concept} is {similarity:.2f}.')\n\n\nconcept_string_sim('rock, music', 'Grateful Dead show')\n\nSimilarity between Grateful and rock, music is 0.18.\nSimilarity between Dead and rock, music is 0.37.\nSimilarity between show and rock, music is 0.28.\nSimilarity between Grateful Dead show and rock, music is 0.36.\n\n\n\nconcept_string_sim('thankful attitude', 'Grateful Dead show')\n\nSimilarity between Grateful and thankful attitude is 0.50.\nSimilarity between Dead and thankful attitude is 0.23.\nSimilarity between show and thankful attitude is 0.20.\nSimilarity between Grateful Dead show and thankful attitude is 0.20.\n\n\nIn the first example, the concept “rock, music” shows varying degrees of similarity with each word in the sentence “Grateful Dead show,” with the highest similarity observed with the word “Dead” (0.37), suggesting that the model captures the association between the band “Grateful Dead” and rock music. The overall sentence similarity score (0.36) closely aligns with the highest word similarity score, indicating that sentence embeddings can indeed capture the essence of the concept in relation to the full sentence context.\nIn the second case, the concept “thankful attitude” has the highest similarity with the word “Grateful” (0.50), which is intuitive given the semantic closeness of “thankful” and “Grateful.” However, the overall similarity between the entire sentence “Grateful Dead show” and the concept “thankful attitude” is lower (0.20), suggesting that while individual words like “Grateful” strongly resonate with the concept, the context provided by the entire sentence shifts the meaning away from the concept’s core, demonstrating how sentence embeddings can differentiate between the significance of individual words and the collective meaning of a sentence.\nNo measuring some sentence similarities.\n\ndef string_string_sim(text1,\n                      text2,\n):\n    text1_embedding = model.encode(text1)\n    text2_embedding = model.encode(text2)\n\n    similarity = util.pytorch_cos_sim(text1_embedding, text2_embedding)[0][0]\n    print(f'Similarity is {similarity:.2f}.')\n\n\nstring_string_sim('We study revolutions.', 'This paper examines social movements.')\n\nSimilarity is 0.47.\n\n\n\nstring_string_sim('We study revolutions.', 'This paper examines health behaviors.')\n\nSimilarity is 0.18.\n\n\n\nstring_string_sim('We study revolutions.', 'This paper examines social movements in Algeria during the 1970s.')\n\nSimilarity is 0.36.\n\n\nThis also worked. Note, however, that the last example had a lower correlation with “We study revolutions.” This is because the the additional information in the sentence, like the country and time, were uncorrelated with the shorter, first sentence. I suspect this means that, on average, longer texts will have a lower correlation with a short, conceptual phrase than shorter texts simply because they are more likely to include different types of information.\nOn to measuring a concept. In this case, I’m interested in to what degree a sociological research article’s abstract is about measuring social movements and protest.\n\nmovement_word_list=['social movement', 'contentious politics', 'mobilization',]\n\nmovement_words = ', '.join(movement_word_list)\n\n\nstring_string_sim(movement_words,\n                      'We study revolutions.',\n)\n\nSimilarity is 0.44.\n\n\n\nstring_string_sim(movement_words,\n                      'This paper examines health behaviors.',\n)\n\nSimilarity is 0.18.\n\n\n\nstring_string_sim(movement_words,\n                      'We study revolutions in Algeria.',\n)\n\nSimilarity is 0.42.\n\n\nGreat. Now an example from something publishing in Mobilization.\n\nabstract_sf = (\n    \"All around the world, school-entry cohorts are organized on an annual \"\n    \"calendar so that the age of students in the same cohort differs by up to \"\n    \"one year. It is a well-established finding that this age gap entails a \"\n    \"consequential (dis)advantage for academic performance referred to as the \"\n    \"relative age effect (RAE). This study contributes to a recent strand of \"\n    \"research that has turned to investigate the RAE on non-academic outcomes \"\n    \"such as personality traits. An experimental setup is used to estimate the \"\n    \"causal effect of monthly age on cognitive effort in a sample of 798 \"\n    \"fifth-grade students enrolled in the Spanish educational system, \"\n    \"characterized by strict enrolment rules. Participants performed three \"\n    \"different real-effort tasks under three different incentive conditions: no \"\n    \"rewards; material rewards; and material and status rewards. We observe \"\n    \"that older students outwork their youngest peers by two-fifths of a \"\n    \"standard deviation, but only when material rewards for performance are in \"\n    \"place. Despite the previously reported higher taste for competition among \"\n    \"the older students within a school-entry cohort, we do not find that the \"\n    \"RAE on cognitive effort increases after inducing competition for peer \"\n    \"recognition. Finally, the study also provides suggestive evidence of a \"\n    \"larger RAE among boys and students from lower social strata. Implications \"\n    \"for sociological research on educational inequality are discussed. To \"\n    \"conclude, we outline policy recommendations such as implementing \"\n    \"evaluation tools that nudge teachers toward being mindful of relative age \"\n    \"differences.\"\n)\n\nstring_string_sim(abstract_moby, movement_words)\n\nSimilarity is 0.36.\n\n\nAn a non-movements article from Social Forces.\n\nabstract_sf = (\n    \"All around the world, school-entry cohorts are organized on an annual \"\n    \"calendar so that the age of students in the same cohort differs by up to \"\n    \"one year. It is a well-established finding that this age gap entails a \"\n    \"consequential (dis)advantage for academic performance referred to as the \"\n    \"relative age effect (RAE). This study contributes to a recent strand of \"\n    \"research that has turned to investigate the RAE on non-academic outcomes \"\n    \"such as personality traits. An experimental setup is used to estimate the \"\n    \"causal effect of monthly age on cognitive effort in a sample of 798 \"\n    \"fifth-grade students enrolled in the Spanish educational system, \"\n    \"characterized by strict enrolment rules. Participants performed three \"\n    \"different real-effort tasks under three different incentive conditions: no \"\n    \"rewards; material rewards; and material and status rewards. We observe \"\n    \"that older students outwork their youngest peers by two-fifths of a \"\n    \"standard deviation, but only when material rewards for performance are in \"\n    \"place. Despite the previously reported higher taste for competition among \"\n    \"the older students within a school-entry cohort, we do not find that the \"\n    \"RAE on cognitive effort increases after inducing competition for peer \"\n    \"recognition. Finally, the study also provides suggestive evidence of a \"\n    \"larger RAE among boys and students from lower social strata. Implications \"\n    \"for sociological research on educational inequality are discussed. To \"\n    \"conclude, we outline policy recommendations such as implementing \"\n    \"evaluation tools that nudge teachers toward being mindful of relative age \"\n    \"differences.\"\n)\n\nstring_string_sim(abstract_sf, movement_words)\n\nSimilarity is -0.04.\n\n\nGreat. Results are plausible. Now I’m going to try it out an entire dataset of 10K recent sociology articles.\n\ndf = pd.read_json('https://raw.githubusercontent.com/nealcaren/notes/main/posts/abstracts/sociology-abstracts.json')\nlen(df)\n\n9797\n\n\nI revised the function so that it takes a word embedding, rather than a word for one of the inputs. This way, it only calculates the movement words embedding once, rather than once for each of the 10,000 comparisons. It also outputs just the numeric value of the correlation, rather than a phrase.\n\n\nmovement_word_embedding = model.encode(movement_words)\n\ndef string_embedding_sim(text1,\n                      text2_embedding=movement_word_embedding,\n):\n    text1_embedding = model.encode(text1)\n\n    similarity = util.pytorch_cos_sim(text1_embedding, text2_embedding)[0][0].item()\n    return similarity\n\n# check that it works\nstring_embedding_sim(abstract_moby )\n\n0.36133891344070435\n\n\nApplying the function, which uses the smallest model, to my dataframe of 10,000 cases takes about 4 minutes on my MacBook Air with an M2 processor. In contrast, it takes only seconds running in an environment with a GPU, such as Google Colab. Also, you would ideally want to store the article embeddings somewhere rather than discarding them, as the encoding phase of the function is the computationally-intense part.\n\ndf['abstract_movement_similarity'] = df['Abstract'].apply(string_embedding_sim)\n\nPlot the results. Looks pretty normal but with a little right skew, which are presumably the articles that focus on movements.\n\ndf['abstract_movement_similarity'].hist(bins=20)\n\n\n\n\n\n\n\n\nNext, look at a sample of articles with different similarity scores, sorted by highest to lowest. The measure has some face validity, as the movementness of the articles declines across the clusters.\n\n# Step 1: Create quintiles\ndf['Quintile'] = pd.qcut(df['abstract_movement_similarity'], 20, labels=False)\n\n# Step 2: Filter for the top quarter quintiles (quintiles 15-19)\ntop_half_quintiles = df[df['Quintile'] &gt;= 15]\n\n# Step 3: Display \"Title\" and \"Source title\" for a random sample of 5 rows within each top half quintile\nfor quintile in range(19, 14, -1):\n    sample = top_half_quintiles[top_half_quintiles['Quintile'] == quintile].sample(n=5)\n    print(f\"Quintile {quintile + 1}:\")\n    display(sample[['Source title', 'Title', 'abstract_movement_similarity']])\n\nQuintile 20:\nQuintile 19:\nQuintile 18:\nQuintile 17:\nQuintile 16:\n\n\n\n\n\n\n\n\n\nSource title\nTitle\nabstract_movement_similarity\n\n\n\n\n4666\nSociological Forum\nGoing Green: Environmental Protest, Policy, and CO2 Emissions in U.S. States, 1990–2007\n0.439596\n\n\n3386\nSocial Currents\nTactics and targets: Explaining shifts in grassroots environmental resistance\n0.476411\n\n\n4199\nMobilization\nMovement-countermovement dynamics and mobilizing the electorate\n0.524236\n\n\n1474\nSociological Forum\nBe Careful What You Wish For: The Ironic Connection Between the Civil Rights Struggle and Today's Divided America\n0.596116\n\n\n8583\nSociological Inquiry\nThe Cultural and the Racial: Stitching Together the Sociology of Race and Ethnicity and the Sociology of Culture\n0.429177\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource title\nTitle\nabstract_movement_similarity\n\n\n\n\n7252\nQualitative Sociology\nThe Social Life of the State: Relational Ethnography and Political Sociology\n0.366393\n\n\n5255\nDu Bois Review\nRethinking models of minority political participation: Inter-and intra-group variation in political \"styles\"\n0.394801\n\n\n5346\nSocial Forces\nThe Persuasive Power of Protest. How Protest wins Public Support\n0.363187\n\n\n1640\nSocial Psychology Quarterly\nSamuel Stouffer and Relative Deprivation\n0.362094\n\n\n8030\nSociological Forum\nBroker Wisdom: How Migrants Navigate a Broker-Centric Migration System in Vietnam1,2\n0.403658\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource title\nTitle\nabstract_movement_similarity\n\n\n\n\n3620\nMobilization\nLoud and clear: The effect of protest signals on congressional attention\n0.335533\n\n\n4417\nGender and Society\n“Manning Up” to be a Good Father: Hybrid Fatherhood, Masculinity, and U.S. Responsible Fatherhood Policy\n0.315162\n\n\n9603\nCity and Community\nCommunity Social Capital, Racial Diversity, and Philanthropic Resource Mobilization in the Time of a Pandemic\n0.330078\n\n\n2222\nSociological Perspectives\nCultural Capital, Motherhood Capital, and Low-income Immigrant Mothers' Institutional Negotiations\n0.339970\n\n\n6999\nSocial Forces\nEmigration and Electoral Outcomes in Mexico: Democratic Diffusion, Clientelism, and Disengagement\n0.326051\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource title\nTitle\nabstract_movement_similarity\n\n\n\n\n881\nSociological Forum\nChanging Childrearing Beliefs Among Indigenous Rural-to-Urban Migrants in El Alto, Bolivia\n0.291009\n\n\n3873\nAmerican Journal of Sociology\nInterlock globally, act domestically: Corporate political unity in the 21st century\n0.289036\n\n\n2762\nSocial Problems\nMoral panic, moral breach: Bernhard goetz, george zimmerman, and racialized news reporting in contested cases of self-defense\n0.302752\n\n\n501\nDu Bois Review\nRace, justice, and desegregation\n0.290871\n\n\n105\nSymbolic Interaction\nChicago, jazz and marijuana: Howard Becker on Outsiders\n0.308609\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource title\nTitle\nabstract_movement_similarity\n\n\n\n\n1024\nSocial Problems\nChilling Effects: Diminished Political Participation among Partners of Formerly Incarcerated Men\n0.267233\n\n\n7057\nGender and Society\nThe Gender Mobility Paradox: Gender Segregation and Women’s Mobility Across Gender-Type Boundaries, 1970–2018\n0.263790\n\n\n7265\nSocial Forces\nThe Limits of Gaining Rights while Remaining Marginalized: The Deferred Action for Childhood Arrivals (DACA) Program and the Psychological Wellbeing of Latina/o Undocumented Youth\n0.267221\n\n\n873\nSocial Currents\nRethinking organizational decoupling: Fields, power struggles, and work routines\n0.267197\n\n\n6205\nSymbolic Interaction\nDigitalization as “an Agent of Social Change” in a Supermarket Chain: Applying Blumer's Theory of Industrialization in Contemporary Society\n0.271341\n\n\n\n\n\n\n\nAnother check. Which journals publish the most movement research?\n\ndf.groupby('Source title')['abstract_movement_similarity'].mean().sort_values(ascending=True).plot(kind='barh')\n\n\n\n\n\n\n\n\nAnd what’s the most movementy article in each journal?\n\n# Group by 'Source Title' and find the index of the max 'abstract_movement_similarity' in each group\nidx = df.groupby('Source title')['abstract_movement_similarity'].idxmax()\n\n# Filter the DataFrame to keep only the rows with the highest 'abstract_movement_similarity' in each group\nhighest_values_df = df.loc[idx]\ndisplay = ['Source title', 'Title', 'abstract_movement_similarity']\nhighest_values_df[display].sort_values(by='abstract_movement_similarity', ascending=False)\n\n\n\n\n\n\n\n\nSource title\nTitle\nabstract_movement_similarity\n\n\n\n\n1923\nMobilization\nSocial movements in an age of participation\n0.702846\n\n\n433\nAmerican Journal of Sociology\nIssue bricolage: Explaining the configuration of the social movement sector, 1960–1995\n0.680012\n\n\n3768\nSocial Problems\nEconomic breakdown and collective action\n0.660584\n\n\n4400\nSociology of Race and Ethnicity\nThe Anti-oppressive Value of Critical Race Theory and Intersectionality in Social Movement Study\n0.652746\n\n\n5221\nSocial Currents\nAssessing the Explanatory Power of Social Movement Theories across the Life Course of the Civil Rights Movement\n0.641801\n\n\n8064\nSociological Perspectives\nPolicy Relay: How Affirmative Consent Went from Controversy to Convention\n0.632110\n\n\n4489\nTheory and Society\nCombining transition studies and social movement theory: towards a new research agenda\n0.631707\n\n\n6325\nSocial Forces\nPathways to modes of movement participation: Micromobilization in the nashville civil rights movement\n0.629331\n\n\n2482\nAmerican Sociological Review\nTactical Innovation in Social Movements: The Effects of Peripheral and Multi-Issue Protest\n0.616886\n\n\n6201\nCity and Community\nConfronting Scale: A Strategy of Solidarity in Urban Social Movements, New York City and Beyond\n0.606433\n\n\n1474\nSociological Forum\nBe Careful What You Wish For: The Ironic Connection Between the Civil Rights Struggle and Today's Divided America\n0.596116\n\n\n4818\nQualitative Sociology\nLife Histories and Political Commitment in a Poor People’s Movement\n0.581159\n\n\n5574\nSociological Theory\nOverflowing Channels: How Democracy Didn’t Work as Planned (and Perhaps a Good Thing It Didn’t)\n0.561438\n\n\n1278\nSociological Science\nDissecting The Spirit Of Gezi: Influence vs. selection in the occupy Gezi movement\n0.559833\n\n\n3367\nSocial Science Research\nHow social media matter: Repression and the diffusion of the Occupy Wall Street movement\n0.559743\n\n\n4354\nSociological Inquiry\nPracticing Gender and Race in Online Activism to Improve Safe Public Space in Sweden\n0.557239\n\n\n7128\nSymbolic Interaction\n“Meet Them Where They Are”: Attentional Processes in Social Movement Listening\n0.546802\n\n\n9150\nSocial Networks\nHow networks of social movement issues motivate climate resistance\n0.531231\n\n\n4094\nWork and Occupations\nRenewed Activism for the Labor Movement: The Urgency of Young Worker Engagement\n0.527430\n\n\n3722\nSocial Psychology Quarterly\nMeasuring Resonance and Dissonance in Social Movement Frames With Affect Control Theory\n0.505426\n\n\n8292\nDu Bois Review\nREACTION to the BLACK CITY AS A CAUSE of MODERN CONSERVATISM\n0.505141\n\n\n4360\nSociological Methods and Research\nSize Matters: Quantifying Protest by Counting Participants\n0.504456\n\n\n7333\nGender and Society\nImmigrant and Refugee Youth Organizing in Solidarity With the Movement for Black Lives\n0.497979\n\n\n2597\nPoetics\nPolitical space and the space of polities: Doing politics across nations\n0.487815\n\n\n833\nSociology of Education\nThe Origins of Race-conscious Affirmative Action in Undergraduate Admissions: A Comparative Analysis of Institutional Change in Higher Education\n0.465736\n\n\n8179\nJournal of Marriage and Family\nCentral American immigrant mothers' narratives of intersecting oppressions: A resistant knowledge project\n0.403927\n\n\n2628\nDemography\nLarge-Scale Urban Riots and Residential Segregation: A Case Study of the 1960s U.S. Riots\n0.347279\n\n\n7289\nJournal of Health and Social Behavior\nFrom Medicine to Health: The Proliferation and Diversification of Cultural Authority\n0.344940\n\n\n\n\n\n\n\nCan I add to my CV that I have the most movementy article in Social Problems?\nOkay, now a different concept. How quantitative is the research?\n\n\nquantitative_words = ', '.join([\n    \"surveys\",\n    \"experiments\",\n    \"quasi-experiments\",\n    \"regression analysis\",\n    \"statistical analysis\",\n    \"correlation\",\n])\n\nquantitative_embedding = model.encode(quantitative_words)\n\n\ndf['abstract_quant_similarity'] = df['Abstract'].apply(string_embedding_sim, \n                                                       text2_embedding=quantitative_embedding)\n\n\ndf.groupby('Source title')['abstract_quant_similarity'].mean().sort_values(ascending=True).plot(kind='barh')\n\n\n\n\n\n\n\n\nVery plausible, although I think that SM&R scores highest both because the publish lots of quantitative work plus the abstracts rarely discuss anything but methods, so it’s not quite an apples-to-apples comparison.\nFinally, how are the two concepts related at the abstract level?\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Assuming df is your DataFrame\n# Calculate the Pearson correlation coefficient\ncorrelation = df['abstract_quant_similarity'].corr(df['abstract_movement_similarity'])\n\n# Determine the common range for both axes based on the min and max of both series\ncommon_range = [\n    min(df['abstract_quant_similarity'].min(), df['abstract_movement_similarity'].min()), \n    max(df['abstract_quant_similarity'].max(), df['abstract_movement_similarity'].max())\n]\n\n# Create the scatter plot with square dimensions\nplt.figure(figsize=(4, 4))  # Makes the figure square in size\nplt.scatter(df['abstract_quant_similarity'], df['abstract_movement_similarity'], alpha=0.5)\n\n# Set the same range for both X and Y axes\nplt.xlim(common_range)\nplt.ylim(common_range)\n\n\n# Add a title with the correlation coefficient, formatted to two decimal places\nplt.title(f'Scatter Plot of Quantitative vs. Movement Similarity in Abstracts\\nCorrelation: {correlation:.2f}')\n\n# Add x and y labels\nplt.xlabel('Quantitative Similarity')\nplt.ylabel('Movement Similarity')\n\n# Ensure the aspect ratio is equal to make the plot truly square\nplt.gca().set_aspect('equal', adjustable='box')\n\nplt.show()\n\n\n\n\n\n\n\n\nI interpret this as both (1) movements research is not heavily quantitative and (2) abstracts that discuss movement theories and cases spend less time talking about methods, but that’s is probably also true of most articles that are about something rather than methods.\nTo do: Can you do math with sentence embeddings, like what’s a similar paper but instead of analyzing surveys, uses in-depth interviews? Here’s a sample code from ChatGPT.\n\nfrom sentence_transformers import SentenceTransformer, util\nimport numpy as np\nfrom torch import nn\n\ndef transform_and_match(text, remove_concept, add_concept, candidates):\n    \"\"\"\n    Transforms the text by removing one concept and adding another, then finds the nearest match from candidates.\n    \"\"\"\n    # Initialize the model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n\n    # Generate embeddings\n    text_embedding = model.encode(text, convert_to_tensor=True)\n    remove_embedding = model.encode(remove_concept, convert_to_tensor=True)\n    add_embedding = model.encode(add_concept, convert_to_tensor=True)\n    candidate_embeddings = model.encode(candidates, convert_to_tensor=True)\n\n    # Perform vector arithmetic (text - remove + add)\n    transformed_embedding = text_embedding - remove_embedding + add_embedding\n\n    # Compute cosine similarities between the transformed embedding and each candidate\n    # Ensure all tensors are moved to CPU memory before conversion to NumPy\n    similarities = util.cos_sim(transformed_embedding, candidate_embeddings).cpu()\n\n    # Find the index of the highest similarity score\n    nearest_match_idx = similarities.argmax()\n\n    # Return the nearest matching candidate text\n    return candidates[nearest_match_idx]\n\n\nnearest_match = transform_and_match(text, remove_concept, add_concept, candidates)\n\nprint(f\"Nearest match: {nearest_match}\")\n\nNearest match: A love story that unfolds in the least expected places."
  },
  {
    "objectID": "posts/abstracts/first-question.html",
    "href": "posts/abstracts/first-question.html",
    "title": "What percent of sociology journal abstracts start with a question?",
    "section": "",
    "text": "import pandas as pd\nimport nltk\nnltk.download('punkt')\n\nfrom nltk.tokenize import sent_tokenize\n\n[nltk_data] Downloading package punkt to /Users/nealcaren/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nLoad a CSV file I created from a Scopus search for specific sociology journals over the last decade.\n\ndf = pd.read_json('https://raw.githubusercontent.com/nealcaren/notes/main/posts/abstracts/sociology-abstracts.json')\ndf['Year'].value_counts()\n\nYear\n2018    1076\n2017    1068\n2019    1033\n2015    1031\n2016     989\n2021     983\n2022     972\n2020     932\n2014     870\n2023     843\nName: count, dtype: int64\n\n\n\ndf['Source title'].value_counts()\n\nSource title\nDemography                               890\nJournal of Marriage and Family           757\nSocial Science Research                  672\nSocial Forces                            594\nSociological Forum                       503\nSociological Perspectives                455\nAmerican Sociological Review             417\nSocial Networks                          407\nSociological Methods and Research        381\nSocial Problems                          366\nGender and Society                       326\nAmerican Journal of Sociology            324\nJournal of Health and Social Behavior    314\nSociological Inquiry                     310\nSociology of Race and Ethnicity          289\nTheory and Society                       287\nSociological Science                     271\nCity and Community                       260\nSocial Currents                          255\nSymbolic Interaction                     244\nQualitative Sociology                    229\nMobilization                             207\nSocial Psychology Quarterly              201\nPoetics                                  183\nDu Bois Review                           182\nSociology of Education                   171\nSociological Theory                      160\nWork and Occupations                     142\nName: count, dtype: int64\n\n\n\ndef first_sentence(abstract):\n    sentences = sent_tokenize(abstract)\n    return sentences[0]\n\n# Apply the function to the 'Abstract' column to create the new feature\ndf['abstract_first_sentence'] = df['Abstract'].apply(first_sentence)\n\n\ndf['abstract_first_sentence_punctuation'] = df['abstract_first_sentence'].str[-1]\ndf['abstract_first_sentence_punctuation'].value_counts()\n\nabstract_first_sentence_punctuation\n.    9290\n?     480\n\"      24\n)       2\n!       1\nName: count, dtype: int64\n\n\n\n# Okay, who used the exclamation mark?!!!\n\nscreen = df['abstract_first_sentence_punctuation']==\"!\"\ndf[screen]\n\n\n\n\n\n\n\n\nDOI\nSource title\nYear\nAuthors\nTitle\nAbstract\nabstract_first_sentence\nabstract_first_sentence_punctuation\n\n\n\n\n4178\n10.1086/700831\nAmerican Journal of Sociology\n2018\nPadgett J.F.\nFaulkner’s assembly of memories into history: ...\nIn Absalom, Absalom! William Faulkner develops...\nIn Absalom, Absalom!\n!\n\n\n\n\n\n\n\n\n# It was a parsing error. So sad!\n\n\ndf['abstract_starts_with_a_question'] = df['abstract_first_sentence_punctuation'] == '?'\ndf['abstract_starts_with_a_question'].mean()\n\n0.04899459018066755\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Set the Seaborn theme\nsns.set_theme(style=\"white\")\n\n# Create the plot with pandas, now styled by Seaborn\nax = pd.crosstab(df['Source title'],\n                 df['abstract_starts_with_a_question'], normalize='index').sort_values(by=True)[True].plot(kind='barh', figsize=(6, 6))\n\n# Add a title and possibly adjust other aspects like labels\nplt.title('What percent of journal abstracts start with a question?')\nplt.xlabel('')\nplt.ylabel('')\n\n# Format the x-axis as percentages\nax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n\n# Remove the top and right borders\nsns.despine()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\ndf['abstract_first_word'] =  df['abstract_first_sentence'].str.split().str[0]\ndf[df['abstract_starts_with_a_question']]['abstract_first_word'].value_counts()[:10]\n\nabstract_first_word\nHow     163\nWhat     65\nWhy      59\nDoes     31\nDo       28\nWhen     16\nIs       15\nAre      12\nCan      10\nThis      7\nName: count, dtype: int64\n\n\n\ndfa = df[df['abstract_starts_with_a_question']==True].copy()\n\n\n# Step 1: Prepare the Data\n# Categorize 'abstract_first_word' into \"How\", \"What\", \"Why\", and \"Other\"\ndfa['category'] = dfa['abstract_first_word'].apply(lambda x: x if x in ['How', 'What', 'Why'] else 'Other')\n\n# Create a crosstab/pivot table for the count or proportion of each category by 'Source title'\ndata_for_plot = pd.crosstab(dfa['Source title'], dfa['category'], normalize='index').sort_values(by='Why')\n\n# Step 2: Plot the Stacked Bar Plot\n# Set the Seaborn theme\nsns.set_theme(style=\"white\")\n\n# Plot\nax = data_for_plot.plot(kind='barh', stacked=True, figsize=(10, 6))\n\n# Add a title and adjust labels\nplt.xlabel('Percent')\nplt.ylabel('Source title')\n\n# Format the x-axis as percentages\nax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))\n\n# Remove the top and right borders\nsns.despine()\n\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/tts/tts.html",
    "href": "posts/tts/tts.html",
    "title": "Text to Speech using OpenAI’s API",
    "section": "",
    "text": "OpenAI has a pretty good overview of their text to speech API. It’s not free or impulsively cheap, like ChatGPT3.5, but, in my opinion, it is the best available model.\n\nfrom openai import OpenAI\n\nThis assumes that you have an OpenAi API key, and have stored it as an environment variable.\n\ndef stt(text, voice, mp3fn, model = \"tts-1-hd\"):\n    # A function to call the API and save it as an MP3\n    \n    client = OpenAI() # \n    response = client.audio.speech.create(\n        model = model, # model=\"tts-1\" is cheaper and pretty close in quality.\n        voice=voice,\n        input=text\n    )\n    \n    response.stream_to_file(mp3fn)\n\n\ngraph = (\n    \"Sociology is the last of the great sciences. It is only a little more than \"\n    \"a generation old, and, as yet, its principles are not quite definite. So that \"\n    \"among any large number of people who call themselves sociologists, one might \"\n    \"find as many shades of opinion as he would among the large number of persons \"\n    \"who call themselves Christians. Unlike biology, or astronomy, or mathematics, \"\n    \"there is as yet no definite set of fundamental principles upon which all \"\n    \"sociologists agree.\"\n) # cite: Wright, Richard R. 1911. \"The Negro Problem\" https://www.crisisopportunity.org/articles/negro_problem.html\n\n\nstt(graph, \"shimmer\", 'shimmer_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/4001769771.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\nNote: The DeprecationWarning is new. Something with OpenAI’s saving method isn’t playing nicely with notebooks.\nSample with the Shimmer voice\n\nfrom IPython.display import Audio\n\nAudio('shimmer_sample.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThe only other voices I like are Onyx and Echo.\n\nstt(graph, \"onyx\", 'onyx_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/4001769771.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\n\nAudio('onyx_sample.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nstt(graph, \"echo\", 'echo_sample.mp3')\nAudio('echo_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/3136648165.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThe HD model costs $0.030 per 1,000 characters, which works out to be about 18 cents for 1,000 words. The non-HD model costs half that.\nFor fun, I used ChatGPT to write an introduction to a W.E.B. DuBois short story, used TTS to produce audio versions of the intro and story using different voices, and then used pydub to splice the two.\nContent warning: The story contains racial epithets, which Du Bois often used when writing dialogue for white racists. Related, whatever content filters exist for ChatGPT don’t seem to be there for their TTS model.\n\nAudio('on_being_crazy_hd.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python snippets",
    "section": "",
    "text": "things I wish I knew 24 hours ago\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring text similarity with sentence embeddings\n\n\n\nsentence embeddings\n\n\n\nUsing Sentence-Transformers to measure the distance between a concept and a text.\n\n\n\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat percent of sociology journal abstracts start with a question?\n\n\n\nNLTK\n\n\n\nUsing NLTK to pull out the first sentence of an abstract.\n\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Synthetic Data with ChatGPT\n\n\n\nOpenAI\n\n\n\nWriting 600 fake article abstracts.\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping in Bulk\n\n\n\nWeb Scraping\n\n\n\nUsing pyppeteer to download websites in parallel\n\n\n\n\n\n\nFeb 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ChatGPT to Make Social Media Posts\n\n\n\nOpenAI\n\n\n\nMore using Pydantic for OpenAI function calling\n\n\n\n\n\n\nFeb 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Structured Data from ChatGPT\n\n\n\nOpenAI\n\n\n\nUsing Pydantic for OpenAI function calling\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAudio Transcription with Speaker Identification\n\n\n\nTranscription\n\n\n\nUsing WhisperpPlus to transcribe an audio file.\n\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText to Speech using OpenAI’s API\n\n\n\nText to Speech\n\n\nOpenAI\n\n\n\nWe are almost there with computer talking.\n\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]