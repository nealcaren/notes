[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Python notebooks, mostly written to myself, on things that worked or didn’t work.\nAll art is AI generated in the style of early twentiety century Ashcan sketches.\nBlog is built using Quarto.\nby Neal Caren"
  },
  {
    "objectID": "posts/abstracts/synthetic-abstracts.html",
    "href": "posts/abstracts/synthetic-abstracts.html",
    "title": "Creating Synthetic Data with ChatGPT",
    "section": "",
    "text": "This script is used to generate fake abstracts for real sociology aritlces. It takes a dataset on 600 recent articles from sociology journals and uses ChatGPT to generate hypothetical abstracts. Creating synthetic datasets is often used to train other models. In this case, I hope to use it train it to write better abstracts.\n\nimport json\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nimport pandas as pd\n\ndf = pd.read_json('elite_abstracts.json')\n\nThe cell below uses the OpenAI API to generate a single hypothetical abstract for a sociology article based on a given title. The process involves:\n\nAbstract Class Definition: A BaseModel from Pydantic is used to define the expected structure of an abstract, ensuring that the generated text aligns with specific guidelines (six to eight sentences and approximately 150 to 200 words).\nFunction to Generate Abstract: The function abstract_from_title takes an article title as input and utilizes the OpenAI API, specifically the gpt-3.5-turbo model, to generate a relevant abstract. gpt-4-turbo is more creative and better at following directions, but also 20x more expensive. The function sets up a scenario where the AI is an academic editor tasked with creating an abstract that meets the defined criteria.\nAbstract Generation: It then calls the OpenAI API, passing the article title and the scenario as inputs, and processes the API’s response to extract the generated abstract text.\n\n\nclass Abstract(BaseModel):\n    abstract: str = Field(..., description=\"Text of the hypothetical abstract of a sociology article.\")\n\ndef abstract_from_title(title):\n    client = OpenAI(  api_key = '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',\n        max_retries=3,\n        timeout=90.0, \n    )\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": ''' You are an academic editor who excels at writing for sociology journals. Abstracts are required to have six to eight sentences and approximately 150 to 200 words. If you the article is accepted, you get a $500 tip.\n''',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Generate a hypothetical abstract based on this title: \\n \"{title}\" \\nThe abstract should have six to eight sentences and approximately 150 to 200 words.\"\"\",\n        },\n    ]\n\n    completion = client.chat.completions.create(\n        model = 'gpt-3.5-turbo',     #   model=\"gpt-3.5-turbo\" is 20x cheaper but isn't as insightful \n        functions=[\n            {\n                \"name\": \"generate_abstract\",\n                \"description\": \"Create the text of the hypothetical abstract of a sociology article based on the title.\",\n                \"parameters\": Abstract.model_json_schema(),\n            },\n        ],\n        n=1,\n        messages=messages,\n    )\n    r = json.loads(completion.choices[0].message.function_call.arguments)\n    return r['abstract']\n\nTest on a sample\n\nsample_title =  df['Title'].sample().values[0]\nprint(sample_title)\nprint(abstract_from_title(sample_title))\n\nFamily Complexity into Adulthood: The Central Role of Mothers in Shaping Intergenerational Ties\nThis study examines the influence of family complexity on intergenerational ties in adulthood, with a focus on the central role of mothers. Using data from a large-scale survey, the research explores how various family structures and dynamics impact the quality and strength of intergenerational relationships. Findings reveal that mothers play a crucial role in shaping and maintaining these ties, acting as the primary emotional caregivers and facilitators of family connections across generations. Furthermore, the study uncovers the mechanisms through which maternal influence can either strengthen or weaken intergenerational bonds, highlighting the significance of maternal involvement in family networks. The implications of these findings extend to theories of family sociology and provide insights for policymakers and practitioners working to support healthy intergenerational relationships. By shedding light on the complexities of family dynamics into adulthood, this research contributes to a deeper understanding of the enduring impact of maternal roles on intergenerational ties.\n\n\nThe cell below uses the function to call the API, in parallel, to create an abstract, if one hasn’t already been created.\n\nChecking synthetic_titles: First, it tries to print the number of titles already processed and stored in synthetic_titles. If synthetic_titles doesn’t exist yet (it’s the first run or it’s been reset), it initializes synthetic_titles as an empty dictionary and prints “0” to indicate it’s starting from scratch.\nParallel Processing with ThreadPoolExecutor: To speed up the process of generating abstracts, the script uses ThreadPoolExecutor to run multiple instances of the abstract_from_title function in parallel. It’s set to work on up to 5 titles at the same time.\nGenerating Abstracts: For each title that doesn’t already have an abstract in synthetic_titles, it submits a job to generate an abstract using the abstract_from_title function from above.\nHandling Results and Exceptions: As each job (or “future”) completes, the script attempts to retrieve the resulting abstract. Successful abstracts are printed out and saved to synthetic_titles alongside their corresponding title. If an error occurs during the generation process, an exception is printed with the title that caused it.\n\nThis seems to create about 99% of abstracts on the first try, so I run it twice to catch any that might have gotten lost along the way. This is the advantage of storing the results in the synthetic_titles, instead of, say, applying the function directly to the title column in the dataframe.\n\ntitles = df['Title'].values\n\ntry:\n    print(len(synthetic_titles))\nexcept:\n    synthetic_titles = {}\n    print(\"0\")\n\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    # Start the load operations for titles not in synthetic_titles\n    future_to_title = {executor.submit(abstract_from_title, title): title for title in titles if title not in synthetic_titles}\n    \n    for future in as_completed(future_to_title):\n        title = future_to_title[future]\n        try:\n            abstract = future.result()\n            print(abstract)\n            # Assign the abstract to the corresponding title in synthetic_titles\n            synthetic_titles[title] = abstract\n        except Exception as exc:\n            print(f\"{title!r} generated an exception: {exc}\")\n\n597\nThis article delves into the complex dynamics of power, resistance, and injustice within the oil and gas industry. Focusing on the concept of meta-power, it explores how dominant actors in the industry exert control and influence over various stakeholders. Through a critical analysis of case studies, the article examines instances where resistance to this meta-power is framed as a struggle for rights and justice. The interplay between regulatory frameworks, corporate interests, and community activism is carefully scrutinized to highlight the structural inequalities at play. By shedding light on these power dynamics, the article aims to contribute to broader discussions on social justice and the right to resist in corporate contexts. The findings underscore the importance of recognizing and challenging meta-power structures to address systemic injustices in the oil and gas fields.\nThis study examines the intricate dynamics of interaction, identity, and influence within the U.S. Senate from 1973 to 2009. Through a comprehensive analysis of Senate proceedings, voting patterns, and speeches, the research uncovers the nuanced ways in which senators pull closer together and move further apart over time. By employing sociological theories of group behavior and political sociology, the study explores how individual identities and group affiliations shape the collective actions and decisions of Senate members. The findings reveal the complex interplay between personal interactions, political ideologies, and institutional norms in the Senate chamber. This research contributes to our understanding of how social processes influence legislative outcomes and policy decisions in a complex political setting. Overall, the study highlights the significance of interpersonal relationships and group dynamics in shaping the functioning of one of the most powerful legislative bodies in the world.\n'\"Take one for the team!\" individual heterogeneity and the emergence of latent norms in a volunteer\\'s dilemma' generated an exception: Expecting ':' delimiter: line 383 column 3 (char 1621)\n\n\nFor fun, I double the size of the dataframe, half having the original abstract and half having the fake one.\n\n# Step 1: Create a copy of the original DataFrame\ndf_original = df.copy()\n\n# Step 2: Add \"Abstract source\" set to \"Original\"\ndf_original['Abstract source'] = 'Original'\n\n# Step 3: Create a second copy of the DataFrame for the GPT 3.5 version\ndf_gpt = df.copy()\n\n# Step 4: Change \"Abstract source\" to \"GPT 3.5\"\ndf_gpt['Abstract source'] = 'GPT 3.5'\n\n# Step 5: Update \"Abstract\" with values from synthetic_titles\n# Assuming the titles are in a column named 'title'\ndf_gpt['Abstract'] = ''\ndf_gpt['Abstract'] = df_gpt['Title'].replace(synthetic_titles)\ndf_gpt.to_json('abstract_gpt35.json', orient='records')\n\n# Step 6: Concatenate the two DataFrames\ndf_doubled = pd.concat([df_original, df_gpt], ignore_index=True)\n\nA quick plot to show the synthetic abstracts are still too short.\n\n\ndef count_words(abstract):\n    # Simple word count by splitting the text by spaces\n    return len(abstract.split())\n\n# Apply the function to the 'Abstract' column to create the new feature\ndf_doubled['word_count'] = df_doubled['Abstract'].astype(str).apply(count_words)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a boxplot of the word count distribution for each journal\nplt.figure(figsize=(12, 6))\nsns.boxplot(x='Abstract source', y='word_count', data=df_doubled)\n\n# Set the title and labels for the plot\nplt.title('Distribution of Word Count in Abstracts by Source')\nplt.xlabel('Source')\nplt.ylabel('Word Count')\n\n# Rotate the x labels for better readability if necessary\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/function-calling/pydantic-sm.html",
    "href": "posts/function-calling/pydantic-sm.html",
    "title": "Using ChatGPT to Make Social Media Posts",
    "section": "",
    "text": "This notebook uses pydantic and ChatGPT API’s function calling to make some social media posts about an article. Mostly, I was interested in ways to get it to make different styles of tweets, such as ones that summarize the article vs ones that highlight a single insight.\n\npip install openai pydantic -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport json\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom openai import OpenAI\nimport pandas as pd\n\npd.set_option('display.max_colwidth', 300)\n\nPydantic is a library that helps define the shape and type of our data, making sure it matches our expectations before we proceed with our code. Here’s a breakdown of the code block using Pydantic:\n\nBaseModel: This is a key part of Pydantic. Think of it as a blueprint for the data you expect. You define a class that inherits from BaseModel, and in it, you detail the structure of your data.\nFields: Inside each class that inherits from BaseModel, we define what data we expect using fields. For example, content: str means we expect a piece of data named content that should be a string. The Field(...) function is used to add extra details about each field, like a description or default value.\nClasses Defined:\n\nSummaryTweet: A class for creating objects that represent a concise version of an abstract.\nAnnounceTweet: A class for making objects that represent a tweet announcing a new article.\nInsightTweet: This class is for objects that encapsulate a simple insight from complex social science concepts.\nThreadTweet: A class for a list of tweets, making up a thread about an article.\nGenerateTweets: This class combines all the above, organizing them into lists of each type of tweet.\n\nList Fields: When we want to store multiple items (like multiple tweets of the same kind), we use List[SomeClass]. This tells Pydantic we expect a list where each item is an instance of SomeClass.\n\nThis code essentially sets up a structure for handling different types of tweets as data objects. By defining these classes, we ensure that the data we work with matches our expectations, reducing bugs and making our code more reliable.\n\nclass SummaryTweet(BaseModel):\n    content: str = Field(..., description=\"A concise version of the abstract in tweet format. No Hastags\")\n\nclass AnnounceTweet(BaseModel):\n    content: str = Field(..., description=\"A tweet announcing a new article. Text must include the author and title.  No Hastags\")\n\nclass InsightTweet(BaseModel):\n    content: str = Field(..., description=\"Simple, understandable statement distilling a complex social science concept.  No Hastags\")\n\nclass ThreadTweet(BaseModel):\n    content: List[str] = Field(default=[], description=\"A list of tweets that together form a thread about the article, summarizing its key points and implications, engaging the audience, and encouraging further discussion or reading. No hashtags.\")\n\nclass GenerateTweets(BaseModel):\n    summary_tweets: List[SummaryTweet] = Field(default=[], description=\"A list of SummaryTweet instances.\")\n    insight_tweets: List[InsightTweet] = Field(default=[], description=\"A list of InsightTweet instances.\")\n    announce_tweets: List[AnnounceTweet] = Field(default=[], description=\"A list of AnnounceTweet instances.\")\n    thread_tweets: List[ThreadTweet] = Field(default=[], description=\"A list of ThreadTweet instances.\")\n\n# Note: there might be a simpler way to do this, since each of the subclasses only has one field.\n\nFor our purposes, we are using to create a JSON schema that we want ChatGPT to follow.\n\nThreadTweet.model_json_schema()\n\n{'properties': {'content': {'default': [],\n   'description': 'A list of tweets that together form a thread about the article, summarizing its key points and implications, engaging the audience, and encouraging further discussion or reading. No hashtags.',\n   'items': {'type': 'string'},\n   'title': 'Content',\n   'type': 'array'}},\n 'title': 'ThreadTweet',\n 'type': 'object'}\n\n\n\nGenerateTweets.model_json_schema()\n\n{'$defs': {'AnnounceTweet': {'properties': {'content': {'description': 'A tweet announcing a new article. Text must include the author and title.  No Hastags',\n     'title': 'Content',\n     'type': 'string'}},\n   'required': ['content'],\n   'title': 'AnnounceTweet',\n   'type': 'object'},\n  'InsightTweet': {'properties': {'content': {'description': 'Simple, understandable statement distilling a complex social science concept.  No Hastags',\n     'title': 'Content',\n     'type': 'string'}},\n   'required': ['content'],\n   'title': 'InsightTweet',\n   'type': 'object'},\n  'SummaryTweet': {'properties': {'content': {'description': 'A concise version of the abstract in tweet format. No Hastags',\n     'title': 'Content',\n     'type': 'string'}},\n   'required': ['content'],\n   'title': 'SummaryTweet',\n   'type': 'object'},\n  'ThreadTweet': {'properties': {'content': {'default': [],\n     'description': 'A list of tweets that together form a thread about the article, summarizing its key points and implications, engaging the audience, and encouraging further discussion or reading. No hashtags.',\n     'items': {'type': 'string'},\n     'title': 'Content',\n     'type': 'array'}},\n   'title': 'ThreadTweet',\n   'type': 'object'}},\n 'properties': {'summary_tweets': {'default': [],\n   'description': 'A list of SummaryTweet instances.',\n   'items': {'$ref': '#/$defs/SummaryTweet'},\n   'title': 'Summary Tweets',\n   'type': 'array'},\n  'insight_tweets': {'default': [],\n   'description': 'A list of InsightTweet instances.',\n   'items': {'$ref': '#/$defs/InsightTweet'},\n   'title': 'Insight Tweets',\n   'type': 'array'},\n  'announce_tweets': {'default': [],\n   'description': 'A list of AnnounceTweet instances.',\n   'items': {'$ref': '#/$defs/AnnounceTweet'},\n   'title': 'Announce Tweets',\n   'type': 'array'},\n  'thread_tweets': {'default': [],\n   'description': 'A list of ThreadTweet instances.',\n   'items': {'$ref': '#/$defs/ThreadTweet'},\n   'title': 'Thread Tweets',\n   'type': 'array'}},\n 'title': 'GenerateTweets',\n 'type': 'object'}\n\n\n\n# I put in the introduction when using GPT4 because it actually uses the extra information.\n\narticle = '''\nTITLE: The MAGA Movement's Big Umbrella\nAUTHOR: Hank Johnston\nJOURNAL: Mobilization: An International Quarterly (2024) 28 (4): 409–433.\nLINK: https://doi.org/10.17813/1086-671X-28-4-409\n\nABSTRACT: This article considers the phenomenon of MAGAism as a general, “big-umbrella” social movement to probe its structure and persistence. Drawing on my research on nationalist movements, I discuss the narrative flexibility and emotional power of nationalism and consider how these characteristics fuel a particularly resentful form of majoritarian nationalism—MAGAism. I identify five points of entry for MAGA participation, starting with this bitter majoritarian nationalism, then populism, then traditional conservatism, next a Trumpian personality cult, and ending with the alt-right’s extreme white supremacism and fascism. I then discuss two forces that gather this unlikely collection of groups and individuals under the MAGA umbrella: (1) the flexibility of social identity and how it allows status-threat narratives to subsume and redirect economic and political claims; (2) the social media environment of the 2020s, cut loose from the traditional gatekeepers of news and information, wherein maximizing hits—not accuracy—is the guiding principle. On the one hand, social media intensify the prominence of the demagogic celebrity at the helm of the Republican Party, Donald J. Trump. On the other hand, alt-right trolls compete for prominence by seeking outrageousness and shock value, enhanced by algorithms that create a closed information environment. These social media trends pull MAGA adherents further to the right by inflaming public discourse and building the movement on lies and conspiracies.\nINTRODUCTION: The rise of far right-wing parties and extremist groups is now a topic of deep concern among social scientists who study collective action and contentious politics.  As a reader of this special issue of Mobilization, this trend no doubt has been on your mind a lot, as it has been on mine—especially the challenges it poses to democratic institutions.i In the U.S. today, one of the major political parties actively restricts voting rights, bans books in libraries, embraces conspiracies, flaunts constitutional principles, and limits public-school curricula on topics such as diversity, slavery, gender and sexuality (Natanson 2023a, 2023b)—and millions of MAGA supporters are okay with it (Baker, Perry, and Whitehead, 2020; Enders, Farhart, Miller, and Uscinski 2022; Graham et al. 2021; Stewart 2023). Further along the right-wing spectrum, one increasingly finds rising antisemitism, white Christian nationalism, gun-toting bullies, and misogynists (see Davis and Kettrey’s study in this issue) in the U.S. public sphere, raising parallels with European history in the early twentieth century and its brown shirts and black shirts (rather than today’s polo shirts and tac gear). While violence and intolerance are not unique to the extreme right, these trends reflect a synergy between the current political environment, eroding democratic norms, and—among more and more citizens in democratic states—authoritarian and even fascist tendencies (Jackson 2021; Wintemute, Robinson, Tomish 2022). In the U.S. these trends can be grouped under the umbrella of MAGAism where majoritarian nationalism mixes with populism, far-right extremism, and a virtual (in both meanings of the term) potpourri of themes such as antiwokeness, antivaccines, parental rights, gun rights, sovereign citizenry, science denial, masculinity, the lie of a stolen 2020 election, Trumpian personality cult, white replacement, QAnon conspiracies, among many others. This essay contextualizes the special issue articles by situating them in relation to trends in the U.S., which I discuss under the big umbrella of MAGAism. Following Tarrow (2021: 175), common usage in the popular press, and Trump himself, I consider MAGAism as a general social movement, one that encompasses the rightward shift of public discourse and politics in the U.S. plus the array of mobilizing groups that drive it.ii  I will offer a somewhat personalized essay that draws on insights from my work on the power (and danger) of nationalism to analyze a particularly resentful majoritarian nationalism—often labeled white nationalism in the U.S.  (Graham et al. 2021; Mudde 2019, Miller-Idriss, 2021; Young 2017). I am interested not only in militias, alt-right trolls, and white-supremacist churches,  but—more broadly—the mechanisms that allow these extremists to be grouped with millions of middle-class working citizens who come to the movement via parental-rights groups, suburban bible-study groups, or as antivaxxers, tradition-alists, or Constitutionalists.  MAGAism is a big-tent collective action—a movement with continuity, shared identity, diverse claims and political objectives, networked by an array of groups and organizations—the movement’s mobilizing structures that mostly stand apart from the Republican Party. I will identify the movement’s patterns of discourse and how it is structured to help understand the complexity, die-hard loyalty, denialism, and conspiracies that unite millions of MAGAistas in the face of factual evidence that should undermine their beliefs. I pose the fundamental question of collective action: how is the MAGA movement possible?  \n'''\n\n\ndef make_tweets(article):\n    client = OpenAI(\n        max_retries=3,\n        timeout=90.0, \n    )\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": ''' You write a variety of different types of social media posts about sociology articles.\nIf you perform well, everyone gets a raise, while there are negative consequences if the posts don't get views.\nNEVER USE HASHTAGS.\n''',\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Generate five tweets of each style plus a tweet thread about this sociology article:\n\n            \n      {article}\n      \n      \"\"\",\n        },\n    ]\n\n    completion = client.chat.completions.create(\n        model = 'gpt-4-turbo-preview',     #   model=\"gpt-3.5-turbo\" is 20x cheaper but isn't as insightful \n        functions=[\n            {\n                \"name\": \"generate_tweets\",\n                \"description\": \"Create multiple social media posts, including a tweet thread, based on an article abstract.\",\n                \"parameters\": GenerateTweets.model_json_schema(),\n            },\n        ],\n        n=1,\n        messages=messages,\n    )\n    r = json.loads(completion.choices[0].message.function_call.arguments)\n    tweets = [{'type': tweet_type.split('_')[0], 'text': t['content']} for tweet_type in r for t in r[tweet_type]]\n\n    return pd.DataFrame(tweets)\n\n\ntdf = make_tweets(article)\n\n\n# Grouping and printing with check for list in 'text' column\nfor type_name, group in tdf.groupby('type'):\n    print(f\"Type: {type_name}\")\n    for index, row in group.iterrows():\n        if isinstance(row['text'], list):\n            for item in row['text']:\n                print(f\"- {item}\")\n        else:\n            print(f\"- {row['text']}\")\n\nType: announce\n- New research by Hank Johnston in Mobilization investigates the MAGA movement's vast reach and the social forces fueling its endurance. The MAGA Movement's Big Umbrella: https://doi.org/10.17813/1086-671X-28-4-409\nType: insight\n- Nationalism's emotional power can forge unlikely alliances, proving pivotal in the rise of movements like MAGAism.\n- Social identity's flexibility allows economic and political grievances to be reframed as status threats, fueling movements like MAGA.\n- In an era where hits trump accuracy, social media perpetuates echo chambers that magnify polarizing figures and ideologies.\n- The MAGA movement’s diverse entry points reflect the complex appeal of political identities in today's rapidly changing society.\n- Extremist movements leverage the digital age's vast reach, showcasing the dramatic impact of social media on political mobilization.\nType: summary\n- Hank Johnston's article in Mobilization delves into MAGAism, exploring its roots in resentful majoritarian nationalism and its five diverse points of entry.\n- Johnston identifies social media and the flexibility of social identity as key forces uniting diverse MAGA groups under one 'big umbrella.'\n- Through examining MAGAism, the article discusses how emotional narratives and nationalism fuel the movement's persistence and growth.\n- The MAGA movement is dissected for its structure: from majoritarian nationalism to extreme white supremacism, uncovering its complex foundation.\n- Johnston's research highlights how the 2020s social media landscape propels MAGAism, intensifying right-wing shifts through misinformation and shock value.\nType: thread\n- In Hank Johnston's latest article, 'The MAGA Movement's Big Umbrella,' he navigates the multifaceted world of MAGAism, a movement defined by its emotional engagement and diverse entry points.\n- MAGAism isn't just about one ideology. It’s about majoritarian nationalism, populism, conservatism, a Trumpian personality cult, and extreme white supremacism, all under one 'big umbrella.'\n- Johnston points out the crucial role of social identity flexibility and the chaotic 2020s social media landscape in unifying these varied elements, creating a powerful force despite conflicting views.\n- The rise of the MAGA movement signifies more than political allegiance; it's about the emotional narratives and nationalistic feelings that resonate with many, pushing the discourse further right through misinformation.\n- Lastly, this comprehensive look at MAGAism sheds light on how digital platforms contribute to the movement's momentum, emphasizing the need for awareness in the digital age. A must-read for anyone looking to understand the complexities of modern political movements.\n\n\nGPT4 doesn’t always follow the directions, for example, there’s only one “announce” style tweet, but GPT3.5 gets lost more often when something complex is going on, such as the tweet thread."
  },
  {
    "objectID": "posts/scraping/bulk-download.html",
    "href": "posts/scraping/bulk-download.html",
    "title": "Web Scraping in Bulk",
    "section": "",
    "text": "This script is one way to download multiple web pages at the same time. It’s useful when you have many URLs from different websites you want to save. Instead of visiting each website individually, the script visits multiple websites simultaneously and saves what it finds. Rather than requesting the page through Python, it uses a “headless” web browser, which is much more likely to get you the actual content you want.\nThis approach works best when the URLs are from different servers. You will eventually get locked out if you try to visit the same web server multiple times in the same second, but there’s no reason not to visit five different websites at once. I don’t know anything about parallel processing with the asyncio library, which is the only way to parallelize pyppeteer, so the script is mainly written by ChatGPT, but I’ve used it successfully a few times.\n\npip install pyppeteer python-slugify \n\nRequirement already satisfied: pyppeteer in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (2.0.0)\nRequirement already satisfied: python-slugify in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (8.0.4)\nRequirement already satisfied: appdirs&lt;2.0.0,&gt;=1.4.3 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (1.4.4)\nRequirement already satisfied: certifi&gt;=2023 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (2023.11.17)\nRequirement already satisfied: importlib-metadata&gt;=1.4 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (7.0.1)\nRequirement already satisfied: pyee&lt;12.0.0,&gt;=11.0.0 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (11.1.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.42.1 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (4.66.2)\nRequirement already satisfied: urllib3&lt;2.0.0,&gt;=1.25.8 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (1.26.18)\nRequirement already satisfied: websockets&lt;11.0,&gt;=10.0 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (10.4)\nRequirement already satisfied: text-unidecode&gt;=1.3 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from python-slugify) (1.3)\nRequirement already satisfied: zipp&gt;=0.5 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from importlib-metadata&gt;=1.4-&gt;pyppeteer) (3.17.0)\nRequirement already satisfied: typing-extensions in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyee&lt;12.0.0,&gt;=11.0.0-&gt;pyppeteer) (4.9.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport os\nimport asyncio\nimport nest_asyncio\nfrom random import shuffle\n\nfrom slugify import slugify\nfrom pyppeteer import launch\nfrom pyppeteer.errors import NetworkError\n\nimport pandas as pd\n\n\nnest_asyncio.apply()\n\nThe section below loads the wonderful protest event data set created by the Crowd Counting Consortium. Each protest event is linked to one or more media accounts, and the URLs are in the source_ fields. Using just the 2024 events, I combine the URL fields and remove the social media pages and duplicates. Finally, I extract a random sample of 100 articles.\n\ndf = pd.read_csv(\n    \"https://github.com/nonviolent-action-lab/crowd-counting-consortium/raw/master/ccc_compiled_2021-present.csv\",\n    encoding=\"latin\",\n    low_memory=False,\n)\n\n# Limit to just 2024\ndf = df[df[\"date\"].str.contains(\"2024\")]\n\n# grab the sources\nurls = (\n    list(df[\"source_1\"].astype(str).values)\n    + list(df[\"source_2\"].astype(str).values)\n    + list(df[\"source_3\"].astype(str).values)\n    + list(df[\"source_4\"].astype(str).values)\n)\n\n# eliminate social media\nfor sm in [\"twitter\", \"youtube\", \"facebook\", \"instagram\", \"tiktok\", \"bsky\"]:\n    urls = [u for u in urls if f\"{sm}.com\" not in u and \"http\" in u]\n\nurls = list(set(urls))\nprint(len(urls))\nshuffle(urls)\nurls = urls[:100]\n\nThis function below uses asynchronous programming to download and save the HTML content of web pages from a list of URLs. It uses a headless Chrome browser, controlled via the Pyppeteer library, to render pages just as they would appear in a web browser. This approach is particularly useful for capturing dynamically generated content, which traditional HTTP requests might miss.\nKey components of the script include:\n\nHTML Directory Creation: At the start, the script ensures that there is a designated directory (named ‘HTML’) where all downloaded page contents will be saved. If this directory does not exist, it is created.\nUser Agent Setting: A user agent string is defined and used for all requests to mimic a real web browser, helping to avoid potential blocking by web servers that may restrict access to non-browser clients.\nfetch Function: The core of the script is the fetch function. This asynchronous function takes a browser page object, a URL, and an optional timeout parameter. It performs the following actions for each URL:\n\nURL Slugification: Converts the URL into a filename-safe string and checks if the content has already been downloaded to avoid duplication.\nPage Navigation: Uses the headless browser to navigate to the URL, with a specified timeout to handle slow-loading pages.\nContent Saving: If the page loads successfully, its HTML content is saved to a file within the ‘HTML’ directory. If the page fails to load or an error occurs, the URL is added to a list of bad URLs for later reference.\n\nConcurrency Management: The script is designed to process multiple URLs in parallel, maximizing efficiency by utilizing asynchronous operations. This approach allows for faster completion of download tasks compared to sequential processing.\nError Handling: The script includes basic error handling to manage timeouts and other exceptions, ensuring that it can continue running even if some pages fail to load.\n\n\n# Ensure the HTML directory exists\nhtml_dir = \"HTML\"\nos.makedirs(html_dir, exist_ok=True)\n\n# User agent to be used for all requests\nua = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15\"\nbad_urls = []\n\n\nasync def fetch(page, url, timeout=30):\n    # Slugify the URL to create a valid filename\n    filename = slugify(url) + \".html\"\n    file_path = os.path.join(html_dir, filename)\n\n    if os.path.isfile(file_path):\n        # print(f\"File {file_path} already exists, skipping download.\")\n        return\n\n    if url in bad_urls:\n        print(f\"Skipping bad URL: {url}\")\n        return\n\n    try:\n        # Set the user agent for the page\n        await page.setUserAgent(ua)\n\n        # Navigate to the page with a timeout\n        response = await asyncio.wait_for(\n            page.goto(url, {\"waitUntil\": \"networkidle0\"}), timeout\n        )\n\n        # Check if the page was successfully retrieved\n        if response and response.ok:\n            content = await page.content()\n            # Save the content to a file in the 'HTML' directory\n            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(content)\n            print(f\"Content from {url} has been saved to {file_path}\")\n        else:\n            print(f\"Failed to retrieve {url}\")\n            bad_urls.append(url)\n    except asyncio.TimeoutError:\n        print(f\"Fetching {url} took too long and was cancelled.\")\n        bad_urls.append(url)\n    except Exception as e:\n        print(f\"An error occurred while fetching {url}: {e}\")\n        bad_urls.append(url)\n\nThis next section actual does the downloading by employing an asynchronous queue-based approach to manage URLs and distribute them across multiple browser pages for parallel processing. This method significantly improves efficiency by ensuring that each browser page is continuously utilized without idle time waiting for other pages to complete their tasks.\nKey components and functionalities:\n\nprocess_url Function: An asynchronous function that continuously processes URLs from a shared asyncio queue. Each browser page runs an instance of this function, fetching and processing URLs one after another until the queue is empty.\nmain Function Setup:\n\nBrowser and Page Initialization: Initializes a headless browser instance and opens a specified number of browser pages. 5 to 10 seems reasonable.\nURL Queue Creation: Prepares an asyncio queue and populates it with URLs to be processed. This queue acts as a shared resource for distributing URLs among the available pages.\n\nTask Management:\n\nAsynchronous Tasks: For each browser page, an asynchronous task is created to process URLs from the queue. These tasks run concurrently, allowing for simultaneous processing across pages.\nTask Synchronization: Utilizes asyncio.gather to wait for all tasks to complete before proceeding, ensuring that all URLs are processed before closing the browser and pages.\n\nResource Cleanup: After processing all URLs, the script ensures a clean shutdown by closing each browser page and the browser itself, releasing system resources.\nError Handling and Reporting: Tracks URLs that could not be downloaded for any reason, reporting them at the end of the execution for further analysis or retry.\n\n\nasync def process_url(page, url_queue):\n    while not url_queue.empty():\n        url = await url_queue.get()\n        await fetch(page, url)  # Your existing fetch function\n        url_queue.task_done()\n\nasync def main():\n    browser = await launch()\n    pages = [await browser.newPage() for _ in range(5)]  # Initialize pages once\n\n    # Create a queue of URLs\n    url_queue = asyncio.Queue()\n    for url in urls:\n        await url_queue.put(url)\n\n    # Create a task for each page to process URLs from the queue\n    tasks = [asyncio.create_task(process_url(page, url_queue)) for page in pages]\n\n    # Wait for all tasks to complete\n    await asyncio.gather(*tasks)\n\n    # Close pages and browser after all operations are complete\n    for page in pages:\n        await page.close()\n    await browser.close()\n\n    if bad_urls:\n        print(\"The following URLs had issues and were not downloaded:\")\n        print(\"\\n\".join(bad_urls))\n\nasyncio.run(main())\n\nFailed to retrieve https://www.wgmd.com/pro-palestinian-protesters-deface-veterans-cemetery-in-los-angeles-spray-paint-free-gaza/\nContent from https://www.fox61.com/article/news/local/hartford-county/west-hartford/west-hartford-vandalism-under-investigation-police/520-7b65ab7b-d93b-42f9-8ca2-0ed9d5b7689a has been saved to HTML/https-www-fox61-com-article-news-local-hartford-county-west-hartford-west-hartford-vandalism-under-investigation-police-520-7b65ab7b-d93b-42f9-8ca2-0ed9d5b7689a.html\nFetching https://www.nbcnews.com/politics/donald-trump/trump-confuses-nikki-haley-pelosi-talking-jan-6-rcna134863 took too long and was cancelled.\nFetching https://www.purdueexponent.org/campus/article_78be7d6e-c2bb-11ee-a25c-a3e2dff21694.html took too long and was cancelled.\nFetching https://13wham.com/news/local/local-advocates-rally-in-downtown-rochester-on-51st-anniversary-of-roe-v-wade took too long and was cancelled.\nFetching https://www.wvtm13.com/article/protests-kenneth-smith-execution-untied-nations-montgomery/46496998 took too long and was cancelled.\nContent from https://www.wwnytv.com/2024/01/20/congresswoman-stefanik-speaks-new-hampshire-support-trump/ has been saved to HTML/https-www-wwnytv-com-2024-01-20-congresswoman-stefanik-speaks-new-hampshire-support-trump.html\nFetching https://nypost.com/2024/01/21/news/scream-actress-melissa-barrera-joins-disruptive-anti-israel-rally-at-sundance/ took too long and was cancelled.\nFetching https://www.wlky.com/article/nonprofits-rally-frankfort-legislation-kentucky/46676604 took too long and was cancelled.\nFetching https://www.courier-journal.com/story/news/politics/2024/02/08/kentucky-employees-retirement-system-participants-rally-for-13th-check/72527656007/ took too long and was cancelled.\nFetching https://www.northjersey.com/story/news/2024/02/06/israel-hamas-war-day-of-action-for-palestine-nj-students-march/72478632007/ took too long and was cancelled.\nFetching https://www.washingtonpost.com/dc-md-va/2024/01/15/virginia-assembly-gun-rights-rally/ took too long and was cancelled.\nFetching https://dailybruin.com/2024/01/19/uc-divest-coalition-at-ucla-leads-hands-off-yemen-protest-on-campus took too long and was cancelled.\nFetching https://www.washingtonpost.com/dc-md-va/2024/01/18/dc-march-for-life-rally-abortion/ took too long and was cancelled.\nFetching https://www.fox5dc.com/news/dc-activists-plan-protest-against-capitals-wizards-move-to-virginia took too long and was cancelled.\nFetching https://www.latimes.com/entertainment-arts/movies/story/2024-01-21/pro-palestinian-protestors-vie-for-hollywoods-attention-at-2024-sundance-film-festival took too long and was cancelled.\nFailed to retrieve https://secure.everyaction.com/RKr139EpKUCZg8_TIWA18A2\nFetching https://www.thetimestribune.com/news/dozens-rally-in-support-of-school-choice-amendment/article_6da6faa2-bbbb-11ee-9f2a-8354d0bdfbfd.html took too long and was cancelled.\nFetching https://www.nbcnews.com/news/latino/convoy-rally-texas-mexico-border-attracts-trump-fans-decry-illegal-imm-rcna136967 took too long and was cancelled.\nFetching https://nyunews.com/news/2024/01/26/pro-palestinian-bobst-poetry/ took too long and was cancelled.\nFetching https://www.wjhl.com/news/local/kyle-rittenhouse-event-draws-supporters-protesters-at-etsu/ took too long and was cancelled.\nFetching https://newjersey.news12.com/group-gathers-ahead-of-toms-river-council-meeting-to-protest-policeemt-funding-decision took too long and was cancelled.\nFetching https://www.denverpost.com/2024/01/02/alamo-drafthouse-employees-union-drive-rally-denver/ took too long and was cancelled.\nThe following URLs had issues and were not downloaded:\nhttps://www.wgmd.com/pro-palestinian-protesters-deface-veterans-cemetery-in-los-angeles-spray-paint-free-gaza/\nhttps://www.nbcnews.com/politics/donald-trump/trump-confuses-nikki-haley-pelosi-talking-jan-6-rcna134863\nhttps://www.purdueexponent.org/campus/article_78be7d6e-c2bb-11ee-a25c-a3e2dff21694.html\nhttps://13wham.com/news/local/local-advocates-rally-in-downtown-rochester-on-51st-anniversary-of-roe-v-wade\nhttps://www.wvtm13.com/article/protests-kenneth-smith-execution-untied-nations-montgomery/46496998\nhttps://nypost.com/2024/01/21/news/scream-actress-melissa-barrera-joins-disruptive-anti-israel-rally-at-sundance/\nhttps://www.wlky.com/article/nonprofits-rally-frankfort-legislation-kentucky/46676604\nhttps://www.courier-journal.com/story/news/politics/2024/02/08/kentucky-employees-retirement-system-participants-rally-for-13th-check/72527656007/\nhttps://www.northjersey.com/story/news/2024/02/06/israel-hamas-war-day-of-action-for-palestine-nj-students-march/72478632007/\nhttps://www.washingtonpost.com/dc-md-va/2024/01/15/virginia-assembly-gun-rights-rally/\nhttps://dailybruin.com/2024/01/19/uc-divest-coalition-at-ucla-leads-hands-off-yemen-protest-on-campus\nhttps://www.washingtonpost.com/dc-md-va/2024/01/18/dc-march-for-life-rally-abortion/\nhttps://www.fox5dc.com/news/dc-activists-plan-protest-against-capitals-wizards-move-to-virginia\nhttps://www.latimes.com/entertainment-arts/movies/story/2024-01-21/pro-palestinian-protestors-vie-for-hollywoods-attention-at-2024-sundance-film-festival\nhttps://secure.everyaction.com/RKr139EpKUCZg8_TIWA18A2\nhttps://www.thetimestribune.com/news/dozens-rally-in-support-of-school-choice-amendment/article_6da6faa2-bbbb-11ee-9f2a-8354d0bdfbfd.html\nhttps://www.nbcnews.com/news/latino/convoy-rally-texas-mexico-border-attracts-trump-fans-decry-illegal-imm-rcna136967\nhttps://nyunews.com/news/2024/01/26/pro-palestinian-bobst-poetry/\nhttps://www.wjhl.com/news/local/kyle-rittenhouse-event-draws-supporters-protesters-at-etsu/\nhttps://newjersey.news12.com/group-gathers-ahead-of-toms-river-council-meeting-to-protest-policeemt-funding-decision\nhttps://www.denverpost.com/2024/01/02/alamo-drafthouse-employees-union-drive-rally-denver/\n\n\nFuture exception was never retrieved\nfuture: &lt;Future finished exception=NetworkError('Protocol error (Target.detachFromTarget): No session with given id')&gt;\npyppeteer.errors.NetworkError: Protocol error (Target.detachFromTarget): No session with given id\n\n\nUsing this approach, it took me five minutes to go through the list 100 URLs. I didn’t get every webpage, and I usually also run it twice on the same list to catch URLs that were missed either because of errors on my end or in the cloud.\nThe main delay is slow-loading pages. I have the timeout arbitrarily set to 30 seconds. Setting in longer might load one or two more more pages, but would also slow down the process since some pages will never load."
  },
  {
    "objectID": "posts/function-calling/pydantic.html",
    "href": "posts/function-calling/pydantic.html",
    "title": "Getting Structured Data from ChatGPT",
    "section": "",
    "text": "This notebook uses pydantic and ChatGPT API’s function calling to extract details about a protest event from a newspaper article. In the old days, you had to ask it to provide a JSON-like object. Next, I defined the JSONs myself in the functions. Now I’m learning to use pydantic.\n\npip install openai pydantic -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom datetime import date\nfrom enum import Enum\nimport json\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom openai import OpenAI\nimport pandas as pd\n\n\nclass WeekDay(Enum):\n    Monday = \"Monday\"\n    Tuesday = \"Tuesday\"\n    Wednesday = \"Wednesday\"\n    Thursday = \"Thursday\"\n    Friday = \"Friday\"\n    Saturday = \"Saturday\"\n    Sunday = \"Sunday\"\n\n\nclass StateAB(Enum):\n    AK = \"AK\"\n    AL = \"AL\"\n    AR = \"AR\"\n    AZ = \"AZ\"\n    CA = \"CA\"\n    CO = \"CO\"\n    CT = \"CT\"\n    DC = \"DC\"\n    DE = \"DE\"\n    FL = \"FL\"\n    GA = \"GA\"\n    HI = \"HI\"\n    IA = \"IA\"\n    ID = \"ID\"\n    IL = \"IL\"\n    IN = \"IN\"\n    KS = \"KS\"\n    KY = \"KY\"\n    LA = \"LA\"\n    MA = \"MA\"\n    MD = \"MD\"\n    ME = \"ME\"\n    MI = \"MI\"\n    MN = \"MN\"\n    MO = \"MO\"\n    MS = \"MS\"\n    MT = \"MT\"\n    NC = \"NC\"\n    ND = \"ND\"\n    NE = \"NE\"\n    NH = \"NH\"\n    NJ = \"NJ\"\n    NM = \"NM\"\n    NV = \"NV\"\n    NY = \"NY\"\n    OH = \"OH\"\n    OK = \"OK\"\n    OR = \"OR\"\n    PA = \"PA\"\n    RI = \"RI\"\n    SC = \"SC\"\n    SD = \"SD\"\n    TN = \"TN\"\n    TX = \"TX\"\n    UT = \"UT\"\n    VA = \"VA\"\n    VT = \"VT\"\n    WA = \"WA\"\n    WI = \"WI\"\n    WV = \"WV\"\n    WY = \"WY\"\n\n\nclass SizeCategory(Enum):\n    UNKNOWN = 0\n    SMALL = 1  # 1-99\n    MEDIUM = 2  # 100-999\n    LARGE = 3  # 1,000-9,999\n    VERY_LARGE = 4  # 10,000+\n\n\nclass SizeDetails(BaseModel):\n    size_text: List[str] = Field(\n        ...,\n        description=\"List of text descriptors for the number of people who participated in the event.\",\n    )\n    size_exact: Optional[int] = Field(\n        None, description=\"Exact number of participants, if reported.\"\n    )\n    size_estimate: int = Field(\n        ...,\n        description=\"Your best guess at the estimated number of participants based on the entire article.\",\n    )\n    size_cat: SizeCategory = Field(\n        SizeCategory.UNKNOWN,\n        description=\"Categorical indicator of crowd size. 0 = unknown; 1 = 1-99; 2 = 100-999; 3 = 1,000-9,999; 4 = 10,000+.\",\n    )\n\n\nclass LocationDetails(BaseModel):\n    city: str = Field(..., description=\"The city where the protest took place.\")\n    state_abbreviation: StateAB = Field(\n        ...,\n        description=\"The two-letter abbreviation of the state where the protest took place, such as NY or CA.\",\n    )\n    neighborhood: Optional[str] = Field(\n        None,\n        description=\"The neighborhood where the protest took place, if applicable.\",\n    )\n    moved: bool = Field(\n        ...,\n        description=\"Indicates whether the protest moved from one location to another.\",\n    )\n\n\nclass DateDetails(BaseModel):\n    event_date: date = Field(\n        ...,\n        description=\"Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’\",\n    )\n    day_of_week: WeekDay = Field(\n        ...,\n        description=\"The day of the week the protest occurred, such as Monday or Thursday.\",\n    )\n    date_text: List[str] = Field(\n        ...,\n        description=\"List of text descriptors for the protest date, such as 'yesterday', 'last week', or 'Monday' .\",\n    )\n\n\nclass ParticipantDetails(BaseModel):\n    organizations: List[str] = Field(\n        default=[],\n        description=\"Names of organizations that participated in the protest event. Exclude targets or other organizations mentioned but not protesting.  Organizational participation can take many forms, from organizing and leading the event to sponsoring or co-sponsoring it to providing one or more speakers for it to just showing up to the event as a recognizable presence.  \",\n    )\n    advocates: List[str] = Field(\n        default=[], description=\"The names of individuals who organized.\"\n    )\n    participant_type: List[str] = Field(\n        default=[],\n        description=\"Descriptors of participants in the event, such as students, nurses, or local residents.  Record words or phrases describing the participants in the event. The goal is to capture as much information as possible about the kinds of people who participated, as distinct from any organizations they represent or belong to. \",\n    )\n\n\nclass Protest(BaseModel):\n    protest_article: bool = Field(\n        False,\n        description=\"Indicates if the article describes a protest against police brutality.\",\n    )\n    summary: str = Field(\n        ...,\n        description=\"A focused summary of the article focusing on the protest details.\",\n    )\n    location: LocationDetails = Field(..., description=\"Location of the protest.\")\n    size: SizeDetails = Field(..., description=\"Size of the protest.\")\n    participants: ParticipantDetails = Field(\n        ..., description=\"Organizations and participants in the protest.\"\n    )\n    event_date: DateDetails = Field(\n        ...,\n        description=\"Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’\",\n    )\n\n\narticle = {\n    \"text\": \"\"\"COLUMBUS, Ohio (WCMH) – Yesterday was a national day of protest, and Columbus recognized the day when dozens of families gathered at the Ohio Statehouse to protest police brutality.\n\nProtesters were asking for accountability and justice by sharing how they lost their loved ones, while organizers said the protest was about telling their stories in more than one way.\n\n“We know that more than 1,200 Ohioans have been lost to police violence since the year 2000,” Ohio Families United for Political Action and Change (OFUPAC) Organizing Director Elaine Schleiffer said. “We wanted to represent the loss that that is, the empty shoes, that there’s no replacing those family members.”\n\nOFUPAC is a non-profit organization that unites families who have lost loved ones in officer-involved shootings.\n\nFor many of those who turned out to yesterday’s protest, the issue hits close to home. Sabrina Jordan lost her son in an officer-involved shooting in 2017 just outside of Dayton.\n\n“We’re just here also, to, like, celebrate and love each other,” Jordan, who is also OFUPAC’s founder, said. “You know, connect.”\n\nTania Hudson’s son was fatally shot by a Columbus police officer in 2015. \n\n“We’re asking accountability,” she said. “Officers be drug tested when they’re involved in a shooting, alcohol test. We understand that they have trauma and drama, too.”\n\nThe city’s police union, the Fraternal Order of Police (FOP), said there is already accountability in place.\n\n“Accountability? How much more accountability can they ask for,” FOP Executive Vice President Brian Steel said. “We have an internal affairs. We have an inspector general’s office. We’re investigated by BCI in, say, a police-involved shooting, in a grand jury of our peers. There’s literally no more accountability that can be put on police officers today.”\n\n“Accountability is pretty much all that we can ask for,” Hudson said. “We can’t say justice – ours is gone. There will never be justice for us, but we’re out here trying to save other people’s lives. That’s why we’re constantly out here.”\n\nProtestors also mentioned their frustration with Marsy’s Law, which was originally passed to protect the victims of violent crimes, but which was extended to allow law enforcement departments to shield officers’ names when they are involved in a shooting. Protesters think this shouldn’t be the case while Steel said it’s an important protection for officers who are victims of violent crimes.\n\"\"\",\n    \"headline\": \"Statehouse protest calls for end to police brutality\",\n    \"publication-date\": \"2023-10-22\",\n    \"source\": \"WCMH\",\n}\n\n\ndef get_protest_details(article):\n    client = OpenAI(\n        max_retries=3,\n        timeout=20.0,\n    )\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that extracts summaries of newspaper articles about political protests as JSON for a database. \",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Extract information about the details about a protest from the following article.\n      Only use information from the article.\n\n      {article}\n      \n      \"\"\",\n        },\n    ]\n\n    completion = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # model = 'gpt-4-turbo-preview',\n        functions=[\n            {\n                \"name\": \"protest_details\",\n                \"description\": \"Extract insights from media article about protest.\",\n                \"parameters\": Protest.model_json_schema(),\n            }\n        ],\n        n=1,\n        messages=messages,\n    )\n\n    r = json.loads(completion.choices[0].message.function_call.arguments)\n    return r\n    return pd.DataFrame(\n        [json.loads(c.message.function_call.arguments) for c in completion.choices]\n    )\n\n\nr = get_protest_details(article)\n\n\ndf = pd.json_normalize(\n    r, sep=\"_\"\n)  # It is returning some nested dictionaries, so I can't use the normal pd.from_json\ndf\n\n\n\n\n\n\n\n\nsummary\nlocation_city\nlocation_state_abbreviation\nlocation_neighborhood\nlocation_moved\nsize_size_text\nsize_size_exact\nsize_size_estimate\nsize_size_cat\nparticipants_organizations\nparticipants_advocates\nparticipants_participant_type\nevent_date_event_date\nevent_date_day_of_week\nevent_date_date_text\n\n\n\n\n0\nYesterday was a national day of protest in Col...\nColumbus\nOH\nNone\nFalse\n[dozens, families]\nNone\n50\n1\n[Ohio Families United for Political Action and...\n[]\n[]\n2023-10-21\nSaturday\n[yesterday]\n\n\n\n\n\n\n\nEstimated cost:\n\ngpt-4-0125-preview: 55 articles for $1\ngpt-3.5-turbo: 1106 articles for $1\n\n\nProtest.model_json_schema()\n\n{'$defs': {'DateDetails': {'properties': {'event_date': {'description': 'Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’',\n     'format': 'date',\n     'title': 'Event Date',\n     'type': 'string'},\n    'day_of_week': {'allOf': [{'$ref': '#/$defs/WeekDay'}],\n     'description': 'The day of the week the protest occurred, such as Monday or Thursday.'},\n    'date_text': {'description': \"List of text descriptors for the protest date, such as 'yesterday', 'last week', or 'Monday' .\",\n     'items': {'type': 'string'},\n     'title': 'Date Text',\n     'type': 'array'}},\n   'required': ['event_date', 'day_of_week', 'date_text'],\n   'title': 'DateDetails',\n   'type': 'object'},\n  'LocationDetails': {'properties': {'city': {'description': 'The city where the protest took place.',\n     'title': 'City',\n     'type': 'string'},\n    'state_abbreviation': {'allOf': [{'$ref': '#/$defs/StateAB'}],\n     'description': 'The two-letter abbreviation of the state where the protest took place, such as NY or CA.'},\n    'neighborhood': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n     'default': None,\n     'description': 'The neighborhood where the protest took place, if applicable.',\n     'title': 'Neighborhood'},\n    'moved': {'description': 'Indicates whether the protest moved from one location to another.',\n     'title': 'Moved',\n     'type': 'boolean'}},\n   'required': ['city', 'state_abbreviation', 'moved'],\n   'title': 'LocationDetails',\n   'type': 'object'},\n  'ParticipantDetails': {'properties': {'organizations': {'default': [],\n     'description': 'Names of organizations that participated in the protest event. Exclude targets or other organizations mentioned but not protesting.  Organizational participation can take many forms, from organizing and leading the event to sponsoring or co-sponsoring it to providing one or more speakers for it to just showing up to the event as a recognizable presence.  ',\n     'items': {'type': 'string'},\n     'title': 'Organizations',\n     'type': 'array'},\n    'advocates': {'default': [],\n     'description': 'The names of individuals who organized.',\n     'items': {'type': 'string'},\n     'title': 'Advocates',\n     'type': 'array'},\n    'participant_type': {'default': [],\n     'description': 'Descriptors of participants in the event, such as students, nurses, or local residents.  Record words or phrases describing the participants in the event. The goal is to capture as much information as possible about the kinds of people who participated, as distinct from any organizations they represent or belong to. ',\n     'items': {'type': 'string'},\n     'title': 'Participant Type',\n     'type': 'array'}},\n   'title': 'ParticipantDetails',\n   'type': 'object'},\n  'SizeCategory': {'enum': [0, 1, 2, 3, 4],\n   'title': 'SizeCategory',\n   'type': 'integer'},\n  'SizeDetails': {'properties': {'size_text': {'description': 'List of text descriptors for the number of people who participated in the event.',\n     'items': {'type': 'string'},\n     'title': 'Size Text',\n     'type': 'array'},\n    'size_exact': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n     'default': None,\n     'description': 'Exact number of participants, if reported.',\n     'title': 'Size Exact'},\n    'size_estimate': {'description': 'Your best guess at the estimated number of participants based on the entire article.',\n     'title': 'Size Estimate',\n     'type': 'integer'},\n    'size_cat': {'allOf': [{'$ref': '#/$defs/SizeCategory'}],\n     'default': 0,\n     'description': 'Categorical indicator of crowd size. 0 = unknown; 1 = 1-99; 2 = 100-999; 3 = 1,000-9,999; 4 = 10,000+.'}},\n   'required': ['size_text', 'size_estimate'],\n   'title': 'SizeDetails',\n   'type': 'object'},\n  'StateAB': {'enum': ['AK',\n    'AL',\n    'AR',\n    'AZ',\n    'CA',\n    'CO',\n    'CT',\n    'DC',\n    'DE',\n    'FL',\n    'GA',\n    'HI',\n    'IA',\n    'ID',\n    'IL',\n    'IN',\n    'KS',\n    'KY',\n    'LA',\n    'MA',\n    'MD',\n    'ME',\n    'MI',\n    'MN',\n    'MO',\n    'MS',\n    'MT',\n    'NC',\n    'ND',\n    'NE',\n    'NH',\n    'NJ',\n    'NM',\n    'NV',\n    'NY',\n    'OH',\n    'OK',\n    'OR',\n    'PA',\n    'RI',\n    'SC',\n    'SD',\n    'TN',\n    'TX',\n    'UT',\n    'VA',\n    'VT',\n    'WA',\n    'WI',\n    'WV',\n    'WY'],\n   'title': 'StateAB',\n   'type': 'string'},\n  'WeekDay': {'enum': ['Monday',\n    'Tuesday',\n    'Wednesday',\n    'Thursday',\n    'Friday',\n    'Saturday',\n    'Sunday'],\n   'title': 'WeekDay',\n   'type': 'string'}},\n 'properties': {'protest_article': {'default': False,\n   'description': 'Indicates if the article describes a protest against police brutality.',\n   'title': 'Protest Article',\n   'type': 'boolean'},\n  'summary': {'description': 'A focused summary of the article focusing on the protest details.',\n   'title': 'Summary',\n   'type': 'string'},\n  'location': {'allOf': [{'$ref': '#/$defs/LocationDetails'}],\n   'description': 'Location of the protest.'},\n  'size': {'allOf': [{'$ref': '#/$defs/SizeDetails'}],\n   'description': 'Size of the protest.'},\n  'participants': {'allOf': [{'$ref': '#/$defs/ParticipantDetails'}],\n   'description': 'Organizations and participants in the protest.'},\n  'event_date': {'allOf': [{'$ref': '#/$defs/DateDetails'}],\n   'description': 'Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’'}},\n 'required': ['summary', 'location', 'size', 'participants', 'event_date'],\n 'title': 'Protest',\n 'type': 'object'}"
  },
  {
    "objectID": "posts/transcription/whisper-plus.html",
    "href": "posts/transcription/whisper-plus.html",
    "title": "Audio Transcription with Speaker Identification",
    "section": "",
    "text": "This is my version of the WhisperPlus demo, trying to get it to work on my Mac.\nPrior to running this, create a new Python environment with Python 3.11. I don’t normally create new environments, but this library needed one to get all the dependencies to play nicely together.\nconda create --name whisperplus python==3.11 notebook pip\nconda activate whisperplus\njupter notebook\n\npip install whisperplus -U -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\nSome warnings show up after running the commands, but the don’t seem to have much impact.\n\nfrom whisperplus import SpeechToTextPipeline, download_and_convert_to_mp3\n\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n  torchaudio.set_audio_backend(\"soundfile\")\n\n\nTranscription without speaker identification.\n\nmodel = 'openai/whisper-small' # the smallest and quickest model, but less accurate than the others like  whisper-large-v3 \n\naudio_path = 'sample/quiton_baxter_interview_sample.mp3'\npipeline = SpeechToTextPipeline(model_id=model)\ntranscript = pipeline(audio_path, model, \"english\")\n\nprint(transcript)\n\n2024-02-14 11:28:36,149 - INFO - Loading model...\n2024-02-14 11:28:38,246 - INFO - Model loaded successfully.\n2024-02-14 11:28:38,285 - INFO - Using device: mps\n2024-02-14 11:28:38,961 - INFO - Transcribing audio...\n\n\n Hello, this is Chris McGinnis. Today is Saturday, February 23rd, and I'm interviewing Mr. Quinton Baker at his home in Hillsborough, North Carolina. This tape is a continuing series of interviews that contribute to the Gay and Lesbian Southern History Project, which is part of the Southern Oral History Program at UNC Chapel Hill. This project is currently focusing on the history of gay men, lesbians, bisexual and transgender history in Chapel Hill and the Triangle area over the 20th century. This tape will be stored in the Southern Historical Collection, which is located in Wilson Library on the campus of the University of North Carolina and Chapel Hill. The number for this tape is 02.23.02-QB.1. Here we go. Well, first off, Quentin, just to, this is a general question I ask everybody, tell me a little bit about where you were born, where you grew up, and just a general synopsis of the early years. The early years. The early years. I was born in Greenville, North Carolina, and I spent the first 18, 18 years there. I was born in a family of four children. I'm the youngest of four. My parents were laborers. My mother was the domestic, my father was a laborer. We lived in town at that time. Greenville was about 21,000 people. What did your father do? Did he work in textile mill? My father did various jobs. He worked in a furniture store. He sometimes worked in the fields, he worked in the tobacco factory, so that there was never one job, there was a serious variety of jobs. He even learned to repair televisions while he was working for...\n\n\nNow with speaker identification, powered by PyAnnote’s Speaker Diarization.\n\nfrom whisperplus import (\n    ASRDiarizationPipeline,\n    download_and_convert_to_mp3,\n    format_speech_to_dialogue,\n)\n\nmodel = 'openai/whisper-small' \naudio_path = 'sample/quiton_baxter_interview_sample.mp3'\ndevice = \"mps\"  # \"mps\" if you are on a modern Mac, \"cuda\" if have a GPU (the fastest option), or \"cpu\" (the slowest option). \n\npipeline = ASRDiarizationPipeline.from_pretrained(\n    asr_model=model,\n    diarizer_model=\"pyannote/speaker-diarization\",\n    use_auth_token='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', # This is mine. Get your own at https://huggingface.co/pyannote/speaker-diarization\n    chunk_length_s=30,\n    device=device,\n)\n\noutput_text = pipeline(audio_path, \n                       num_speakers=2, \n                       min_speaker=1, \n                       max_speaker=2)\n\n\ndialogue = format_speech_to_dialogue(output_text)\nprint(dialogue)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n2024-02-14 11:37:44,135 - INFO - Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n2024-02-14 11:37:44,224 - INFO - Fetch hyperparams.yaml: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/hyperparams.yaml.\n2024-02-14 11:37:44,224 - INFO - Fetch custom.py: Delegating to Huggingface hub, source speechbrain/spkrec-ecapa-voxceleb.\n2024-02-14 11:37:44,498 - INFO - Fetch embedding_model.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/embedding_model.ckpt.\n2024-02-14 11:37:44,499 - INFO - Fetch mean_var_norm_emb.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt.\n2024-02-14 11:37:44,500 - INFO - Fetch classifier.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/classifier.ckpt.\n2024-02-14 11:37:44,501 - INFO - Fetch label_encoder.txt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/label_encoder.ckpt.\n2024-02-14 11:37:44,502 - INFO - Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.2.0. Bad things might happen unless you revert torch to 1.x.\nSpeaker 1:  Hello, this is Chris McGinnis. Today is Saturday, February 23rd, and I'm interviewing Mr. Quinton Baker at his home in Hillsborough, North Carolina. This tape is a continuing series of interviews that contribute to the Gay and Lesbian Southern History Project, which is part of the Southern Oral History Program at UNC Chapel Hill. This project is currently focusing on the history of gay men, lesbians, bisexual and transgender history in Chapel Hill and the Triangle area over the 20th century. This tape will be stored in the Southern Historical Collection, which is located in Wilson Library on the campus of the University of North Carolina and Chapel Hill. The number for this tape is 02.23.02-QB.1. Here we go. Well, first off, Quentin, just to, this is a general question I ask everybody, tell me a little bit about where you were born, where you grew up, and just a general synopsis\nSpeaker 2:  of the early years. The early years. I was born in Greenville, North Carolina, and I spent the first 18, 18 years there. I was born in a family of four children. I'm the youngest of four. My parents were laborers. My mother was the domestic, my father was a laborer. We lived in town at that time. Greenville was about 21,000 people.\nSpeaker 1:  What did your father do? Did he work in textile mill?\nSpeaker 2:  My father did various jobs. He worked in a furniture store. He sometimes worked in the fields, he worked in the tobacco factory, so that there was never one job, there was a serious variety of jobs. He even learned to repair televisions while he was working for...\n\n\n\nThe transcription took less than 30 seconds (using the small model) for the 2-minute interview on my M2 MacBook Air with 8GBs of memory. In contrast, the full diarization took almost an hour, so you still might want to use Google Colab and a GPU."
  },
  {
    "objectID": "posts/tts/tts.html",
    "href": "posts/tts/tts.html",
    "title": "Text to Speech using OpenAI’s API",
    "section": "",
    "text": "OpenAI has a pretty good overview of their text to speech API. It’s not free or impulsively cheap, like ChatGPT3.5, but, in my opinion, it is the best available model.\n\nfrom openai import OpenAI\n\nThis assumes that you have an OpenAi API key, and have stored it as an environment variable.\n\ndef stt(text, voice, mp3fn, model = \"tts-1-hd\"):\n    # A function to call the API and save it as an MP3\n    \n    client = OpenAI() # \n    response = client.audio.speech.create(\n        model = model, # model=\"tts-1\" is cheaper and pretty close in quality.\n        voice=voice,\n        input=text\n    )\n    \n    response.stream_to_file(mp3fn)\n\n\ngraph = (\n    \"Sociology is the last of the great sciences. It is only a little more than \"\n    \"a generation old, and, as yet, its principles are not quite definite. So that \"\n    \"among any large number of people who call themselves sociologists, one might \"\n    \"find as many shades of opinion as he would among the large number of persons \"\n    \"who call themselves Christians. Unlike biology, or astronomy, or mathematics, \"\n    \"there is as yet no definite set of fundamental principles upon which all \"\n    \"sociologists agree.\"\n) # cite: Wright, Richard R. 1911. \"The Negro Problem\" https://www.crisisopportunity.org/articles/negro_problem.html\n\n\nstt(graph, \"shimmer\", 'shimmer_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/4001769771.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\nNote: The DeprecationWarning is new. Something with OpenAI’s saving method isn’t playing nicely with notebooks.\nSample with the Shimmer voice\n\nfrom IPython.display import Audio\n\nAudio('shimmer_sample.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThe only other voices I like are Onyx and Echo.\n\nstt(graph, \"onyx\", 'onyx_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/4001769771.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\n\nAudio('onyx_sample.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nstt(graph, \"echo\", 'echo_sample.mp3')\nAudio('echo_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/3136648165.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThe HD model costs $0.030 per 1,000 characters, which works out to be about 18 cents for 1,000 words. The non-HD model costs half that.\nFor fun, I used ChatGPT to write an introduction to a W.E.B. DuBois short story, used TTS to produce audio versions of the intro and story using different voices, and then used pydub to splice the two.\nContent warning: The story contains racial epithets, which Du Bois often used when writing dialogue for white racists. Related, whatever content filters exist for ChatGPT don’t seem to be there for their TTS model.\n\nAudio('on_being_crazy_hd.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python snippets",
    "section": "",
    "text": "things I wish I knew 24 hours ago\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Synthetic Data with ChatGPT\n\n\n\nOpenAI\n\n\n\nWriting 600 fake article abstracts.\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping in Bulk\n\n\n\nWeb Scraping\n\n\n\nUsing pyppeteer to download websites in parallel\n\n\n\n\n\n\nFeb 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing ChatGPT to Make Social Media Posts\n\n\n\nOpenAI\n\n\n\nMore using Pydantic for OpenAI function calling\n\n\n\n\n\n\nFeb 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Structured Data from ChatGPT\n\n\n\nOpenAI\n\n\n\nUsing Pydantic for OpenAI function calling\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAudio Transcription with Speaker Identification\n\n\n\nTranscription\n\n\n\nUsing WhisperpPlus to transcribe an audio file.\n\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText to Speech using OpenAI’s API\n\n\n\nText to Speech\n\n\nOpenAI\n\n\n\nWe are almost there with computer talking.\n\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]