[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Python notebooks, mostly written to myself, on things that worked or didn’t work.\nAll art is AI generated in the style of early twentiety century Ashcan sketches.\nBlog is built using Quarto.\nby Neal Caren"
  },
  {
    "objectID": "posts/replication/pydantic.html",
    "href": "posts/replication/pydantic.html",
    "title": "Getting Structured Data from ChatGPT",
    "section": "",
    "text": "This notebook uses pydantic and ChatGPT API’s function calling to extract details about a protest event from a newspaper article. In the old days, you had to ask it to provide a JSON-like object. Next, I defined the JSONs myself in the functions. Now I’m learning to use pydantic.\n\npip install openai pydantic -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom datetime import date\nfrom enum import Enum\nimport json\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\nfrom openai import OpenAI\nimport pandas as pd\n\n\nclass WeekDay(Enum):\n    Monday = \"Monday\"\n    Tuesday = \"Tuesday\"\n    Wednesday = \"Wednesday\"\n    Thursday = \"Thursday\"\n    Friday = \"Friday\"\n    Saturday = \"Saturday\"\n    Sunday = \"Sunday\"\n\n\nclass StateAB(Enum):\n    AK = \"AK\"\n    AL = \"AL\"\n    AR = \"AR\"\n    AZ = \"AZ\"\n    CA = \"CA\"\n    CO = \"CO\"\n    CT = \"CT\"\n    DC = \"DC\"\n    DE = \"DE\"\n    FL = \"FL\"\n    GA = \"GA\"\n    HI = \"HI\"\n    IA = \"IA\"\n    ID = \"ID\"\n    IL = \"IL\"\n    IN = \"IN\"\n    KS = \"KS\"\n    KY = \"KY\"\n    LA = \"LA\"\n    MA = \"MA\"\n    MD = \"MD\"\n    ME = \"ME\"\n    MI = \"MI\"\n    MN = \"MN\"\n    MO = \"MO\"\n    MS = \"MS\"\n    MT = \"MT\"\n    NC = \"NC\"\n    ND = \"ND\"\n    NE = \"NE\"\n    NH = \"NH\"\n    NJ = \"NJ\"\n    NM = \"NM\"\n    NV = \"NV\"\n    NY = \"NY\"\n    OH = \"OH\"\n    OK = \"OK\"\n    OR = \"OR\"\n    PA = \"PA\"\n    RI = \"RI\"\n    SC = \"SC\"\n    SD = \"SD\"\n    TN = \"TN\"\n    TX = \"TX\"\n    UT = \"UT\"\n    VA = \"VA\"\n    VT = \"VT\"\n    WA = \"WA\"\n    WI = \"WI\"\n    WV = \"WV\"\n    WY = \"WY\"\n\n\nclass SizeCategory(Enum):\n    UNKNOWN = 0\n    SMALL = 1  # 1-99\n    MEDIUM = 2  # 100-999\n    LARGE = 3  # 1,000-9,999\n    VERY_LARGE = 4  # 10,000+\n\n\nclass SizeDetails(BaseModel):\n    size_text: List[str] = Field(\n        ...,\n        description=\"List of text descriptors for the number of people who participated in the event.\",\n    )\n    size_exact: Optional[int] = Field(\n        None, description=\"Exact number of participants, if reported.\"\n    )\n    size_estimate: int = Field(\n        ...,\n        description=\"Your best guess at the estimated number of participants based on the entire article.\",\n    )\n    size_cat: SizeCategory = Field(\n        SizeCategory.UNKNOWN,\n        description=\"Categorical indicator of crowd size. 0 = unknown; 1 = 1-99; 2 = 100-999; 3 = 1,000-9,999; 4 = 10,000+.\",\n    )\n\n\nclass LocationDetails(BaseModel):\n    city: str = Field(..., description=\"The city where the protest took place.\")\n    state_abbreviation: StateAB = Field(\n        ...,\n        description=\"The two-letter abbreviation of the state where the protest took place, such as NY or CA.\",\n    )\n    neighborhood: Optional[str] = Field(\n        None,\n        description=\"The neighborhood where the protest took place, if applicable.\",\n    )\n    moved: bool = Field(\n        ...,\n        description=\"Indicates whether the protest moved from one location to another.\",\n    )\n\n\nclass DateDetails(BaseModel):\n    event_date: date = Field(\n        ...,\n        description=\"Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’\",\n    )\n    day_of_week: WeekDay = Field(\n        ...,\n        description=\"The day of the week the protest occurred, such as Monday or Thursday.\",\n    )\n    date_text: List[str] = Field(\n        ...,\n        description=\"List of text descriptors for the protest date, such as 'yesterday', 'last week', or 'Monday' .\",\n    )\n\n\nclass ParticipantDetails(BaseModel):\n    organizations: List[str] = Field(\n        default=[],\n        description=\"Names of organizations that participated in the protest event. Exclude targets or other organizations mentioned but not protesting.  Organizational participation can take many forms, from organizing and leading the event to sponsoring or co-sponsoring it to providing one or more speakers for it to just showing up to the event as a recognizable presence.  \",\n    )\n    advocates: List[str] = Field(\n        default=[], description=\"The names of individuals who organized.\"\n    )\n    participant_type: List[str] = Field(\n        default=[],\n        description=\"Descriptors of participants in the event, such as students, nurses, or local residents.  Record words or phrases describing the participants in the event. The goal is to capture as much information as possible about the kinds of people who participated, as distinct from any organizations they represent or belong to. \",\n    )\n\n\nclass Protest(BaseModel):\n    protest_article: bool = Field(\n        False,\n        description=\"Indicates if the article describes a protest against police brutality.\",\n    )\n    summary: str = Field(\n        ...,\n        description=\"A focused summary of the article focusing on the protest details.\",\n    )\n    location: LocationDetails = Field(..., description=\"Location of the protest.\")\n    size: SizeDetails = Field(..., description=\"Size of the protest.\")\n    participants: ParticipantDetails = Field(\n        ..., description=\"Organizations and participants in the protest.\"\n    )\n    event_date: DateDetails = Field(\n        ...,\n        description=\"Date of the protest. Pay attention to dates mentioned in the article and words such as ‘yesterday,’ ‘last week,’ and ‘Monday.’\",\n    )\n\n\narticle = {\n    \"text\": \"\"\"COLUMBUS, Ohio (WCMH) – Yesterday was a national day of protest, and Columbus recognized the day when dozens of families gathered at the Ohio Statehouse to protest police brutality.\n\nProtesters were asking for accountability and justice by sharing how they lost their loved ones, while organizers said the protest was about telling their stories in more than one way.\n\n“We know that more than 1,200 Ohioans have been lost to police violence since the year 2000,” Ohio Families United for Political Action and Change (OFUPAC) Organizing Director Elaine Schleiffer said. “We wanted to represent the loss that that is, the empty shoes, that there’s no replacing those family members.”\n\nOFUPAC is a non-profit organization that unites families who have lost loved ones in officer-involved shootings.\n\nFor many of those who turned out to yesterday’s protest, the issue hits close to home. Sabrina Jordan lost her son in an officer-involved shooting in 2017 just outside of Dayton.\n\n“We’re just here also, to, like, celebrate and love each other,” Jordan, who is also OFUPAC’s founder, said. “You know, connect.”\n\nTania Hudson’s son was fatally shot by a Columbus police officer in 2015. \n\n“We’re asking accountability,” she said. “Officers be drug tested when they’re involved in a shooting, alcohol test. We understand that they have trauma and drama, too.”\n\nThe city’s police union, the Fraternal Order of Police (FOP), said there is already accountability in place.\n\n“Accountability? How much more accountability can they ask for,” FOP Executive Vice President Brian Steel said. “We have an internal affairs. We have an inspector general’s office. We’re investigated by BCI in, say, a police-involved shooting, in a grand jury of our peers. There’s literally no more accountability that can be put on police officers today.”\n\n“Accountability is pretty much all that we can ask for,” Hudson said. “We can’t say justice – ours is gone. There will never be justice for us, but we’re out here trying to save other people’s lives. That’s why we’re constantly out here.”\n\nProtestors also mentioned their frustration with Marsy’s Law, which was originally passed to protect the victims of violent crimes, but which was extended to allow law enforcement departments to shield officers’ names when they are involved in a shooting. Protesters think this shouldn’t be the case while Steel said it’s an important protection for officers who are victims of violent crimes.\n\"\"\",\n    \"headline\": \"Statehouse protest calls for end to police brutality\",\n    \"publication-date\": \"2023-10-22\",\n    \"source\": \"WCMH\",\n}\n\n\ndef get_protest_details(article):\n    client = OpenAI(\n        max_retries=3,\n        timeout=20.0,\n    )\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that extracts summaries of newspaper articles about political protests as JSON for a database. \",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"Extract information about the details about a protest from the following article.\n      Only use information from the article.\n\n      {article}\n      \n      \"\"\",\n        },\n    ]\n\n    completion = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # model = 'gpt-4-turbo-preview',\n        functions=[\n            {\n                \"name\": \"protest_details\",\n                \"description\": \"Extract insights from media article about protest.\",\n                \"parameters\": Protest.model_json_schema(),\n            }\n        ],\n        n=1,\n        messages=messages,\n    )\n\n    r = json.loads(completion.choices[0].message.function_call.arguments)\n    return r\n    return pd.DataFrame(\n        [json.loads(c.message.function_call.arguments) for c in completion.choices]\n    )\n\n\nr = get_protest_details(article)\n\n\ndf = pd.json_normalize(\n    r, sep=\"_\"\n)  # It is returning some nested dictionaries, so I can't use the normal pd.from_json\ndf\n\n\n\n\n\n\n\n\nsummary\nlocation_city\nlocation_state_abbreviation\nlocation_neighborhood\nlocation_moved\nsize_size_text\nsize_size_exact\nsize_size_estimate\nsize_size_cat\nparticipants_organizations\nparticipants_advocates\nparticipants_participant_type\nevent_date_event_date\nevent_date_day_of_week\nevent_date_date_text\n\n\n\n\n0\nYesterday was a national day of protest in Col...\nColumbus\nOH\nNone\nFalse\n[dozens, families]\nNone\n50\n1\n[Ohio Families United for Political Action and...\n[]\n[]\n2023-10-21\nSaturday\n[yesterday]\n\n\n\n\n\n\n\nEstimated cost:\n\ngpt-4-0125-preview: 55 articles for $1\ngpt-3.5-turbo: 1106 articles for $1"
  },
  {
    "objectID": "posts/transcription/whisper-plus.html",
    "href": "posts/transcription/whisper-plus.html",
    "title": "Audio Transcription with Speaker Identification",
    "section": "",
    "text": "This is my version of the WhisperPlus demo, trying to get it to work on my Mac.\nPrior to running this, create a new Python environment with Python 3.11. I don’t normally create new environments, but this library needed one to get all the dependencies to play nicely together.\nconda create --name whisperplus python==3.11 notebook pip\nconda activate whisperplus\njupter notebook\n\npip install whisperplus -U -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\nSome warnings show up after running the commands, but the don’t seem to have much impact.\n\nfrom whisperplus import SpeechToTextPipeline, download_and_convert_to_mp3\n\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n/Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n  torchaudio.set_audio_backend(\"soundfile\")\n\n\nTranscription without speaker identification.\n\nmodel = 'openai/whisper-small' # the smallest and quickest model, but less accurate than the others like  whisper-large-v3 \n\naudio_path = 'sample/quiton_baxter_interview_sample.mp3'\npipeline = SpeechToTextPipeline(model_id=model)\ntranscript = pipeline(audio_path, model, \"english\")\n\nprint(transcript)\n\n2024-02-14 11:28:36,149 - INFO - Loading model...\n2024-02-14 11:28:38,246 - INFO - Model loaded successfully.\n2024-02-14 11:28:38,285 - INFO - Using device: mps\n2024-02-14 11:28:38,961 - INFO - Transcribing audio...\n\n\n Hello, this is Chris McGinnis. Today is Saturday, February 23rd, and I'm interviewing Mr. Quinton Baker at his home in Hillsborough, North Carolina. This tape is a continuing series of interviews that contribute to the Gay and Lesbian Southern History Project, which is part of the Southern Oral History Program at UNC Chapel Hill. This project is currently focusing on the history of gay men, lesbians, bisexual and transgender history in Chapel Hill and the Triangle area over the 20th century. This tape will be stored in the Southern Historical Collection, which is located in Wilson Library on the campus of the University of North Carolina and Chapel Hill. The number for this tape is 02.23.02-QB.1. Here we go. Well, first off, Quentin, just to, this is a general question I ask everybody, tell me a little bit about where you were born, where you grew up, and just a general synopsis of the early years. The early years. The early years. I was born in Greenville, North Carolina, and I spent the first 18, 18 years there. I was born in a family of four children. I'm the youngest of four. My parents were laborers. My mother was the domestic, my father was a laborer. We lived in town at that time. Greenville was about 21,000 people. What did your father do? Did he work in textile mill? My father did various jobs. He worked in a furniture store. He sometimes worked in the fields, he worked in the tobacco factory, so that there was never one job, there was a serious variety of jobs. He even learned to repair televisions while he was working for...\n\n\nNow with speaker identification, powered by PyAnnote’s Speaker Diarization.\n\nfrom whisperplus import (\n    ASRDiarizationPipeline,\n    download_and_convert_to_mp3,\n    format_speech_to_dialogue,\n)\n\nmodel = 'openai/whisper-small' \naudio_path = 'sample/quiton_baxter_interview_sample.mp3'\ndevice = \"mps\"  # \"mps\" if you are on a modern Mac, \"cuda\" if have a GPU (the fastest option), or \"cpu\" (the slowest option). \n\npipeline = ASRDiarizationPipeline.from_pretrained(\n    asr_model=model,\n    diarizer_model=\"pyannote/speaker-diarization\",\n    use_auth_token='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', # This is mine. Get your own at https://huggingface.co/pyannote/speaker-diarization\n    chunk_length_s=30,\n    device=device,\n)\n\noutput_text = pipeline(audio_path, \n                       num_speakers=2, \n                       min_speaker=1, \n                       max_speaker=2)\n\n\ndialogue = format_speech_to_dialogue(output_text)\nprint(dialogue)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n2024-02-14 11:37:44,135 - INFO - Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n2024-02-14 11:37:44,224 - INFO - Fetch hyperparams.yaml: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/hyperparams.yaml.\n2024-02-14 11:37:44,224 - INFO - Fetch custom.py: Delegating to Huggingface hub, source speechbrain/spkrec-ecapa-voxceleb.\n2024-02-14 11:37:44,498 - INFO - Fetch embedding_model.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/embedding_model.ckpt.\n2024-02-14 11:37:44,499 - INFO - Fetch mean_var_norm_emb.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt.\n2024-02-14 11:37:44,500 - INFO - Fetch classifier.ckpt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/classifier.ckpt.\n2024-02-14 11:37:44,501 - INFO - Fetch label_encoder.txt: Using existing file/symlink in /Users/nealcaren/.cache/torch/pyannote/speechbrain/label_encoder.ckpt.\n2024-02-14 11:37:44,502 - INFO - Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.2.0. Bad things might happen unless you revert torch to 1.x.\nSpeaker 1:  Hello, this is Chris McGinnis. Today is Saturday, February 23rd, and I'm interviewing Mr. Quinton Baker at his home in Hillsborough, North Carolina. This tape is a continuing series of interviews that contribute to the Gay and Lesbian Southern History Project, which is part of the Southern Oral History Program at UNC Chapel Hill. This project is currently focusing on the history of gay men, lesbians, bisexual and transgender history in Chapel Hill and the Triangle area over the 20th century. This tape will be stored in the Southern Historical Collection, which is located in Wilson Library on the campus of the University of North Carolina and Chapel Hill. The number for this tape is 02.23.02-QB.1. Here we go. Well, first off, Quentin, just to, this is a general question I ask everybody, tell me a little bit about where you were born, where you grew up, and just a general synopsis\nSpeaker 2:  of the early years. The early years. I was born in Greenville, North Carolina, and I spent the first 18, 18 years there. I was born in a family of four children. I'm the youngest of four. My parents were laborers. My mother was the domestic, my father was a laborer. We lived in town at that time. Greenville was about 21,000 people.\nSpeaker 1:  What did your father do? Did he work in textile mill?\nSpeaker 2:  My father did various jobs. He worked in a furniture store. He sometimes worked in the fields, he worked in the tobacco factory, so that there was never one job, there was a serious variety of jobs. He even learned to repair televisions while he was working for...\n\n\n\nThe transcription took less than 30 seconds (using the small model) for the 2-minute interview on my M2 MacBook Air with 8GBs of memory. In contrast, the full diarization took almost an hour, so you still might want to use Google Colab and a GPU."
  },
  {
    "objectID": "posts/tts/tts.html",
    "href": "posts/tts/tts.html",
    "title": "Text to Speech using OpenAI’s API",
    "section": "",
    "text": "OpenAI has a pretty good overview of their text to speech API. It’s not free or impulsively cheap, like ChatGPT3.5, but, in my opinion, it is the best available model.\n\nfrom openai import OpenAI\n\nThis assumes that you have an OpenAi API key, and have stored it as an environment variable.\n\ndef stt(text, voice, mp3fn, model = \"tts-1-hd\"):\n    # A function to call the API and save it as an MP3\n    \n    client = OpenAI() # \n    response = client.audio.speech.create(\n        model = model, # model=\"tts-1\" is cheaper and pretty close in quality.\n        voice=voice,\n        input=text\n    )\n    \n    response.stream_to_file(mp3fn)\n\n\ngraph = (\n    \"Sociology is the last of the great sciences. It is only a little more than \"\n    \"a generation old, and, as yet, its principles are not quite definite. So that \"\n    \"among any large number of people who call themselves sociologists, one might \"\n    \"find as many shades of opinion as he would among the large number of persons \"\n    \"who call themselves Christians. Unlike biology, or astronomy, or mathematics, \"\n    \"there is as yet no definite set of fundamental principles upon which all \"\n    \"sociologists agree.\"\n) # cite: Wright, Richard R. 1911. \"The Negro Problem\" https://www.crisisopportunity.org/articles/negro_problem.html\n\n\nstt(graph, \"shimmer\", 'shimmer_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/4001769771.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\nNote: The DeprecationWarning is new. Something with OpenAI’s saving method isn’t playing nicely with notebooks.\nSample with the Shimmer voice\n\nfrom IPython.display import Audio\n\nAudio('shimmer_sample.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThe only other voices I like are Onyx and Echo.\n\nstt(graph, \"onyx\", 'onyx_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/4001769771.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\n\nAudio('onyx_sample.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nstt(graph, \"echo\", 'echo_sample.mp3')\nAudio('echo_sample.mp3')\n\n/var/folders/6c/yvlqyrq97gz6xg8h66c__jgc0000gn/T/ipykernel_6745/3136648165.py:11: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n  response.stream_to_file(mp3fn)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nThe HD model costs $0.030 per 1,000 characters, which works out to be about 18 cents for 1,000 words. The non-HD model costs half that.\nFor fun, I used ChatGPT to write an introduction to a W.E.B. DuBois short story, used TTS to produce audio versions of the intro and story using different voices, and then used pydub to splice the two.\nContent warning: The story contains racial epithets, which Du Bois often used when writing dialogue for white racists. Related, whatever content filters exist for ChatGPT don’t seem to be there for their TTS model.\n\nAudio('on_being_crazy_hd.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python snippets",
    "section": "",
    "text": "things I wish I knew 24 hours ago\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Structured Data from ChatGPT\n\n\n\nOpenAI\n\n\n\nUsing Pydantic for OpenAI function calling\n\n\n\n\n\n\nFeb 16, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAudio Transcription with Speaker Identification\n\n\n\nTranscription\n\n\n\nUsing WhisperpPlus to transcribe an audio file.\n\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText to Speech using OpenAI’s API\n\n\n\nText to Speech\n\n\nOpenAI\n\n\n\nChatGPT Conjoint Experiment Replication\n\n\n\n\n\n\nFeb 14, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]