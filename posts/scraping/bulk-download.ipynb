{
 "cells": [
  {
   "cell_type": "raw",
   "id": "06bee579-ceb8-4bbf-af7a-780a1f1df285",
   "metadata": {},
   "source": [
    "---\n",
    "title: Web Scraping in Bulk\n",
    "date: 2024-02-19\n",
    "description: Dwnloading websites in parallel with pyppeteer\n",
    "categories: [Web Scraping]\n",
    "image: library.webp\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2752f46",
   "metadata": {},
   "source": [
    "This script is one way to download multiple web pages at the same time. It's useful when you have many URLs from different websites you want to save. Instead of visiting each website individually, the script visits multiple websites simultaneously and saves what it finds. Rather than requesting the page through Python, it uses a \"headless\" web browser, which is much more likely to get you the actual content you want. \n",
    "\n",
    "This approach works best when the URLs are from different servers. You will eventually get locked out if you try to visit the same web server multiple times in the same second, but there's no reason not to visit five different websites at once. I don't know anything about parallel processing with the `asyncio` library, which is the only way to parallelize pyppeteer, so the script is mainly written by ChatGPT, but I've used it successfully a few times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b52e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyppeteer in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: python-slugify in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (8.0.4)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (2023.11.17)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (7.0.1)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (11.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (4.66.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (1.26.18)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyppeteer) (10.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from python-slugify) (1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from importlib-metadata>=1.4->pyppeteer) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/nealcaren/anaconda3/envs/whisperplus/lib/python3.11/site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyppeteer python-slugify chromedriver-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aac60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from random import shuffle\n",
    "\n",
    "from slugify import slugify\n",
    "from pyppeteer import launch\n",
    "from pyppeteer.errors import NetworkError\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bd667",
   "metadata": {},
   "source": [
    "The section below loads the wonderful [protest event data](https://github.com/nonviolent-action-lab/crowd-counting-consortium) set created by the [Crowd Counting Consortium](https://sites.google.com/view/crowdcountingconsortium/home). Each protest event is linked to one or more media accounts, and the URLs are in the `source_` fields. Using just the 2024 events, I combine the URL fields and remove the social media pages and duplicates. Finally, I extract a random sample of 100 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045da0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://github.com/nonviolent-action-lab/crowd-counting-consortium/raw/master/ccc_compiled_2021-present.csv\",\n",
    "    encoding=\"latin\",\n",
    "    low_memory=False,\n",
    ")\n",
    "\n",
    "# Limit to just 2024\n",
    "df = df[df[\"date\"].str.contains(\"2024\")]\n",
    "\n",
    "# grab the sources\n",
    "urls = (\n",
    "    list(df[\"source_1\"].astype(str).values)\n",
    "    + list(df[\"source_2\"].astype(str).values)\n",
    "    + list(df[\"source_3\"].astype(str).values)\n",
    "    + list(df[\"source_4\"].astype(str).values)\n",
    ")\n",
    "\n",
    "# eliminate social media\n",
    "for sm in [\"twitter\", \"youtube\", \"facebook\", \"instagram\", \"tiktok\", \"bsky\"]:\n",
    "    urls = [u for u in urls if f\"{sm}.com\" not in u and \"http\" in u]\n",
    "\n",
    "urls = list(set(urls))\n",
    "print(len(urls))\n",
    "shuffle(urls)\n",
    "urls = urls[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c5ad6",
   "metadata": {},
   "source": [
    "This function below uses asynchronous programming to  download and save the HTML content of web pages from a list of URLs. It uses a headless Chrome browser, controlled via the Pyppeteer library, to render pages just as they would appear in a web browser. This approach is particularly useful for capturing dynamically generated content, which traditional HTTP requests might miss.\n",
    "\n",
    "Key components of the script include:\n",
    "\n",
    "- **HTML Directory Creation**: At the start, the script ensures that there is a designated directory (named 'HTML') where all downloaded page contents will be saved. If this directory does not exist, it is created.\n",
    "\n",
    "- **User Agent Setting**: A user agent string is defined and used for all requests to mimic a real web browser, helping to avoid potential blocking by web servers that may restrict access to non-browser clients.\n",
    "\n",
    "- **`fetch` Function**: The core of the script is the `fetch` function. This asynchronous function takes a browser page object, a URL, and an optional timeout parameter. It performs the following actions for each URL:\n",
    "  - **URL Slugification**: Converts the URL into a filename-safe string and checks if the content has already been downloaded to avoid duplication.\n",
    "  - **Page Navigation**: Uses the headless browser to navigate to the URL, with a specified timeout to handle slow-loading pages.\n",
    "  - **Content Saving**: If the page loads successfully, its HTML content is saved to a file within the 'HTML' directory. If the page fails to load or an error occurs, the URL is added to a list of bad URLs for later reference.\n",
    "\n",
    "- **Concurrency Management**: The script is designed to process multiple URLs in parallel, maximizing efficiency by utilizing asynchronous operations. This approach allows for faster completion of download tasks compared to sequential processing.\n",
    "\n",
    "- **Error Handling**: The script includes basic error handling to manage timeouts and other exceptions, ensuring that it can continue running even if some pages fail to load.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f7b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the HTML directory exists\n",
    "html_dir = \"HTML\"\n",
    "os.makedirs(html_dir, exist_ok=True)\n",
    "\n",
    "# User agent to be used for all requests\n",
    "ua = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15\"\n",
    "bad_urls = []\n",
    "\n",
    "\n",
    "async def fetch(page, url, timeout=30):\n",
    "    # Slugify the URL to create a valid filename\n",
    "    filename = slugify(url) + \".html\"\n",
    "    file_path = os.path.join(html_dir, filename)\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        # print(f\"File {file_path} already exists, skipping download.\")\n",
    "        return\n",
    "\n",
    "    if url in bad_urls:\n",
    "        print(f\"Skipping bad URL: {url}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Set the user agent for the page\n",
    "        await page.setUserAgent(ua)\n",
    "\n",
    "        # Navigate to the page with a timeout\n",
    "        response = await asyncio.wait_for(\n",
    "            page.goto(url, {\"waitUntil\": \"networkidle0\"}), timeout\n",
    "        )\n",
    "\n",
    "        # Check if the page was successfully retrieved\n",
    "        if response and response.ok:\n",
    "            content = await page.content()\n",
    "            # Save the content to a file in the 'HTML' directory\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(content)\n",
    "            print(f\"Content from {url} has been saved to {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}\")\n",
    "            bad_urls.append(url)\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Fetching {url} took too long and was cancelled.\")\n",
    "        bad_urls.append(url)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching {url}: {e}\")\n",
    "        bad_urls.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb179a1",
   "metadata": {},
   "source": [
    "This next section actual does the downloading by employing an asynchronous queue-based approach to manage URLs and distribute them across multiple browser pages for parallel processing. This method significantly improves efficiency by ensuring that each browser page is continuously utilized without idle time waiting for other pages to complete their tasks.\n",
    "\n",
    "Key components and functionalities:\n",
    "\n",
    "- **`process_url` Function**: An asynchronous function that continuously processes URLs from a shared asyncio queue. Each browser page runs an instance of this function, fetching and processing URLs one after another until the queue is empty.\n",
    "\n",
    "- **`main` Function Setup**:\n",
    "  - **Browser and Page Initialization**: Initializes a headless browser instance and opens a specified number of browser pages. 5 to 10 seems reasonable. \n",
    "  - **URL Queue Creation**: Prepares an asyncio queue and populates it with URLs to be processed. This queue acts as a shared resource for distributing URLs among the available pages.\n",
    "  \n",
    "- **Task Management**:\n",
    "  - **Asynchronous Tasks**: For each browser page, an asynchronous task is created to process URLs from the queue. These tasks run concurrently, allowing for simultaneous processing across pages.\n",
    "  - **Task Synchronization**: Utilizes `asyncio.gather` to wait for all tasks to complete before proceeding, ensuring that all URLs are processed before closing the browser and pages.\n",
    "\n",
    "- **Resource Cleanup**: After processing all URLs, the script ensures a clean shutdown by closing each browser page and the browser itself, releasing system resources.\n",
    "\n",
    "- **Error Handling and Reporting**: Tracks URLs that could not be downloaded for any reason, reporting them at the end of the execution for further analysis or retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a08f75be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://www.wgmd.com/pro-palestinian-protesters-deface-veterans-cemetery-in-los-angeles-spray-paint-free-gaza/\n",
      "Content from https://www.fox61.com/article/news/local/hartford-county/west-hartford/west-hartford-vandalism-under-investigation-police/520-7b65ab7b-d93b-42f9-8ca2-0ed9d5b7689a has been saved to HTML/https-www-fox61-com-article-news-local-hartford-county-west-hartford-west-hartford-vandalism-under-investigation-police-520-7b65ab7b-d93b-42f9-8ca2-0ed9d5b7689a.html\n",
      "Fetching https://www.nbcnews.com/politics/donald-trump/trump-confuses-nikki-haley-pelosi-talking-jan-6-rcna134863 took too long and was cancelled.\n",
      "Fetching https://www.purdueexponent.org/campus/article_78be7d6e-c2bb-11ee-a25c-a3e2dff21694.html took too long and was cancelled.\n",
      "Fetching https://13wham.com/news/local/local-advocates-rally-in-downtown-rochester-on-51st-anniversary-of-roe-v-wade took too long and was cancelled.\n",
      "Fetching https://www.wvtm13.com/article/protests-kenneth-smith-execution-untied-nations-montgomery/46496998 took too long and was cancelled.\n",
      "Content from https://www.wwnytv.com/2024/01/20/congresswoman-stefanik-speaks-new-hampshire-support-trump/ has been saved to HTML/https-www-wwnytv-com-2024-01-20-congresswoman-stefanik-speaks-new-hampshire-support-trump.html\n",
      "Fetching https://nypost.com/2024/01/21/news/scream-actress-melissa-barrera-joins-disruptive-anti-israel-rally-at-sundance/ took too long and was cancelled.\n",
      "Fetching https://www.wlky.com/article/nonprofits-rally-frankfort-legislation-kentucky/46676604 took too long and was cancelled.\n",
      "Fetching https://www.courier-journal.com/story/news/politics/2024/02/08/kentucky-employees-retirement-system-participants-rally-for-13th-check/72527656007/ took too long and was cancelled.\n",
      "Fetching https://www.northjersey.com/story/news/2024/02/06/israel-hamas-war-day-of-action-for-palestine-nj-students-march/72478632007/ took too long and was cancelled.\n",
      "Fetching https://www.washingtonpost.com/dc-md-va/2024/01/15/virginia-assembly-gun-rights-rally/ took too long and was cancelled.\n",
      "Fetching https://dailybruin.com/2024/01/19/uc-divest-coalition-at-ucla-leads-hands-off-yemen-protest-on-campus took too long and was cancelled.\n",
      "Fetching https://www.washingtonpost.com/dc-md-va/2024/01/18/dc-march-for-life-rally-abortion/ took too long and was cancelled.\n",
      "Fetching https://www.fox5dc.com/news/dc-activists-plan-protest-against-capitals-wizards-move-to-virginia took too long and was cancelled.\n",
      "Fetching https://www.latimes.com/entertainment-arts/movies/story/2024-01-21/pro-palestinian-protestors-vie-for-hollywoods-attention-at-2024-sundance-film-festival took too long and was cancelled.\n",
      "Failed to retrieve https://secure.everyaction.com/RKr139EpKUCZg8_TIWA18A2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error (Target.detachFromTarget): No session with given id')>\n",
      "pyppeteer.errors.NetworkError: Protocol error (Target.detachFromTarget): No session with given id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://www.thetimestribune.com/news/dozens-rally-in-support-of-school-choice-amendment/article_6da6faa2-bbbb-11ee-9f2a-8354d0bdfbfd.html took too long and was cancelled.\n",
      "Fetching https://www.nbcnews.com/news/latino/convoy-rally-texas-mexico-border-attracts-trump-fans-decry-illegal-imm-rcna136967 took too long and was cancelled.\n",
      "Fetching https://nyunews.com/news/2024/01/26/pro-palestinian-bobst-poetry/ took too long and was cancelled.\n",
      "Fetching https://www.wjhl.com/news/local/kyle-rittenhouse-event-draws-supporters-protesters-at-etsu/ took too long and was cancelled.\n",
      "Fetching https://newjersey.news12.com/group-gathers-ahead-of-toms-river-council-meeting-to-protest-policeemt-funding-decision took too long and was cancelled.\n",
      "Fetching https://www.denverpost.com/2024/01/02/alamo-drafthouse-employees-union-drive-rally-denver/ took too long and was cancelled.\n",
      "The following URLs had issues and were not downloaded:\n",
      "https://www.wgmd.com/pro-palestinian-protesters-deface-veterans-cemetery-in-los-angeles-spray-paint-free-gaza/\n",
      "https://www.nbcnews.com/politics/donald-trump/trump-confuses-nikki-haley-pelosi-talking-jan-6-rcna134863\n",
      "https://www.purdueexponent.org/campus/article_78be7d6e-c2bb-11ee-a25c-a3e2dff21694.html\n",
      "https://13wham.com/news/local/local-advocates-rally-in-downtown-rochester-on-51st-anniversary-of-roe-v-wade\n",
      "https://www.wvtm13.com/article/protests-kenneth-smith-execution-untied-nations-montgomery/46496998\n",
      "https://nypost.com/2024/01/21/news/scream-actress-melissa-barrera-joins-disruptive-anti-israel-rally-at-sundance/\n",
      "https://www.wlky.com/article/nonprofits-rally-frankfort-legislation-kentucky/46676604\n",
      "https://www.courier-journal.com/story/news/politics/2024/02/08/kentucky-employees-retirement-system-participants-rally-for-13th-check/72527656007/\n",
      "https://www.northjersey.com/story/news/2024/02/06/israel-hamas-war-day-of-action-for-palestine-nj-students-march/72478632007/\n",
      "https://www.washingtonpost.com/dc-md-va/2024/01/15/virginia-assembly-gun-rights-rally/\n",
      "https://dailybruin.com/2024/01/19/uc-divest-coalition-at-ucla-leads-hands-off-yemen-protest-on-campus\n",
      "https://www.washingtonpost.com/dc-md-va/2024/01/18/dc-march-for-life-rally-abortion/\n",
      "https://www.fox5dc.com/news/dc-activists-plan-protest-against-capitals-wizards-move-to-virginia\n",
      "https://www.latimes.com/entertainment-arts/movies/story/2024-01-21/pro-palestinian-protestors-vie-for-hollywoods-attention-at-2024-sundance-film-festival\n",
      "https://secure.everyaction.com/RKr139EpKUCZg8_TIWA18A2\n",
      "https://www.thetimestribune.com/news/dozens-rally-in-support-of-school-choice-amendment/article_6da6faa2-bbbb-11ee-9f2a-8354d0bdfbfd.html\n",
      "https://www.nbcnews.com/news/latino/convoy-rally-texas-mexico-border-attracts-trump-fans-decry-illegal-imm-rcna136967\n",
      "https://nyunews.com/news/2024/01/26/pro-palestinian-bobst-poetry/\n",
      "https://www.wjhl.com/news/local/kyle-rittenhouse-event-draws-supporters-protesters-at-etsu/\n",
      "https://newjersey.news12.com/group-gathers-ahead-of-toms-river-council-meeting-to-protest-policeemt-funding-decision\n",
      "https://www.denverpost.com/2024/01/02/alamo-drafthouse-employees-union-drive-rally-denver/\n"
     ]
    }
   ],
   "source": [
    "async def process_url(page, url_queue):\n",
    "    while not url_queue.empty():\n",
    "        url = await url_queue.get()\n",
    "        await fetch(page, url)  # Your existing fetch function\n",
    "        url_queue.task_done()\n",
    "\n",
    "async def main():\n",
    "    browser = await launch()\n",
    "    pages = [await browser.newPage() for _ in range(5)]  # Initialize pages once\n",
    "\n",
    "    # Create a queue of URLs\n",
    "    url_queue = asyncio.Queue()\n",
    "    for url in urls:\n",
    "        await url_queue.put(url)\n",
    "\n",
    "    # Create a task for each page to process URLs from the queue\n",
    "    tasks = [asyncio.create_task(process_url(page, url_queue)) for page in pages]\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    # Close pages and browser after all operations are complete\n",
    "    for page in pages:\n",
    "        await page.close()\n",
    "    await browser.close()\n",
    "\n",
    "    if bad_urls:\n",
    "        print(\"The following URLs had issues and were not downloaded:\")\n",
    "        print(\"\\n\".join(bad_urls))\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7038b7-c8f3-4ab8-83d5-f06c7e6f2909",
   "metadata": {},
   "source": [
    "Using this approach, it took me five minutes to go through the list  100 URLs. I didn't get every webpage, and I usually also run it twice on the same list to catch URLs that were missed either because of errors on my end or in the cloud.\n",
    "\n",
    "The main delay is slow-loading pages. I have the timeout arbitrarily set to 30 seconds. Setting in longer might load one or two more more pages, but would also slow down the process since some pages will never load. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
