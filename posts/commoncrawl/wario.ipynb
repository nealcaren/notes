{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: 60,000 news articles from April 20, 2024\n",
    "date: 2024-05-12\n",
    "description: Working with the Common Crawl News dataset\n",
    "categories: [Common Crawl, Web Scraping]\n",
    "draft: false\n",
    "image: spider.webp\n",
    "author-meta: Neal Caren, Associate Professor, Department of Sociology, University of North Carolina, Chapel Hill\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've always been intrigued by the idea of the Common Crawl News dataset. Since 2016, the non-profit organization Common Crawl has been collecting a specialized dataset of media articles, with new releases every day. While this dataset is smaller than the full Common Crawl, which currently stands at 454 terabytes and contains around 3 billion web pages, it is still a potentially useful and underutilized resource, both in terms of size and the process of extracting the news stories.\n",
    "\n",
    "Below, you'll find my code for downloading one day's worth of data from the Common Crawl News dataset, which amounts to approximately 20 gigabytes. After running the code, I obtained 59,866 English language articles for April 20th, 2024. The process was time-consuming, taking about eight hours, with most of that time spent on extracting the text from the WARC (Web ARChive) files. \n",
    "\n",
    "I attempted to optimize the code and speed up the extraction process, but most of those efforts were unsuccessful, likely because the parts I was trying to parallelize were not the primary bottlenecks. It's important to note that a significant portion of the articles in the dataset are likely to be spam or press releases. However, I believe there is enough valuable content within the dataset to make it a worthwhile resource for those willing to invest the time and effort to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: warcio in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (1.7.4)\n",
      "Requirement already satisfied: news-please in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (1.5.44)\n",
      "Requirement already satisfied: six in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from warcio) (1.16.0)\n",
      "Requirement already satisfied: Scrapy>=1.1.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (2.11.1)\n",
      "Requirement already satisfied: PyMySQL>=0.7.9 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (1.1.0)\n",
      "Requirement already satisfied: psycopg2-binary>=2.8.4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (2.9.9)\n",
      "Requirement already satisfied: hjson>=1.5.8 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (3.1.0)\n",
      "Requirement already satisfied: elasticsearch>=2.4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (8.13.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.3.2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (4.12.3)\n",
      "Requirement already satisfied: readability-lxml>=0.6.2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (0.8.1)\n",
      "Requirement already satisfied: newspaper3k>=0.2.8 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (0.2.8)\n",
      "Requirement already satisfied: langdetect>=1.0.7 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (1.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.4.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (2.8.2)\n",
      "Requirement already satisfied: plac>=0.9.6 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (1.4.3)\n",
      "Requirement already satisfied: dotmap>=1.2.17 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (1.3.30)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (2.0.7)\n",
      "Requirement already satisfied: ago>=0.0.9 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (0.0.95)\n",
      "Requirement already satisfied: lxml>=3.3.5 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (5.1.0)\n",
      "Requirement already satisfied: hurry.filesize>=0.9 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (0.9)\n",
      "Requirement already satisfied: bs4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (0.0.2)\n",
      "Requirement already satisfied: faust-cchardet>=2.1.18 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (2.1.19)\n",
      "Requirement already satisfied: boto3 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from news-please) (1.34.90)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from beautifulsoup4>=4.3.2->news-please) (2.5)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from elasticsearch>=2.4->news-please) (8.13.0)\n",
      "Requirement already satisfied: setuptools in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from hurry.filesize>=0.9->news-please) (68.2.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (10.2.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (1.2.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (2.31.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (5.1.1)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (0.35.1)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k>=0.2.8->news-please) (0.3)\n",
      "Requirement already satisfied: chardet in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from readability-lxml>=0.6.2->news-please) (5.2.0)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (24.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (42.0.5)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (1.2.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (1.9.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (24.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (24.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (6.3)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (0.3.1)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (0.8.0)\n",
      "Requirement already satisfied: packaging in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Scrapy>=1.1.0->news-please) (23.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.90 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from boto3->news-please) (1.34.90)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from boto3->news-please) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from boto3->news-please) (0.10.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.90->boto3->news-please) (2.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from cryptography>=36.0.0->Scrapy>=1.1.0->news-please) (1.16.0)\n",
      "Requirement already satisfied: certifi in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from elastic-transport<9,>=8.13->elasticsearch>=2.4->news-please) (2023.11.17)\n",
      "Requirement already satisfied: sgmllib3k in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from feedparser>=5.2.1->newspaper3k>=0.2.8->news-please) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k>=0.2.8->news-please) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k>=0.2.8->news-please) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k>=0.2.8->news-please) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k>=0.2.8->news-please) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k>=0.2.8->news-please) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k>=0.2.8->news-please) (3.6)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please) (23.2.0)\n",
      "Requirement already satisfied: pyasn1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please) (0.4.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->news-please) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k>=0.2.8->news-please) (3.13.1)\n",
      "Requirement already satisfied: automat>=0.8.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please) (22.10.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please) (22.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please) (4.9.0)\n",
      "Requirement already satisfied: pycparser in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->Scrapy>=1.1.0->news-please) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install warcio news-please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import warcio\n",
    "from lxml.etree import ParserError\n",
    "from newsplease import NewsPlease, EmptyResponseError\n",
    "from warcio.archiveiterator import ArchiveIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COMMON_CRAWL_DATA_DIR = \"./commoncrawl-data\"\n",
    "COMMON_CRAWL_DATA_DIR = os.environ.get(\n",
    "    \"COMMON_CRAWL_DATA_DIR\", DEFAULT_COMMON_CRAWL_DATA_DIR\n",
    ")\n",
    "\n",
    "DEFAULT_WARC_EXTRACT_DIR = \"warc-extract\"\n",
    "WARC_EXTRACT_DIR = os.environ.get(\"WARC_EXTRACT_DIR\", DEFAULT_WARC_EXTRACT_DIR)\n",
    "\n",
    "DEFAULT_PROCESSED_CONTENT_DIR = \"processed-content\"\n",
    "PROCESSED_CONTENT_DIR = os.environ.get(\n",
    "    \"PROCESSED_CONTENT_DIR\", DEFAULT_PROCESSED_CONTENT_DIR\n",
    ")\n",
    "\n",
    "JSON_OUT_FILE_EXT = \".json\"\n",
    "\n",
    "COMMON_CRAWL_BUCKET = \"commoncrawl\"\n",
    "COMMON_CRAWL_CC_NEWS_PREFIX = \"crawl-data/CC-NEWS\"\n",
    "now = datetime.now()\n",
    "CC_DATA_ROOT = f\"https://data.commoncrawl.org/\"\n",
    "\n",
    "WARC_LISTING_FILE_URL = f\"{CC_DATA_ROOT}{COMMON_CRAWL_CC_NEWS_PREFIX}/{now.year}/{now.strftime('%m')}/warc.paths.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, destination_path):\n",
    "    \"\"\"\n",
    "    Download a file from a URL to a local destination path using the requests library.\n",
    "\n",
    "    Args:\n",
    "    - url (str): URL of the file to download.\n",
    "    - destination_path (str): Local path to save the downloaded file.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(destination_path, \"wb\") as f:\n",
    "        shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "    print(f\"Downloaded {os.path.basename(destination_path)}\")\n",
    "\n",
    "\n",
    "def download_warc_files(date, base_dir):\n",
    "    \"\"\"\n",
    "    Downloads all WARC files for a specific date into a designated directory, ensuring no duplicates.\n",
    "    \"\"\"\n",
    "    COMMON_CRAWL_CC_NEWS_PREFIX = \"crawl-data/CC-NEWS\"\n",
    "    CC_DATA_ROOT = \"https://data.commoncrawl.org/\"\n",
    "    date_folder = date.strftime(\"%Y-%m-%d\")\n",
    "    day_dir = os.path.join(base_dir, date_folder)\n",
    "\n",
    "    os.makedirs(day_dir, exist_ok=True)\n",
    "\n",
    "    warc_listing_url = f\"{CC_DATA_ROOT}{COMMON_CRAWL_CC_NEWS_PREFIX}/{date.year}/{date.strftime('%m')}/warc.paths.gz\"\n",
    "    with urllib.request.urlopen(warc_listing_url) as response:\n",
    "        with gzip.open(response, \"rb\") as decompressed_file:\n",
    "            return process_warc_listing(\n",
    "                decompressed_file, CC_DATA_ROOT, day_dir, date.strftime(\"%Y%m%d\")\n",
    "            )\n",
    "\n",
    "\n",
    "def process_warc_listing(decompressed_file, data_root, day_dir, date_str):\n",
    "    \"\"\"\n",
    "    Process each line in the decompressed warc listing to download files.\n",
    "    \"\"\"\n",
    "    files_downloaded = []\n",
    "    for line in decompressed_file:\n",
    "        if date_str in str(line):\n",
    "            file_url = data_root + str(line.strip(), \"utf-8\")\n",
    "            destination_path = os.path.join(day_dir, os.path.basename(file_url))\n",
    "            if not os.path.exists(destination_path):\n",
    "                download_file(file_url, destination_path)\n",
    "            else:\n",
    "                print(f\"File already exists: {os.path.basename(destination_path)}\")\n",
    "            files_downloaded.append(destination_path)\n",
    "    return files_downloaded\n",
    "\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"Check if the text is English.\"\"\"\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def file_exists(file_path):\n",
    "    \"\"\"Check if the file exists.\"\"\"\n",
    "    return os.path.exists(file_path)\n",
    "\n",
    "\n",
    "def is_relevant_domain(url):\n",
    "    \"\"\"Check if the domain is relevant based on the specified criteria.\"\"\"\n",
    "    domain = urlparse(url).netloc\n",
    "    return (\n",
    "        (domain.endswith(\".org\") or domain.endswith(\".com\"))\n",
    "        and \"hindustan\" not in url\n",
    "        and \"minga.\" not in url\n",
    "    )\n",
    "\n",
    "\n",
    "def is_english_and_has_text(article):\n",
    "    \"\"\"Check if the article is in English and has text.\"\"\"\n",
    "    return article and article.language == \"en\" and article.maintext\n",
    "\n",
    "\n",
    "def process_record(record):\n",
    "    \"\"\"Process a single record, extracting the article if conditions are met.\"\"\"\n",
    "    if record.rec_type == \"response\" and \"html\" in record.http_headers.get_header(\n",
    "        \"Content-Type\", \"\"\n",
    "    ):\n",
    "        url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "        if is_relevant_domain(url):\n",
    "            try:\n",
    "                article = NewsPlease.from_warc(record)\n",
    "                if is_english_and_has_text(article):\n",
    "                    print(article.title)\n",
    "                    return article.get_serializable_dict()\n",
    "            except (EmptyResponseError, ParserError):\n",
    "                print(\"Blank!!!\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_articles_from_warc(file_path):\n",
    "    \"\"\"Extracts articles from a WARC file, filtering for English language and .org/.com domains.\"\"\"\n",
    "    articles = []\n",
    "    if file_exists(file_path):\n",
    "        with open(file_path, \"rb\") as stream:\n",
    "            for record in ArchiveIterator(stream):\n",
    "                article = process_record(record)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "    return articles\n",
    "\n",
    "\n",
    "def save_articles_to_json(articles, output_file):\n",
    "    \"\"\"\n",
    "    Saves a list of articles to a JSON file using pandas, which automatically handles datetime serialization.\n",
    "    \"\"\"\n",
    "    # Convert the list of dictionaries (articles) to a DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Save the DataFrame to a JSON file\n",
    "    df.to_json(output_file, orient=\"records\", lines=True, date_format=\"iso\")\n",
    "\n",
    "\n",
    "def process_warc_file(warc_file):\n",
    "    \"\"\"\n",
    "    Processes a single WARC file, extracts articles, and saves them to a JSON file.\n",
    "    This function only runs if the corresponding JSON file does not already exist.\n",
    "    \"\"\"\n",
    "    json_filename = os.path.splitext(warc_file)[0] + \".json\"\n",
    "\n",
    "    # Check if the JSON file already exists\n",
    "    if os.path.exists(json_filename):\n",
    "        return f\"JSON file already exists. Skipped processing for: {json_filename}\"\n",
    "\n",
    "    # Extract articles from the WARC file if JSON does not exist\n",
    "    articles = extract_articles_from_warc(warc_file)\n",
    "    if articles:  # Ensure there are articles to write\n",
    "        df = pd.DataFrame(articles)\n",
    "        df.to_json(json_filename, orient=\"records\", lines=True, date_format=\"iso\")\n",
    "        return f\"Saved articles to {json_filename}\"\n",
    "    else:\n",
    "        return f\"No articles found in {warc_file}. Nothing was saved.\"\n",
    "\n",
    "\n",
    "def process_date(date_str, base_dir, num_workers=None):\n",
    "    \"\"\"\n",
    "    Processes all WARC files for a given date using threads to speed up IO-bound tasks.\n",
    "\n",
    "    Args:\n",
    "    - date_str (str): The date string in 'YYYY-MM-DD' format.\n",
    "    - base_dir (str): The base directory where WARC files are downloaded and processed.\n",
    "    - num_workers (int): The number of worker threads to use; defaults to the number of CPUs.\n",
    "    \"\"\"\n",
    "    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    warc_files = download_warc_files(date, base_dir)\n",
    "    num_workers = num_workers or os.cpu_count()\n",
    "\n",
    "    # Use ThreadPoolExecutor to process files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Map the process_warc_file function to each WARC file\n",
    "        futures = {\n",
    "            executor.submit(process_warc_file, warc_file): warc_file\n",
    "            for warc_file in warc_files\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: CC-NEWS-20240420012526-03589.warc.gz\n",
      "File already exists: CC-NEWS-20240420031225-03590.warc.gz\n",
      "File already exists: CC-NEWS-20240420044652-03591.warc.gz\n",
      "File already exists: CC-NEWS-20240420061440-03592.warc.gz\n",
      "File already exists: CC-NEWS-20240420073737-03593.warc.gz\n",
      "File already exists: CC-NEWS-20240420085814-03594.warc.gz\n",
      "File already exists: CC-NEWS-20240420101143-03595.warc.gz\n",
      "File already exists: CC-NEWS-20240420113539-03596.warc.gz\n",
      "File already exists: CC-NEWS-20240420125652-03597.warc.gz\n",
      "File already exists: CC-NEWS-20240420141952-03598.warc.gz\n",
      "File already exists: CC-NEWS-20240420153927-03599.warc.gz\n",
      "File already exists: CC-NEWS-20240420165810-03600.warc.gz\n",
      "File already exists: CC-NEWS-20240420182948-03601.warc.gz\n",
      "File already exists: CC-NEWS-20240420195655-03602.warc.gz\n",
      "File already exists: CC-NEWS-20240420214148-03603.warc.gz\n",
      "File already exists: CC-NEWS-20240420234347-03604.warc.gz\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "process_date(\"2024-04-20\", \"commoncrawl-data/\", num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "df_names = glob('commoncrawl-data/2024-04-20/*.json')\n",
    "dfs = [pd.read_json(df_name, lines=True) for df_name in df_names]\n",
    "df= pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59866"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>published</th>\n",
       "      <th>text</th>\n",
       "      <th>authors</th>\n",
       "      <th>date_download</th>\n",
       "      <th>date_modify</th>\n",
       "      <th>date_publish</th>\n",
       "      <th>description</th>\n",
       "      <th>filename</th>\n",
       "      <th>image_url</th>\n",
       "      <th>language</th>\n",
       "      <th>localpath</th>\n",
       "      <th>maintext</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>title_page</th>\n",
       "      <th>title_rss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>After VW plant victory, UAW sets its sights on...</td>\n",
       "      <td>https://finance.yahoo.com/news/vw-plant-victor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Nora Eckert]</td>\n",
       "      <td>2024-04-20 13:44:57+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-04-20 13:06:58</td>\n",
       "      <td>CHATTANOOGA, Tennessee (Reuters) -The United A...</td>\n",
       "      <td>https%3A%2F%2Ffinance.yahoo.com%2Fnews%2Fvw-pl...</td>\n",
       "      <td>https://media.zenfs.com/en/reuters-finance.com...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(Fixes media identifier)\\nBy Nora Eckert\\nCHAT...</td>\n",
       "      <td>finance.yahoo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Donovan quoted in Business Times on US enforce...</td>\n",
       "      <td>https://www.atlanticcouncil.org/insight-impact...</td>\n",
       "      <td>2024-04-16T18:11:53.000</td>\n",
       "      <td>New Atlanticist\\nNew Atlanticist is where top ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Allegri says his players didn't understand wha...</td>\n",
       "      <td>https://www.juvefc.com/allegri-says-his-player...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Martin U]</td>\n",
       "      <td>2024-04-20 18:55:08+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-04-20 14:00:00</td>\n",
       "      <td>Juventus manager Max Allegri suggested that hi...</td>\n",
       "      <td>https%3A%2F%2Fwww.juvefc.com%2Fallegri-says-hi...</td>\n",
       "      <td>https://icdn.juvefc.com/wp-content/uploads/202...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Juventus manager Max Allegri suggested that hi...</td>\n",
       "      <td>www.juvefc.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2798  After VW plant victory, UAW sets its sights on...   \n",
       "265   Donovan quoted in Business Times on US enforce...   \n",
       "1126  Allegri says his players didn't understand wha...   \n",
       "\n",
       "                                                    url  \\\n",
       "2798  https://finance.yahoo.com/news/vw-plant-victor...   \n",
       "265   https://www.atlanticcouncil.org/insight-impact...   \n",
       "1126  https://www.juvefc.com/allegri-says-his-player...   \n",
       "\n",
       "                    published  \\\n",
       "2798                      NaN   \n",
       "265   2024-04-16T18:11:53.000   \n",
       "1126                      NaN   \n",
       "\n",
       "                                                   text        authors  \\\n",
       "2798                                                NaN  [Nora Eckert]   \n",
       "265   New Atlanticist\\nNew Atlanticist is where top ...            NaN   \n",
       "1126                                                NaN     [Martin U]   \n",
       "\n",
       "                  date_download date_modify         date_publish  \\\n",
       "2798  2024-04-20 13:44:57+00:00        None  2024-04-20 13:06:58   \n",
       "265                         NaN         NaN                  NaN   \n",
       "1126  2024-04-20 18:55:08+00:00        None  2024-04-20 14:00:00   \n",
       "\n",
       "                                            description  \\\n",
       "2798  CHATTANOOGA, Tennessee (Reuters) -The United A...   \n",
       "265                                                 NaN   \n",
       "1126  Juventus manager Max Allegri suggested that hi...   \n",
       "\n",
       "                                               filename  \\\n",
       "2798  https%3A%2F%2Ffinance.yahoo.com%2Fnews%2Fvw-pl...   \n",
       "265                                                 NaN   \n",
       "1126  https%3A%2F%2Fwww.juvefc.com%2Fallegri-says-hi...   \n",
       "\n",
       "                                              image_url language  localpath  \\\n",
       "2798  https://media.zenfs.com/en/reuters-finance.com...       en        NaN   \n",
       "265                                                 NaN      NaN        NaN   \n",
       "1126  https://icdn.juvefc.com/wp-content/uploads/202...       en        NaN   \n",
       "\n",
       "                                               maintext      source_domain  \\\n",
       "2798  (Fixes media identifier)\\nBy Nora Eckert\\nCHAT...  finance.yahoo.com   \n",
       "265                                                 NaN                NaN   \n",
       "1126  Juventus manager Max Allegri suggested that hi...     www.juvefc.com   \n",
       "\n",
       "      title_page  title_rss  \n",
       "2798         NaN        NaN  \n",
       "265          NaN        NaN  \n",
       "1126         NaN        NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
