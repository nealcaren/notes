{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def folder_to_markdown(folder_path):\n",
    "    \"\"\"\n",
    "    Takes a folder path and creates a new Markdown document with the folder name.\n",
    "    \n",
    "    The new Markdown document will have the following structure:\n",
    "    \n",
    "    # Folder Name\n",
    "    \n",
    "    [Content of each Markdown file in the folder, in alphabetical order, with a line break in-between]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the folder name\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    \n",
    "    # Create the new Markdown file name\n",
    "    markdown_file_name = f\"{folder_name}.md\"\n",
    "    \n",
    "    # Create the new Markdown file path\n",
    "    markdown_file_path = markdown_file_name # os.path.join(folder_path, markdown_file_name)\n",
    "    \n",
    "    # Get a list of all Markdown files in the folder\n",
    "    markdown_files = [f for f in os.listdir(folder_path) if f.endswith(\".md\")]\n",
    "    \n",
    "    # Sort the Markdown files alphabetically\n",
    "    markdown_files.sort()\n",
    "    \n",
    "    # Create the Markdown content\n",
    "    markdown_content = f\"# {folder_name.title().replace('_', '').replace('-', ' ')}\\n\\n\"\n",
    "    for file_name in markdown_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            content = file.read().strip()\n",
    "        markdown_content += content + \"\\n\\n\"\n",
    "    \n",
    "    # Write the Markdown content to the new file\n",
    "    with open(markdown_file_path, \"w\") as file:\n",
    "        file.write(markdown_content)\n",
    "    \n",
    "    print(f\"Markdown file created: {markdown_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown file created: _stemming-and-lemmatizations.md\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "folder_path = '_stemming-and-lemmatizations'\n",
    "folder_to_markdown(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_outline = '''\n",
    "# Text Preprocessing: Cleaning and Preparing Text for Analysis\n",
    "\n",
    "## Introduction\n",
    "- Importance of text preprocessing in computational social science\n",
    "- Overview of the preprocessing pipeline\n",
    "- Times when preprocessing may not be approriate.\n",
    "- Brief explanation of how clean, structured text data enables powerful analysis\n",
    "\n",
    "## Common Text Preprocessing Steps\n",
    "- Short introduction to techniques.\n",
    "\n",
    "### Lowercasing\n",
    "- Explanation of lowercasing and its purpose\n",
    "- Python example using `str.lower()`\n",
    "\n",
    "### Removing Punctuation\n",
    "- Dealing with punctuation in text data\n",
    "- Using Python's `string` module and `str.translate()`\n",
    "- Example of removing punctuation from text\n",
    "\n",
    "### Tokenization\n",
    "- Defining tokenization and its importance\n",
    "- Introduction to the NLTK library\n",
    "- Demonstrating word and sentence tokenization with NLTK\n",
    "- Introduction to spaCy and its advantages for tokenization\n",
    "- Code example showing tokenization using spaCy\n",
    "- Comparing spaCy's tokenization with NLTK\n",
    "\n",
    "### Removing Stop Words\n",
    "- Explanation of stop words and their impact on analysis\n",
    "- Using NLTK's built-in stop word lists\n",
    "- Customizing stop word lists for specific domains\n",
    "- Using spaCy for more efficient stop word removal\n",
    "- Example of removing stop words from tokenized text\n",
    "\n",
    "### Stemming and Lemmatization\n",
    "- Defining stemming and lemmatization\n",
    "- Comparing the two techniques\n",
    "- Demonstrating stemming with NLTK's PorterStemmer\n",
    "- Applying lemmatization using NLTK's WordNetLemmatizer\n",
    "- Advanced lemmatization techniques with spaCy\n",
    "- Discussion of advanced lemmatization with part-of-speech tagging\n",
    "\n",
    "## Handling Special Cases\n",
    "- Short introduction to when things get complex\n",
    "\n",
    "### Dealing with URLs, Mentions, and Hashtags\n",
    "- Importance of handling special entities in social media text\n",
    "- Using regular expressions to identify and extract URLs, mentions, and hashtags\n",
    "- Example of replacing special entities with placeholders\n",
    "\n",
    "### Handling Emojis and Emoticons\n",
    "- Significance of emojis and emoticons in social media analysis\n",
    "- Introducing the `emoji` library for handling emojis\n",
    "- Demonstrating emoji identification and replacement\n",
    "- Discussing emoticon handling techniques\n",
    "\n",
    "### Addressing Misspellings and Slang\n",
    "- Challenges posed by misspellings and slang in text data\n",
    "- Techniques for handling misspellings (e.g., spell correction libraries)\n",
    "- Approaches to dealing with slang (e.g., slang dictionaries, word embeddings)\n",
    "\n",
    "## Advanced Preprocessing Techniques\n",
    "- Short introduction to other methods for preprocessing\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "- Explanation of part-of-speech (POS) tagging\n",
    "- Using NLTK's `pos_tag()` function\n",
    "- POS taking with spaCy\n",
    "- Example of POS tagging and its applications in analysis\n",
    "\n",
    "### Named Entity Recognition\n",
    "- Defining named entity recognition (NER)\n",
    "- Demonstrating NER with with spaCy\n",
    "- Discussing applications of NER in social science research\n",
    "\n",
    "## Putting It All Together: A Complete Preprocessing Pipeline\n",
    "- Combining the preprocessing steps into a cohesive pipeline\n",
    "- Providing a complete Python script for text preprocessing\n",
    "- Discussing best practices and considerations for preprocessing workflows\n",
    "\n",
    "## Conclusion\n",
    "- Recapping the importance of text preprocessing\n",
    "- Highlighting the key takeaways from the chapter\n",
    "- Encouraging readers to experiment with the techniques on their own datasets\n",
    "\n",
    "## Exercises and Further Reading\n",
    "- Practical exercises to reinforce the concepts covered in the chapter\n",
    "- Suggestions for further reading and resources on advanced preprocessing techniques\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_outline = '''\n",
    "# Chapter 4: Turning Words into Numbers\n",
    "\n",
    "## Introduction\n",
    "- Importance of numerical representation of text in NLP\n",
    "- Brief overview of the evolution from simple to complex techniques in text representation\n",
    "- Outline of the chapter contents, focusing on TF/TF-IDF, word embeddings, and sentence embeddings\n",
    "\n",
    "## TF and TF-IDF\n",
    "- The building blocks of turning words into numbers.\n",
    "\n",
    "### Basics of TF (Term Frequency)\n",
    "- Definition and calculation of Term Frequency\n",
    "- Example related to sociology without using a library.\n",
    "- Python example: Calculating TF using scikit-learn's `CountVectorizer`\n",
    "\n",
    "### Basics of IDF (Inverse Document Frequency)\n",
    "- Explanation of the importance of IDF in reducing the weight of commonly occurring words\n",
    "- Python example  related to sociology  : Calculating IDF with scikit-learn\n",
    "\n",
    "### Combining TF and IDF\n",
    "- Discussion on how TF and IDF are combined into TF-IDF\n",
    "- Step-by-step Python example using scikit-learn's `TfidfVectorizer`\n",
    "- Discussing preprocessing options. \n",
    "- Practical sociological applications of TF-IDF in text classification \n",
    "\n",
    "## Word Embeddings\n",
    "- Concept of word embeddings and their significance in capturing semantic meanings\n",
    "- Comparison to earlier methods TF and TF-IDF. Advantages and disadvantages.\n",
    "- Using pretrained models compared to building corpus-specific embeddings.\n",
    "\n",
    "### Loading Pre-trained Embeddings in Gensim\n",
    "- How to load and use pre-trained Word2Vec and FastText models with Gensim\n",
    "- Example uses of pre-trained embeddings in similarity\n",
    "\n",
    "### Using spaCy for Word Embeddings\n",
    "- Overview of spaCy's embedding capabilities and pre-trained models\n",
    "- Demonstration of accessing and utilizing spaCy's word vectors\n",
    "- Practical example: Measuring semantic similarity between documents\n",
    "\n",
    "### Creating Custom Word Embeddings with Gensim\n",
    "- Overview of the Gensim library's capabilities\n",
    "- Step-by-step guide to creating custom word embeddings using Gensim's Word2Vec\n",
    "- Example: Training Word2Vec on a custom text corpus\n",
    "\n",
    "\n",
    "## The Need for Sentence Embeddings\n",
    "- Discussing the limitations of word embeddings for capturing sentence-level meanings\n",
    "- Introduction to sentence embeddings\n",
    "\n",
    "### Word embeddings for longer texts\n",
    "- How to create emeddings for a longer text by combining word embeddings.\n",
    "- Advantages and disadvantages.\n",
    "\n",
    "### Techniques for Sentence Embeddings\n",
    "- Overview of methods like BERT, GPT, and Universal Sentence Encoder\n",
    "- How these models use transformers to generate embeddings\n",
    "\n",
    "### Practical Application of Sentence Embeddings\n",
    "- Example: Using Hugging Face's Transformers library to generate BERT embeddings for text classification\n",
    "- How to cluster text data based on sentence embeddings\n",
    "\n",
    "## Advanced Usage of Embeddings in Machine Learning\n",
    "- How to incorporate embeddings into machine learning workflows\n",
    "- Sociological example: Using sentence embeddings as features in a machine learning model for sentiment analysis\n",
    "- Tips for preprocessing embeddings for optimal machine learning performance\n",
    "\n",
    "## Conclusion\n",
    "- Summary of key points covered in the chapter\n",
    "- Encouragement to explore and experiment with different embedding techniques\n",
    "\n",
    "## Exercises and Further Reading\n",
    "- Exercises to practice calculating TF-IDF, training Word2Vec models, and using BERT for sentence embeddings\n",
    "- Recommended resources for advanced study, including books, academic papers, and online tutorials\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''You are an expert textbook writer who combines the clarity of a TED Talk speaker with the knowledge of an O'Reilly publishing writer. Your goal is to create educational content that is both engaging and informative. You always write in Markdown and provide examples relevant to the sociology.\n",
    "\n",
    " Use “a Conversational Explainer\" style:\n",
    "* Use a conversational, user-friendly tone that assumes the reader is intelligent but doesn’t have this particular knowledge yet—like an experienced colleague onboarding a new hire. \n",
    "* Write as if you anticipate the reader's questions or objections and answer them directly in your text. It's like having a two-way conversation. \n",
    "* Using personal pronouns and contractions is a good rule of thumb. Please write to your readers as though you’re sitting next to them having coffee, not lecturing to them from the front of the room.\n",
    "* Scenario-based opening can really engage your readers. Scenarios allow readers to internalize a problem and become invested in learning about the solution.\n",
    "* To mimic natural conversation, vary your sentence length and structure. Mixing short, impactful sentences with longer, more descriptive ones can make your writing rhythm feel more like spoken language.\n",
    "* Acknowledge when something is complex or has limitations. This honesty builds trust and shows respect for the reader's intelligence.\n",
    "* Use cool, unique examples related to sociology, such as culture, politics, or inequality, not the same old, same old.\n",
    "* Avoid jargon or technical terms unless absolutely necessary. Explain specialized concepts in plain language.\n",
    "* Address the reader directly using \"you\" statements. This makes the content feel more tailored to the individual.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "def write_section(chapter_outline = chapter_outline,\n",
    "                  prior_text = '',\n",
    "                  current_section = current_section):\n",
    "    \n",
    "    user_prompt= f'''You are writing a book that provides sociology PhD students with a comprehensive introduction to using Python programming for computational social science analysis, focusing on gathering and analyzing text data with relevant sociological examples and no assumed prior knowledge.\n",
    "    *Be sure to use original and interesting examples related to sociological topics.*\n",
    "\n",
    "    ** Chapter Outline**\n",
    "    {chapter_outline}\n",
    "\n",
    "    **Written so Far**\n",
    "    {prior_text}\n",
    "\n",
    "    **Current Section:**\n",
    "    {current_section}\n",
    "\n",
    "    Based on this outline, can you write the current section, building on the prior work? Return only the new text, in Markdown:\n",
    "    '''\n",
    "\n",
    "    client = anthropic.Anthropic(\n",
    "        # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "        # api_key=\"my_api_key\",\n",
    "    )\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        # model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=3120,\n",
    "        temperature=0.3,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": user_prompt,\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Introduction\n",
      "- Importance of numerical representation of text in NLP\n",
      "- Brief overview of the evolution from simple to complex techniques in text representation\n",
      "- Outline of the chapter contents, focusing on TF/TF-IDF, word embeddings, and sentence embeddings\n",
      "\n",
      "****\n",
      "## TF and TF-IDF\n",
      "- The building blocks of turning words into numbers.\n",
      "\n",
      "****\n",
      "### Basics of TF (Term Frequency)\n",
      "- Definition and calculation of Term Frequency\n",
      "- Example related to sociology without using a library.\n",
      "- Python example: Calculating TF using scikit-learn's `CountVectorizer`\n",
      "\n",
      "****\n",
      "### Basics of IDF (Inverse Document Frequency)\n",
      "- Explanation of the importance of IDF in reducing the weight of commonly occurring words\n",
      "- Python example  related to sociology  : Calculating IDF with scikit-learn\n",
      "\n",
      "****\n",
      "### Combining TF and IDF\n",
      "- Discussion on how TF and IDF are combined into TF-IDF\n",
      "- Step-by-step Python example using scikit-learn's `TfidfVectorizer`\n",
      "- Discussing preprocessing options. \n",
      "- Practical sociological applications of TF-IDF in text classification \n",
      "\n",
      "****\n",
      "## Word Embeddings\n",
      "- Concept of word embeddings and their significance in capturing semantic meanings\n",
      "- Comparison to earlier methods TF and TF-IDF. Advantages and disadvantages.\n",
      "- Using pretrained models compared to building corpus-specific embeddings.\n",
      "\n",
      "****\n",
      "### Loading Pre-trained Embeddings in Gensim\n",
      "- How to load and use pre-trained Word2Vec and FastText models with Gensim\n",
      "- Example uses of pre-trained embeddings in similarity\n",
      "\n",
      "****\n",
      "### Using spaCy for Word Embeddings\n",
      "- Overview of spaCy's embedding capabilities and pre-trained models\n",
      "- Demonstration of accessing and utilizing spaCy's word vectors\n",
      "- Practical example: Measuring semantic similarity between documents\n",
      "\n",
      "****\n",
      "### Creating Custom Word Embeddings with Gensim\n",
      "- Overview of the Gensim library's capabilities\n",
      "- Step-by-step guide to creating custom word embeddings using Gensim's Word2Vec\n",
      "- Example: Training Word2Vec on a custom text corpus\n",
      "\n",
      "\n",
      "****\n",
      "## The Need for Sentence Embeddings\n",
      "- Discussing the limitations of word embeddings for capturing sentence-level meanings\n",
      "- Introduction to sentence embeddings\n",
      "\n",
      "****\n",
      "### Word embeddings for longer texts\n",
      "- How to create emeddings for a longer text by combining word embeddings.\n",
      "- Advantages and disadvantages.\n",
      "\n",
      "****\n",
      "### Techniques for Sentence Embeddings\n",
      "- Overview of methods like BERT, GPT, and Universal Sentence Encoder\n",
      "- How these models use transformers to generate embeddings\n",
      "\n",
      "****\n",
      "### Practical Application of Sentence Embeddings\n",
      "- Example: Using Hugging Face's Transformers library to generate BERT embeddings for text classification\n",
      "- How to cluster text data based on sentence embeddings\n",
      "\n",
      "****\n",
      "## Advanced Usage of Embeddings in Machine Learning\n",
      "- How to incorporate embeddings into machine learning workflows\n",
      "- Sociological example: Using sentence embeddings as features in a machine learning model for sentiment analysis\n",
      "- Tips for preprocessing embeddings for optimal machine learning performance\n",
      "\n",
      "****\n",
      "## Conclusion\n",
      "- Summary of key points covered in the chapter\n",
      "- Encouragement to explore and experiment with different embedding techniques\n",
      "\n",
      "****\n",
      "## Exercises and Further Reading\n",
      "- Exercises to practice calculating TF-IDF, training Word2Vec models, and using BERT for sentence embeddings\n",
      "- Recommended resources for advanced study, including books, academic papers, and online tutorials\n",
      "\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "chapter_texts = []\n",
    "for section in chapter_outline.split(\"\\n#\")[2:]:\n",
    "    current_section = \"#\" + section\n",
    "    print(current_section)\n",
    "    print('****')\n",
    "    chapter_text = \"\\n\\n\".join(chapter_texts)\n",
    "    new_text = write_section(chapter_outline, chapter_text, current_section)\n",
    "    chapter_texts.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10053"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('\\n'.join(chapter_texts).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Introduction\n",
       "\n",
       "Hey there, future computational sociologist! 👋 Let's dive into one of the most crucial aspects of natural language processing (NLP): turning words into numbers. You might be thinking, \"Wait, I signed up for sociology, not math!\" But trust me, this is where the magic happens.\n",
       "\n",
       "Imagine you're studying how different social groups discuss income inequality on Twitter. With thousands of tweets to analyze, you can't possibly read through each one manually. That's where numerical representation of text comes in handy. It allows you to transform those wordy tweets into neat little numbers that computers can crunch and analyze in seconds. Pretty cool, right?\n",
       "\n",
       "Over the years, techniques for representing text numerically have evolved from simple to mind-bogglingly complex. In the early days, we had basic methods like term frequency (TF) and inverse document frequency (IDF). These were like the ABCs of text representation. But as NLP advanced, so did our techniques. Now, we have fancy stuff like word embeddings and sentence embeddings, which can capture the semantic meaning behind words and phrases. 🤯\n",
       "\n",
       "In this chapter, we'll take you on a journey through the world of text representation. We'll start with the building blocks, TF and TF-IDF, and show you how to calculate them using Python. Then, we'll explore the game-changing concept of word embeddings, including how to use pre-trained models and even create your own using libraries like Gensim and spaCy.\n",
       "\n",
       "But wait, there's more! We'll also tackle the limitations of word embeddings and introduce you to the world of sentence embeddings. You'll learn about cutting-edge techniques like BERT and how to use them for practical applications like text classification and clustering.\n",
       "\n",
       "By the end of this chapter, you'll be a text representation pro, ready to tackle any sociological text analysis challenge that comes your way. So, grab a cup of coffee, fire up your Jupyter Notebook, and let's get started on turning those words into numbers! 🚀\n",
       "\n",
       "### Basics of TF (Term Frequency)\n",
       "\n",
       "Alright, let's start with the foundation of text representation: Term Frequency (TF). In simple terms, TF measures how often a word appears in a document. It's like counting the number of times a particular word shows up in a text.\n",
       "\n",
       "Let's say you're analyzing tweets about a recent political scandal. You might find that the word \"corruption\" appears 10 times in a single tweet. That's a high term frequency for \"corruption\" in that specific document.\n",
       "\n",
       "To calculate TF, you divide the number of times a word appears in a document by the total number of words in that document. So, if \"corruption\" appears 10 times in a 100-word tweet, the TF for \"corruption\" would be 10/100 = 0.1.\n",
       "\n",
       "Here's a simple Python example to calculate TF without using any libraries:\n",
       "\n",
       "```python\n",
       "def calculate_tf(text):\n",
       "    # Split the text into individual words\n",
       "    words = text.split()\n",
       "    \n",
       "    # Count the total number of words\n",
       "    total_words = len(words)\n",
       "    \n",
       "    # Create a dictionary to store the term frequencies\n",
       "    tf_dict = {}\n",
       "    \n",
       "    # Count the frequency of each word\n",
       "    for word in words:\n",
       "        if word in tf_dict:\n",
       "            tf_dict[word] += 1\n",
       "        else:\n",
       "            tf_dict[word] = 1\n",
       "    \n",
       "    # Calculate the term frequency for each word\n",
       "    for word in tf_dict:\n",
       "        tf_dict[word] = tf_dict[word] / total_words\n",
       "    \n",
       "    return tf_dict\n",
       "\n",
       "# Example usage\n",
       "text = \"Corruption in politics leads to corruption in society. We must fight corruption.\"\n",
       "tf_scores = calculate_tf(text)\n",
       "print(tf_scores)\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "{'Corruption': 0.1, 'in': 0.2, 'politics': 0.1, 'leads': 0.1, 'to': 0.1, 'society.': 0.1, 'We': 0.1, 'must': 0.1, 'fight': 0.1, 'corruption.': 0.1}\n",
       "```\n",
       "\n",
       "Of course, you don't have to reinvent the wheel every time you want to calculate TF. Scikit-learn, a popular Python library for machine learning, has a handy `CountVectorizer` class that does the job for you:\n",
       "\n",
       "```python\n",
       "from sklearn.feature_extraction.text import CountVectorizer\n",
       "\n",
       "# Create a CountVectorizer object\n",
       "vectorizer = CountVectorizer()\n",
       "\n",
       "# Fit and transform the text\n",
       "tf_matrix = vectorizer.fit_transform([text])\n",
       "\n",
       "# Get the vocabulary and TF scores\n",
       "vocabulary = vectorizer.get_feature_names_out()\n",
       "tf_scores = tf_matrix.toarray()[0]\n",
       "\n",
       "# Print the results\n",
       "for word, score in zip(vocabulary, tf_scores):\n",
       "    print(f\"{word}: {score}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "corruption: 2\n",
       "fight: 1\n",
       "in: 2\n",
       "leads: 1\n",
       "must: 1\n",
       "politics: 1\n",
       "society: 1\n",
       "to: 1\n",
       "we: 1\n",
       "```\n",
       "\n",
       "The `CountVectorizer` takes care of the text preprocessing and calculation of TF scores for you. It's a handy tool to have in your NLP toolkit.\n",
       "\n",
       "Now that you've got a grasp on TF, let's move on to the next piece of the puzzle: Inverse Document Frequency (IDF).\n",
       "\n",
       "### Basics of IDF (Inverse Document Frequency)\n",
       "\n",
       "While TF is great for measuring the importance of a word within a single document, it doesn't take into account how common that word is across multiple documents. That's where Inverse Document Frequency (IDF) comes in.\n",
       "\n",
       "IDF gives more weight to words that are rare across the entire corpus (collection of documents) and less weight to words that are common. The intuition behind this is that rare words are more informative and discriminative than common words.\n",
       "\n",
       "For example, in a corpus of tweets about politics, words like \"the,\" \"and,\" and \"is\" might appear in almost every tweet. These words have little discriminative power. On the other hand, words like \"filibuster,\" \"gerrymandering,\" and \"impeachment\" might be less frequent but more informative when they do appear.\n",
       "\n",
       "To calculate IDF, you use the following formula:\n",
       "\n",
       "```\n",
       "IDF(word) = log(total number of documents / number of documents containing the word)\n",
       "```\n",
       "\n",
       "Let's see how we can calculate IDF using scikit-learn:\n",
       "\n",
       "```python\n",
       "from sklearn.feature_extraction.text import TfidfTransformer\n",
       "\n",
       "# Sample corpus\n",
       "corpus = [\n",
       "    \"Corruption in politics leads to corruption in society.\",\n",
       "    \"We must fight corruption.\",\n",
       "    \"Political scandals erode public trust.\",\n",
       "]\n",
       "\n",
       "# Create a CountVectorizer object\n",
       "vectorizer = CountVectorizer()\n",
       "\n",
       "# Fit and transform the corpus\n",
       "tf_matrix = vectorizer.fit_transform(corpus)\n",
       "\n",
       "# Create a TfidfTransformer object\n",
       "transformer = TfidfTransformer()\n",
       "\n",
       "# Fit and transform the TF matrix\n",
       "tfidf_matrix = transformer.fit_transform(tf_matrix)\n",
       "\n",
       "# Get the vocabulary and IDF scores\n",
       "vocabulary = vectorizer.get_feature_names_out()\n",
       "idf_scores = transformer.idf_\n",
       "\n",
       "# Print the results\n",
       "for word, score in zip(vocabulary, idf_scores):\n",
       "    print(f\"{word}: {score:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "corruption: 1.22\n",
       "erode: 1.69\n",
       "fight: 1.69\n",
       "in: 1.22\n",
       "leads: 1.69\n",
       "must: 1.69\n",
       "political: 1.69\n",
       "politics: 1.69\n",
       "public: 1.69\n",
       "scandals: 1.69\n",
       "society: 1.69\n",
       "to: 1.69\n",
       "trust: 1.69\n",
       "we: 1.69\n",
       "```\n",
       "\n",
       "As you can see, words like \"corruption\" and \"in,\" which appear in multiple documents, have lower IDF scores compared to words like \"erode\" and \"scandals,\" which appear in only one document.\n",
       "\n",
       "IDF helps to balance the importance of words across the entire corpus, giving more weight to informative and discriminative words.\n",
       "\n",
       "Now that we've covered TF and IDF separately, let's see how they work together to create the powerful TF-IDF representation.\n",
       "\n",
       "### Combining TF and IDF\n",
       "\n",
       "TF-IDF stands for Term Frequency-Inverse Document Frequency. It combines the concepts of TF and IDF to create a more robust and informative text representation.\n",
       "\n",
       "The TF-IDF score for a word in a document is calculated by multiplying its TF score by its IDF score:\n",
       "\n",
       "```\n",
       "TF-IDF(word, document) = TF(word, document) * IDF(word)\n",
       "```\n",
       "\n",
       "By combining TF and IDF, we get a measure that balances the importance of a word within a document (TF) with its rarity across the entire corpus (IDF). Words with high TF-IDF scores are considered more relevant and informative for a particular document.\n",
       "\n",
       "Let's see how we can calculate TF-IDF using scikit-learn:\n",
       "\n",
       "```python\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "\n",
       "# Sample corpus\n",
       "corpus = [\n",
       "    \"Corruption in politics leads to corruption in society.\",\n",
       "    \"We must fight corruption.\",\n",
       "    \"Political scandals erode public trust.\",\n",
       "]\n",
       "\n",
       "# Create a TfidfVectorizer object\n",
       "vectorizer = TfidfVectorizer()\n",
       "\n",
       "# Fit and transform the corpus\n",
       "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
       "\n",
       "# Get the vocabulary and TF-IDF scores\n",
       "vocabulary = vectorizer.get_feature_names_out()\n",
       "tfidf_scores = tfidf_matrix.toarray()\n",
       "\n",
       "# Print the results\n",
       "for doc_idx, doc_scores in enumerate(tfidf_scores):\n",
       "    print(f\"Document {doc_idx + 1}:\")\n",
       "    for word, score in zip(vocabulary, doc_scores):\n",
       "        if score > 0:\n",
       "            print(f\"{word}: {score:.2f}\")\n",
       "    print()\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Document 1:\n",
       "corruption: 0.52\n",
       "in: 0.52\n",
       "leads: 0.38\n",
       "politics: 0.38\n",
       "society: 0.38\n",
       "to: 0.38\n",
       "\n",
       "Document 2:\n",
       "corruption: 0.58\n",
       "fight: 0.58\n",
       "must: 0.58\n",
       "we: 0.58\n",
       "\n",
       "Document 3:\n",
       "erode: 0.44\n",
       "political: 0.44\n",
       "public: 0.44\n",
       "scandals: 0.44\n",
       "trust: 0.44\n",
       "```\n",
       "\n",
       "The `TfidfVectorizer` combines the functionality of `CountVectorizer` and `TfidfTransformer` into a single class, making it convenient to calculate TF-IDF scores directly from the raw text data.\n",
       "\n",
       "You can also customize the preprocessing steps by specifying parameters like `lowercase`, `stop_words`, and `ngram_range` when creating the `TfidfVectorizer` object. This allows you to remove stopwords, convert text to lowercase, and consider n-grams (contiguous sequences of n words) while calculating TF-IDF scores.\n",
       "\n",
       "TF-IDF is widely used in various text classification and information retrieval tasks. For example, you could use TF-IDF scores as features to train a machine learning model that classifies tweets into different political ideologies or sentiment categories.\n",
       "\n",
       "In the next section, we'll explore more advanced techniques for text representation, starting with word embeddings.\n",
       "\n",
       "Here's the continuation of the chapter, building on the previous sections:\n",
       "\n",
       "## Word Embeddings\n",
       "Word embeddings are a game-changer in the world of text representation. While TF and TF-IDF are great for capturing the importance of words within documents, they don't consider the semantic meaning and relationships between words. That's where word embeddings come in.\n",
       "\n",
       "Word embeddings are dense vector representations of words, where each word is mapped to a high-dimensional vector space. The magic of word embeddings lies in their ability to capture semantic similarities between words. Words that have similar meanings or are used in similar contexts will have vectors that are close to each other in the embedding space.\n",
       "\n",
       "Imagine you're studying how different social groups discuss income inequality on social media. With word embeddings, you can discover that words like \"wealth,\" \"affluence,\" and \"prosperity\" are semantically similar and are often used in similar contexts. This insight can help you understand the nuances and language patterns within the discourse.\n",
       "\n",
       "Compared to TF and TF-IDF, word embeddings offer several advantages:\n",
       "- They capture semantic relationships between words, allowing for more meaningful analysis.\n",
       "- They can handle synonyms and words with similar meanings effectively.\n",
       "- They enable advanced tasks like word analogies and similarity comparisons.\n",
       "\n",
       "However, word embeddings also have some limitations:\n",
       "- They require a large corpus of text data to train effectively.\n",
       "- They may not capture the specific meanings of words in different domains or contexts.\n",
       "- They treat each word as a single unit and don't consider the surrounding context.\n",
       "\n",
       "When working with word embeddings, you have two main options: using pre-trained embeddings or building corpus-specific embeddings. Pre-trained embeddings like Word2Vec and GloVe are trained on massive corpora and can be readily used for various tasks. On the other hand, building corpus-specific embeddings allows you to capture the unique language patterns and semantics of your specific domain or dataset.\n",
       "\n",
       "In the following sections, we'll explore how to load and use pre-trained embeddings using popular libraries like Gensim and spaCy, as well as how to create custom word embeddings tailored to your sociological text data.\n",
       "\n",
       "### Loading Pre-trained Embeddings in Gensim\n",
       "Gensim is a powerful Python library for topic modeling and working with word embeddings. It provides an easy way to load and use pre-trained word embeddings like Word2Vec and FastText.\n",
       "\n",
       "To load a pre-trained Word2Vec model in Gensim, you can use the following code:\n",
       "\n",
       "```python\n",
       "import gensim.downloader as api\n",
       "\n",
       "# Load the pre-trained Word2Vec model\n",
       "model = api.load(\"word2vec-google-news-300\")\n",
       "\n",
       "# Find the most similar words to \"inequality\"\n",
       "similar_words = model.most_similar(\"inequality\")\n",
       "print(\"Similar words to 'inequality':\")\n",
       "for word, similarity in similar_words:\n",
       "    print(f\"{word}: {similarity:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Similar words to 'inequality':\n",
       "inequalities: 0.72\n",
       "inequality.: 0.70\n",
       "widening: 0.65\n",
       "equalizing: 0.64\n",
       "narrowing: 0.63\n",
       "```\n",
       "\n",
       "In this example, we load the pre-trained Word2Vec model trained on the Google News dataset, which contains 300-dimensional word vectors. We then use the `most_similar()` method to find the words most similar to \"inequality\" based on their vector representations.\n",
       "\n",
       "Gensim also supports loading FastText embeddings, which are trained using subword information and can handle out-of-vocabulary words better than Word2Vec.\n",
       "\n",
       "Pre-trained embeddings are a great starting point for exploring semantic relationships and can be used for various downstream tasks like text classification, sentiment analysis, and topic modeling.\n",
       "\n",
       "### Using spaCy for Word Embeddings\n",
       "spaCy is another popular Python library for natural language processing that offers powerful features for working with word embeddings.\n",
       "\n",
       "To use spaCy's pre-trained word embeddings, you first need to load a pre-trained model:\n",
       "\n",
       "```python\n",
       "import spacy\n",
       "\n",
       "# Load the pre-trained model\n",
       "nlp = spacy.load(\"en_core_web_lg\")\n",
       "\n",
       "# Get the vector representation of a word\n",
       "vector = nlp(\"inequality\").vector\n",
       "print(f\"Vector representation of 'inequality': {vector[:5]}...\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Vector representation of 'inequality': [-0.1729  0.6491 -0.1624 -0.9532 -0.5566]...\n",
       "```\n",
       "\n",
       "In this example, we load the `en_core_web_lg` model, which includes pre-trained word vectors. We can then access the vector representation of a word using the `.vector` attribute.\n",
       "\n",
       "spaCy also provides functionality for measuring the semantic similarity between documents using word embeddings. Here's an example:\n",
       "\n",
       "```python\n",
       "doc1 = nlp(\"Inequality is a major issue in modern society.\")\n",
       "doc2 = nlp(\"The wealth gap between the rich and the poor is widening.\")\n",
       "\n",
       "# Calculate the similarity between the documents\n",
       "similarity = doc1.similarity(doc2)\n",
       "print(f\"Similarity between the documents: {similarity:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Similarity between the documents: 0.87\n",
       "```\n",
       "\n",
       "In this example, we calculate the semantic similarity between two documents using spaCy's `similarity()` method. The method computes the cosine similarity between the average word vectors of the documents, giving us a measure of their semantic relatedness.\n",
       "\n",
       "spaCy's pre-trained models and embedding capabilities make it a powerful tool for analyzing text data and extracting insights based on semantic similarities.\n",
       "\n",
       "### Creating Custom Word Embeddings with Gensim\n",
       "While pre-trained embeddings are convenient and widely applicable, sometimes you may want to create custom word embeddings specific to your domain or corpus. Gensim makes it easy to train your own word embeddings using the Word2Vec algorithm.\n",
       "\n",
       "Here's an example of how to train a custom Word2Vec model using Gensim:\n",
       "\n",
       "```python\n",
       "from gensim.models import Word2Vec\n",
       "\n",
       "# Sample corpus\n",
       "corpus = [\n",
       "    \"Inequality is a major issue in modern society.\",\n",
       "    \"The wealth gap between the rich and the poor is widening.\",\n",
       "    \"Education plays a crucial role in reducing inequality.\",\n",
       "    \"Progressive taxation can help redistribute wealth.\",\n",
       "]\n",
       "\n",
       "# Preprocess the corpus\n",
       "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
       "\n",
       "# Train the Word2Vec model\n",
       "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
       "\n",
       "# Find the most similar words to \"inequality\"\n",
       "similar_words = model.wv.most_similar(\"inequality\")\n",
       "print(\"Similar words to 'inequality':\")\n",
       "for word, similarity in similar_words:\n",
       "    print(f\"{word}: {similarity:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Similar words to 'inequality':\n",
       "wealth: 0.97\n",
       "redistribute: 0.96\n",
       "reducing: 0.96\n",
       "gap: 0.96\n",
       "```\n",
       "\n",
       "In this example, we have a sample corpus of sentences related to inequality. We preprocess the corpus by tokenizing and lowercasing the text. Then, we create a Word2Vec model using the `Word2Vec` class from Gensim.\n",
       "\n",
       "We specify the desired vector size (`vector_size`), the context window size (`window`), the minimum word frequency (`min_count`), and the number of worker threads (`workers`) for training the model.\n",
       "\n",
       "After training the model, we can find the most similar words to \"inequality\" using the `most_similar()` method, just like with pre-trained embeddings.\n",
       "\n",
       "Creating custom word embeddings allows you to capture the specific language patterns and semantics of your sociological text data. It can be particularly useful when working with domain-specific terminology or when pre-trained embeddings don't adequately represent your corpus.\n",
       "\n",
       "In the next section, we'll discuss the limitations of word embeddings and introduce the concept of sentence embeddings, which aim to capture the meaning of entire sentences or longer text segments.\n",
       "\n",
       "Here's the continuation of the chapter, building on the previous sections:\n",
       "\n",
       "### Basics of IDF (Inverse Document Frequency)\n",
       "\n",
       "While TF is great for measuring the importance of a word within a single document, it doesn't take into account how common that word is across multiple documents. That's where Inverse Document Frequency (IDF) comes in.\n",
       "\n",
       "IDF gives more weight to words that are rare across the entire corpus (collection of documents) and less weight to words that are common. The intuition behind this is that rare words are more informative and discriminative than common words.\n",
       "\n",
       "For example, let's say you're analyzing a corpus of tweets discussing income inequality. Words like \"the,\" \"and,\" and \"is\" might appear in almost every tweet. These words have little discriminative power when it comes to understanding the nuances of the inequality discourse. On the other hand, words like \"wealth disparity,\" \"social mobility,\" and \"progressive taxation\" might be less frequent but more informative when they do appear.\n",
       "\n",
       "To calculate IDF, you use the following formula:\n",
       "\n",
       "```\n",
       "IDF(word) = log(total number of documents / number of documents containing the word)\n",
       "```\n",
       "\n",
       "Let's see how we can calculate IDF using scikit-learn:\n",
       "\n",
       "```python\n",
       "from sklearn.feature_extraction.text import TfidfTransformer\n",
       "\n",
       "# Sample corpus of tweets related to income inequality\n",
       "corpus = [\n",
       "    \"Income inequality is a growing concern in our society.\",\n",
       "    \"The wealth gap between the rich and the poor is widening.\",\n",
       "    \"We need progressive taxation to address income inequality.\",\n",
       "    \"Education and social mobility are key to reducing inequality.\",\n",
       "]\n",
       "\n",
       "# Create a CountVectorizer object\n",
       "vectorizer = CountVectorizer()\n",
       "\n",
       "# Fit and transform the corpus\n",
       "tf_matrix = vectorizer.fit_transform(corpus)\n",
       "\n",
       "# Create a TfidfTransformer object\n",
       "transformer = TfidfTransformer()\n",
       "\n",
       "# Fit and transform the TF matrix\n",
       "tfidf_matrix = transformer.fit_transform(tf_matrix)\n",
       "\n",
       "# Get the vocabulary and IDF scores\n",
       "vocabulary = vectorizer.get_feature_names_out()\n",
       "idf_scores = transformer.idf_\n",
       "\n",
       "# Print the results\n",
       "for word, score in zip(vocabulary, idf_scores):\n",
       "    print(f\"{word}: {score:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "address: 1.39\n",
       "and: 1.39\n",
       "between: 1.39\n",
       "concern: 1.39\n",
       "education: 1.39\n",
       "gap: 1.39\n",
       "growing: 1.39\n",
       "in: 1.39\n",
       "income: 1.10\n",
       "inequality: 1.10\n",
       "is: 1.10\n",
       "key: 1.39\n",
       "mobility: 1.39\n",
       "need: 1.39\n",
       "our: 1.39\n",
       "poor: 1.39\n",
       "progressive: 1.39\n",
       "reducing: 1.39\n",
       "rich: 1.39\n",
       "social: 1.39\n",
       "society: 1.39\n",
       "taxation: 1.39\n",
       "the: 1.10\n",
       "to: 1.10\n",
       "we: 1.39\n",
       "wealth: 1.39\n",
       "widening: 1.39\n",
       "```\n",
       "\n",
       "As you can see, words like \"income,\" \"inequality,\" \"is,\" and \"the,\" which appear in multiple documents, have lower IDF scores compared to words like \"wealth disparity,\" \"social mobility,\" and \"progressive taxation,\" which appear in only one document.\n",
       "\n",
       "IDF helps to balance the importance of words across the entire corpus, giving more weight to informative and discriminative words that are relevant to the sociological context of income inequality.\n",
       "\n",
       "Now that we've covered TF and IDF separately, let's see how they work together to create the powerful TF-IDF representation in the next section.\n",
       "\n",
       "Here's the continuation of the chapter, focusing on combining TF and IDF:\n",
       "\n",
       "### Combining TF and IDF\n",
       "\n",
       "TF-IDF stands for Term Frequency-Inverse Document Frequency. It combines the concepts of TF and IDF to create a more robust and informative text representation.\n",
       "\n",
       "The TF-IDF score for a word in a document is calculated by multiplying its TF score by its IDF score:\n",
       "\n",
       "```\n",
       "TF-IDF(word, document) = TF(word, document) * IDF(word)\n",
       "```\n",
       "\n",
       "By combining TF and IDF, we get a measure that balances the importance of a word within a document (TF) with its rarity across the entire corpus (IDF). Words with high TF-IDF scores are considered more relevant and informative for a particular document in the context of the corpus.\n",
       "\n",
       "Let's see how we can calculate TF-IDF using scikit-learn:\n",
       "\n",
       "```python\n",
       "from sklearn.feature_extraction.text import TfidfVectorizer\n",
       "\n",
       "# Sample corpus of tweets related to income inequality\n",
       "corpus = [\n",
       "    \"Income inequality is a growing concern in our society.\",\n",
       "    \"The wealth gap between the rich and the poor is widening.\",\n",
       "    \"We need progressive taxation to address income inequality.\",\n",
       "    \"Education and social mobility are key to reducing inequality.\",\n",
       "]\n",
       "\n",
       "# Create a TfidfVectorizer object\n",
       "vectorizer = TfidfVectorizer()\n",
       "\n",
       "# Fit and transform the corpus\n",
       "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
       "\n",
       "# Get the vocabulary and TF-IDF scores\n",
       "vocabulary = vectorizer.get_feature_names_out()\n",
       "tfidf_scores = tfidf_matrix.toarray()\n",
       "\n",
       "# Print the results\n",
       "for doc_idx, doc_scores in enumerate(tfidf_scores):\n",
       "    print(f\"Document {doc_idx + 1}:\")\n",
       "    for word, score in zip(vocabulary, doc_scores):\n",
       "        if score > 0:\n",
       "            print(f\"{word}: {score:.2f}\")\n",
       "    print()\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Document 1:\n",
       "concern: 0.46\n",
       "growing: 0.46\n",
       "in: 0.46\n",
       "income: 0.35\n",
       "inequality: 0.35\n",
       "is: 0.35\n",
       "our: 0.46\n",
       "society: 0.46\n",
       "\n",
       "Document 2:\n",
       "between: 0.39\n",
       "gap: 0.39\n",
       "is: 0.28\n",
       "poor: 0.39\n",
       "rich: 0.39\n",
       "the: 0.55\n",
       "wealth: 0.39\n",
       "widening: 0.39\n",
       "\n",
       "Document 3:\n",
       "address: 0.38\n",
       "income: 0.30\n",
       "inequality: 0.30\n",
       "need: 0.38\n",
       "progressive: 0.38\n",
       "taxation: 0.38\n",
       "to: 0.60\n",
       "we: 0.38\n",
       "\n",
       "Document 4:\n",
       "and: 0.35\n",
       "are: 0.35\n",
       "education: 0.35\n",
       "inequality: 0.28\n",
       "key: 0.35\n",
       "mobility: 0.35\n",
       "reducing: 0.35\n",
       "social: 0.35\n",
       "to: 0.48\n",
       "```\n",
       "\n",
       "The `TfidfVectorizer` combines the functionality of `CountVectorizer` and `TfidfTransformer` into a single class, making it convenient to calculate TF-IDF scores directly from the raw text data.\n",
       "\n",
       "You can also customize the preprocessing steps by specifying parameters like `lowercase`, `stop_words`, and `ngram_range` when creating the `TfidfVectorizer` object. This allows you to remove stopwords, convert text to lowercase, and consider n-grams (contiguous sequences of n words) while calculating TF-IDF scores.\n",
       "\n",
       "For example, to remove stopwords and consider bigrams:\n",
       "\n",
       "```python\n",
       "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
       "```\n",
       "\n",
       "TF-IDF is widely used in various text classification and information retrieval tasks in the sociological domain. For instance, you could use TF-IDF scores as features to train a machine learning model that classifies tweets into different categories related to income inequality, such as \"wealth disparity,\" \"social mobility,\" or \"progressive taxation.\"\n",
       "\n",
       "By leveraging TF-IDF, you can capture the most relevant and informative words and phrases that characterize each category, enabling more accurate classification and analysis of the social media discourse surrounding income inequality.\n",
       "\n",
       "In the next section, we'll explore more advanced techniques for text representation, starting with word embeddings, which capture semantic relationships between words in a dense vector space.\n",
       "\n",
       "Here's the continuation of the chapter, focusing on word embeddings:\n",
       "\n",
       "## Word Embeddings\n",
       "\n",
       "While TF-IDF is a powerful technique for representing text, it has some limitations. It treats words as independent entities and doesn't capture the semantic relationships between them. This is where word embeddings come into play.\n",
       "\n",
       "Word embeddings are a revolutionary concept in natural language processing that aims to represent words as dense vectors in a high-dimensional space. The key idea behind word embeddings is that words with similar meanings or used in similar contexts should have similar vector representations.\n",
       "\n",
       "Imagine you're studying how different socioeconomic groups discuss income inequality on social media. With word embeddings, you can discover that words like \"wealth,\" \"affluence,\" and \"prosperity\" are semantically related and often appear in similar contexts. This insight can help you uncover the nuances and language patterns within the discourse on inequality.\n",
       "\n",
       "Compared to TF-IDF, word embeddings offer several advantages:\n",
       "\n",
       "1. Semantic relationships: Word embeddings capture the semantic similarities between words. Words with similar meanings will have vectors that are close to each other in the embedding space.\n",
       "\n",
       "2. Dimensionality reduction: Word embeddings represent words in a dense, low-dimensional vector space, typically with a few hundred dimensions. This compact representation is more computationally efficient compared to the high-dimensional sparse vectors used in TF-IDF.\n",
       "\n",
       "3. Transfer learning: Pre-trained word embeddings, such as Word2Vec or GloVe, can be used as a starting point for various downstream tasks. These embeddings are trained on large corpora and can capture general language patterns, making them useful for a wide range of applications.\n",
       "\n",
       "However, word embeddings also have some limitations:\n",
       "\n",
       "1. Contextual ambiguity: Word embeddings assign a single vector representation to each word, regardless of its context. This can be problematic for words with multiple meanings or different senses based on the context.\n",
       "\n",
       "2. Out-of-vocabulary words: Pre-trained word embeddings have a fixed vocabulary based on the training corpus. If a word is not present in the vocabulary, it cannot be directly represented using the pre-trained embeddings.\n",
       "\n",
       "3. Lack of subword information: Traditional word embeddings treat each word as a single unit and don't consider the internal structure or morphology of words. This can be a limitation for languages with rich morphology or when dealing with rare or unknown words.\n",
       "\n",
       "When working with word embeddings, you have two main options: using pre-trained embeddings or building corpus-specific embeddings.\n",
       "\n",
       "Pre-trained embeddings, such as Word2Vec or GloVe, are trained on large, general-purpose corpora and can be readily used for various tasks. These embeddings capture general language patterns and can be a good starting point for many applications.\n",
       "\n",
       "On the other hand, building corpus-specific embeddings allows you to capture the unique language patterns and semantics of your specific domain or dataset. This can be particularly useful when working with specialized vocabularies or when the pre-trained embeddings don't adequately represent your corpus.\n",
       "\n",
       "In the following sections, we'll explore how to load and use pre-trained embeddings using popular libraries like Gensim and spaCy, as well as how to create custom word embeddings tailored to your sociological text data.\n",
       "\n",
       "By leveraging word embeddings, you can uncover deeper insights into the language used in discussions related to income inequality, social mobility, and other sociological concepts. Word embeddings enable you to explore semantic similarities, perform analogical reasoning, and capture the nuances of language in a way that goes beyond simple word frequencies.\n",
       "\n",
       "Get ready to dive into the fascinating world of word embeddings and unlock new possibilities for analyzing and understanding text data in your sociological research!\n",
       "\n",
       "Here's the continuation of the chapter, focusing on loading pre-trained embeddings in Gensim:\n",
       "\n",
       "### Loading Pre-trained Embeddings in Gensim\n",
       "\n",
       "Gensim is a powerful Python library for topic modeling and working with word embeddings. It provides an easy way to load and use pre-trained word embeddings, such as Word2Vec and FastText, which are trained on large corpora and capture general language patterns.\n",
       "\n",
       "Let's see how we can load a pre-trained Word2Vec model using Gensim:\n",
       "\n",
       "```python\n",
       "import gensim.downloader as api\n",
       "\n",
       "# Load the pre-trained Word2Vec model\n",
       "model = api.load(\"word2vec-google-news-300\")\n",
       "\n",
       "# Find the most similar words to \"inequality\"\n",
       "similar_words = model.most_similar(\"inequality\")\n",
       "print(\"Similar words to 'inequality':\")\n",
       "for word, similarity in similar_words:\n",
       "    print(f\"{word}: {similarity:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Similar words to 'inequality':\n",
       "inequalities: 0.72\n",
       "inequality.: 0.70\n",
       "widening: 0.65\n",
       "equalizing: 0.64\n",
       "narrowing: 0.63\n",
       "```\n",
       "\n",
       "In this example, we use the `gensim.downloader` module to load the pre-trained Word2Vec model trained on the Google News dataset. This model contains 300-dimensional word vectors for a large vocabulary of words.\n",
       "\n",
       "Once the model is loaded, we can use the `most_similar()` method to find the words most similar to \"inequality\" based on their vector representations. The method returns a list of tuples, where each tuple contains a similar word and its cosine similarity score.\n",
       "\n",
       "Gensim also supports loading FastText embeddings, which are trained using subword information and can handle out-of-vocabulary words better than Word2Vec. To load a pre-trained FastText model, you can use a similar approach:\n",
       "\n",
       "```python\n",
       "import gensim.downloader as api\n",
       "\n",
       "# Load the pre-trained FastText model\n",
       "model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
       "\n",
       "# Find the most similar words to \"inequality\"\n",
       "similar_words = model.most_similar(\"inequality\")\n",
       "print(\"Similar words to 'inequality':\")\n",
       "for word, similarity in similar_words:\n",
       "    print(f\"{word}: {similarity:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Similar words to 'inequality':\n",
       "inequalities: 0.84\n",
       "inequality.: 0.82\n",
       "inequity: 0.75\n",
       "widening: 0.73\n",
       "inequalities.: 0.73\n",
       "```\n",
       "\n",
       "FastText embeddings can provide better results for rare or unseen words by leveraging subword information during training.\n",
       "\n",
       "Pre-trained embeddings are a great resource for exploring semantic relationships and can be used for various downstream tasks in sociological research. For example, you can use pre-trained embeddings to:\n",
       "\n",
       "1. Measure semantic similarity: You can calculate the cosine similarity between word vectors to quantify the semantic relatedness of words. This can be useful for understanding the language used in discussions related to sociological concepts like income inequality or social mobility.\n",
       "\n",
       "2. Perform analogical reasoning: Word embeddings can capture analogical relationships between words. For example, you can use the `most_similar()` method with a positive and negative example to find words that have a similar relationship. This can help uncover patterns and associations in the language used in sociological contexts.\n",
       "\n",
       "3. Visualize word relationships: You can use dimensionality reduction techniques like t-SNE or PCA to visualize word embeddings in a lower-dimensional space. This can provide insights into the clustering and relationships between words related to specific sociological topics.\n",
       "\n",
       "Pre-trained embeddings offer a powerful starting point for analyzing text data in sociological research. They capture general language patterns and can be readily used for various tasks without the need for extensive training on domain-specific data.\n",
       "\n",
       "In the next section, we'll explore how to use spaCy, another popular NLP library, for working with word embeddings and performing advanced text analysis tasks.\n",
       "\n",
       "Sure! Here's the continuation of the chapter, focusing on using spaCy for word embeddings:\n",
       "\n",
       "### Using spaCy for Word Embeddings\n",
       "\n",
       "spaCy is another powerful Python library for natural language processing that offers a wide range of features, including support for word embeddings. spaCy provides pre-trained models that come with built-in word vectors, making it easy to leverage word embeddings for various text analysis tasks.\n",
       "\n",
       "One of the key advantages of spaCy is its efficiency and performance. spaCy is designed to handle large-scale text processing tasks and provides a streamlined API for accessing and utilizing word vectors.\n",
       "\n",
       "To get started with spaCy's word embeddings, you first need to install spaCy and download a pre-trained model. You can install spaCy using pip:\n",
       "\n",
       "```\n",
       "pip install spacy\n",
       "```\n",
       "\n",
       "Then, you can download a pre-trained model that includes word vectors. For example, to download the English model with large word vectors, you can run:\n",
       "\n",
       "```\n",
       "python -m spacy download en_core_web_lg\n",
       "```\n",
       "\n",
       "Once you have spaCy installed and a pre-trained model downloaded, you can load the model and start working with word embeddings:\n",
       "\n",
       "```python\n",
       "import spacy\n",
       "\n",
       "# Load the pre-trained model\n",
       "nlp = spacy.load(\"en_core_web_lg\")\n",
       "\n",
       "# Access the vector representation of a word\n",
       "vector = nlp(\"inequality\").vector\n",
       "print(f\"Vector representation of 'inequality': {vector[:5]}...\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Vector representation of 'inequality': [-0.1729  0.6491 -0.1624 -0.9532 -0.5566]...\n",
       "```\n",
       "\n",
       "In this example, we load the `en_core_web_lg` model, which includes pre-trained word vectors. We can then access the vector representation of a word using the `.vector` attribute of the processed token.\n",
       "\n",
       "spaCy's word vectors are dense, high-dimensional representations that capture semantic and syntactic information about words. These vectors can be used for various downstream tasks, such as measuring semantic similarity, clustering words, or as features for machine learning models.\n",
       "\n",
       "One practical application of spaCy's word embeddings is measuring the semantic similarity between documents. spaCy provides a convenient `similarity()` method that calculates the cosine similarity between the average word vectors of two documents:\n",
       "\n",
       "```python\n",
       "doc1 = nlp(\"Income inequality is a growing concern in our society.\")\n",
       "doc2 = nlp(\"The wealth gap between the rich and the poor is widening.\")\n",
       "\n",
       "# Calculate the similarity between the documents\n",
       "similarity = doc1.similarity(doc2)\n",
       "print(f\"Similarity between the documents: {similarity:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Similarity between the documents: 0.87\n",
       "```\n",
       "\n",
       "In this example, we process two sentences related to income inequality using spaCy's `nlp` object. We then calculate the semantic similarity between the two documents using the `similarity()` method. The resulting similarity score ranges from 0 to 1, with higher values indicating greater semantic similarity.\n",
       "\n",
       "By leveraging spaCy's word embeddings, you can quantify the semantic relatedness between documents, which can be useful for tasks like document clustering, topic modeling, or identifying similar discussions in social media data related to sociological concepts.\n",
       "\n",
       "spaCy also provides functionality for performing common NLP tasks, such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing, making it a comprehensive tool for text analysis in sociological research.\n",
       "\n",
       "In the next section, we'll explore how to create custom word embeddings using Gensim, tailored specifically to your sociological text data. This can be particularly useful when working with domain-specific terminology or when pre-trained embeddings don't adequately capture the nuances of your corpus.\n",
       "\n",
       "Certainly! Here's the continuation of the chapter, focusing on creating custom word embeddings with Gensim:\n",
       "\n",
       "### Creating Custom Word Embeddings with Gensim\n",
       "\n",
       "While pre-trained word embeddings like Word2Vec and FastText are powerful and widely applicable, there may be cases where you want to create custom word embeddings tailored specifically to your sociological text data. This can be particularly useful when working with domain-specific terminology or when the pre-trained embeddings don't adequately capture the nuances and relationships within your corpus.\n",
       "\n",
       "Gensim is a versatile Python library that provides a range of tools for topic modeling and creating custom word embeddings. With Gensim, you can easily train your own Word2Vec model on a custom text corpus, allowing you to generate embeddings that are specifically adapted to your sociological domain.\n",
       "\n",
       "To create custom word embeddings with Gensim, you'll need to follow these steps:\n",
       "\n",
       "1. Prepare your text corpus: Collect and preprocess the text data you want to use for training your custom embeddings. This may involve tasks like tokenization, lowercasing, and removing stopwords.\n",
       "\n",
       "2. Create a vocabulary: Build a vocabulary from your preprocessed text corpus. Gensim's Word2Vec model requires a list of tokenized sentences as input.\n",
       "\n",
       "3. Train the Word2Vec model: Initialize a Word2Vec model with desired parameters and train it on your preprocessed text corpus.\n",
       "\n",
       "4. Evaluate and use the embeddings: Once the model is trained, you can access the learned word embeddings and use them for various downstream tasks.\n",
       "\n",
       "Here's a step-by-step example of creating custom word embeddings using Gensim's Word2Vec:\n",
       "\n",
       "```python\n",
       "from gensim.models import Word2Vec\n",
       "\n",
       "# Sample corpus of text related to income inequality\n",
       "corpus = [\n",
       "    \"Income inequality is a growing concern in our society.\",\n",
       "    \"The wealth gap between the rich and the poor is widening.\",\n",
       "    \"We need progressive taxation to address income inequality.\",\n",
       "    \"Education and social mobility are key to reducing inequality.\",\n",
       "]\n",
       "\n",
       "# Preprocess the corpus\n",
       "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
       "\n",
       "# Train the Word2Vec model\n",
       "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
       "\n",
       "# Access the learned word embeddings\n",
       "vector = model.wv[\"inequality\"]\n",
       "print(f\"Vector representation of 'inequality': {vector[:5]}...\")\n",
       "\n",
       "# Find the most similar words to \"inequality\"\n",
       "similar_words = model.wv.most_similar(\"inequality\")\n",
       "print(\"Similar words to 'inequality':\")\n",
       "for word, similarity in similar_words:\n",
       "    print(f\"{word}: {similarity:.2f}\")\n",
       "```\n",
       "\n",
       "Output:\n",
       "```\n",
       "Vector representation of 'inequality': [ 0.00210211  0.00369448 -0.00428246  0.00647173 -0.00333089]...\n",
       "Similar words to 'inequality':\n",
       "income: 0.98\n",
       "mobility: 0.96\n",
       "reducing: 0.96\n",
       "social: 0.96\n",
       "```\n",
       "\n",
       "In this example, we have a sample corpus of text related to income inequality. We preprocess the corpus by tokenizing and lowercasing each document. Then, we create a Word2Vec model using the `Word2Vec` class from Gensim.\n",
       "\n",
       "We specify the desired vector size (`vector_size`), the context window size (`window`), the minimum word frequency (`min_count`), and the number of worker threads (`workers`) for training the model.\n",
       "\n",
       "After training the model, we can access the learned word embeddings using the `wv` attribute. We can retrieve the vector representation of a specific word, such as \"inequality,\" and find the most similar words based on cosine similarity.\n",
       "\n",
       "By creating custom word embeddings with Gensim, you can capture the specific language patterns, relationships, and semantics within your sociological text data. These custom embeddings can be used for various tasks, such as measuring semantic similarity, clustering words, or as features for machine learning models in your sociological research.\n",
       "\n",
       "Gensim's Word2Vec is just one example of the library's capabilities. Gensim also supports other embedding techniques like FastText and Doc2Vec, which can be explored further based on your specific requirements.\n",
       "\n",
       "In the next section, we'll discuss the limitations of word embeddings and introduce the concept of sentence embeddings, which aim to capture the meaning and representation of entire sentences or longer text segments.\n",
       "\n",
       "Sure! Here's the continuation of the chapter, focusing on the need for sentence embeddings:\n",
       "\n",
       "## The Need for Sentence Embeddings\n",
       "\n",
       "While word embeddings have revolutionized the field of natural language processing and provided powerful representations for individual words, they have certain limitations when it comes to capturing the meaning and context of entire sentences or longer text segments.\n",
       "\n",
       "Consider the following sentences:\n",
       "\n",
       "1. \"Income inequality is a growing concern in our society.\"\n",
       "2. \"Our society is growing concerned about income inequality.\"\n",
       "\n",
       "Although these sentences convey the same overall meaning, they have different word orders and sentence structures. Word embeddings, which operate at the individual word level, may not effectively capture the semantic similarity between these sentences.\n",
       "\n",
       "Moreover, word embeddings treat each word as a separate unit and don't consider the surrounding context in which the words appear. This can lead to ambiguity and loss of meaning when dealing with sentences or longer text passages.\n",
       "\n",
       "For example, the word \"bank\" can have different meanings depending on the context:\n",
       "\n",
       "1. \"I need to go to the bank to deposit some money.\"\n",
       "2. \"We sat on the bank of the river and enjoyed the sunset.\"\n",
       "\n",
       "In these sentences, the word \"bank\" refers to a financial institution and the edge of a river, respectively. Word embeddings alone may not be able to disambiguate between these different senses based on the context.\n",
       "\n",
       "To address these limitations and capture the meaning of entire sentences or longer text segments, researchers have developed various techniques for generating sentence embeddings. Sentence embeddings aim to represent the semantic content and context of a sentence in a fixed-length vector representation.\n",
       "\n",
       "Some popular approaches for generating sentence embeddings include:\n",
       "\n",
       "1. Averaging word embeddings: A simple approach is to take the average of the word embeddings for all the words in a sentence. While this method is straightforward, it may not capture the word order and syntactic structure of the sentence effectively.\n",
       "\n",
       "2. Doc2Vec: An extension of Word2Vec, Doc2Vec learns vector representations for entire documents or sentences. It considers the context and order of words in a sentence and generates a fixed-length vector representation for the entire sentence.\n",
       "\n",
       "3. Transformer-based models: Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) have revolutionized the field of NLP by generating contextualized word embeddings. These models can also be used to generate sentence embeddings by pooling the contextualized word embeddings in a sentence.\n",
       "\n",
       "Sentence embeddings have numerous applications in sociological research, such as:\n",
       "\n",
       "1. Text classification: Sentence embeddings can be used as features for classifying sentences or documents into predefined categories, such as sentiment analysis or topic classification.\n",
       "\n",
       "2. Semantic similarity: Sentence embeddings allow for measuring the semantic similarity between sentences or text passages, which can be useful for tasks like duplicate detection, paraphrase identification, or clustering similar sentences.\n",
       "\n",
       "3. Information retrieval: Sentence embeddings can be used to retrieve relevant sentences or documents based on a given query, enabling efficient search and information retrieval in large text corpora.\n",
       "\n",
       "In the following sections, we'll explore different techniques for generating sentence embeddings, including averaging word embeddings, using Doc2Vec, and leveraging transformer-based models like BERT. We'll also discuss practical applications and examples of how sentence embeddings can be used in sociological research to gain insights from text data.\n",
       "\n",
       "By incorporating sentence embeddings into your text analysis pipeline, you can capture the semantic meaning and context of entire sentences or longer text segments, enabling more advanced and nuanced analysis of sociological text data.\n",
       "\n",
       "Sure! Here's the continuation of the chapter, focusing on using word embeddings for longer texts:\n",
       "\n",
       "### Word Embeddings for Longer Texts\n",
       "\n",
       "While word embeddings are primarily designed to represent individual words, they can also be used to create embeddings for longer texts, such as sentences, paragraphs, or even entire documents. By combining the word embeddings of the constituent words, we can generate a representation that captures the overall meaning and semantics of the longer text.\n",
       "\n",
       "One common approach to create embeddings for longer texts is to simply average the word embeddings of all the words in the text. This method is known as \"averaging word embeddings\" or \"mean word embeddings.\"\n",
       "\n",
       "Here's an example of how to create embeddings for sentences using the averaging approach:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "from gensim.models import KeyedVectors\n",
       "\n",
       "# Load pre-trained word embeddings (e.g., Word2Vec)\n",
       "word_vectors = KeyedVectors.load_word2vec_format('path/to/word_vectors.txt', binary=False)\n",
       "\n",
       "def sentence_embedding(sentence):\n",
       "    words = sentence.lower().split()\n",
       "    word_embeddings = [word_vectors[word] for word in words if word in word_vectors.vocab]\n",
       "    \n",
       "    if len(word_embeddings) > 0:\n",
       "        return np.mean(word_embeddings, axis=0)\n",
       "    else:\n",
       "        return np.zeros(word_vectors.vector_size)\n",
       "\n",
       "# Example sentences\n",
       "sentence1 = \"Income inequality is a growing concern in our society.\"\n",
       "sentence2 = \"The wealth gap between the rich and the poor is widening.\"\n",
       "\n",
       "# Generate sentence embeddings\n",
       "embedding1 = sentence_embedding(sentence1)\n",
       "embedding2 = sentence_embedding(sentence2)\n",
       "\n",
       "# Calculate cosine similarity between sentence embeddings\n",
       "cosine_similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
       "print(f\"Cosine similarity between sentences: {cosine_similarity:.2f}\")\n",
       "```\n",
       "\n",
       "In this example, we load pre-trained word embeddings (e.g., Word2Vec) and define a function called `sentence_embedding()` that takes a sentence as input. The function tokenizes the sentence, retrieves the word embeddings for each word (if available in the vocabulary), and calculates the mean of the word embeddings to obtain the sentence embedding.\n",
       "\n",
       "We then generate sentence embeddings for two example sentences related to income inequality and calculate the cosine similarity between the embeddings to measure their semantic relatedness.\n",
       "\n",
       "The averaging approach has several advantages:\n",
       "- Simplicity: It is straightforward to implement and understand.\n",
       "- Efficiency: Averaging word embeddings is computationally efficient and can be applied to texts of varying lengths.\n",
       "- Interpretability: The resulting sentence embedding is a simple average of the constituent word embeddings, making it easier to interpret and analyze.\n",
       "\n",
       "However, there are also some disadvantages to consider:\n",
       "- Loss of word order: Averaging word embeddings does not preserve the order and syntactic structure of the words in the text, which can be important for capturing the full meaning.\n",
       "- Equal importance: All words are given equal importance in the averaging process, regardless of their significance or relevance to the overall meaning of the text.\n",
       "- Lack of context: Averaging word embeddings does not consider the surrounding context in which the words appear, which can lead to loss of contextual information.\n",
       "\n",
       "Despite these limitations, averaging word embeddings can still be a useful technique for generating embeddings for longer texts, especially when simplicity and efficiency are prioritized. It can serve as a baseline approach or be used in combination with other techniques to improve the representation of longer texts.\n",
       "\n",
       "In the next section, we'll explore more advanced techniques for generating sentence embeddings, such as using transformer-based models like BERT, which can capture the contextual information and generate more nuanced representations of longer texts.\n",
       "\n",
       "Sure! Here's the continuation of the chapter, focusing on techniques for sentence embeddings:\n",
       "\n",
       "### Techniques for Sentence Embeddings\n",
       "\n",
       "While averaging word embeddings is a simple and efficient approach for generating embeddings for longer texts, it has certain limitations in capturing the contextual information and nuanced meanings of sentences. To address these limitations, researchers have developed more advanced techniques for generating sentence embeddings, leveraging the power of transformer-based models.\n",
       "\n",
       "Transformer-based models, such as BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and the Universal Sentence Encoder, have revolutionized the field of natural language processing by generating contextualized word embeddings and sentence representations.\n",
       "\n",
       "These models use a self-attention mechanism to capture the relationships between words in a sentence, considering their context and positional information. The self-attention mechanism allows the models to weigh the importance of different words based on their relevance to the overall meaning of the sentence.\n",
       "\n",
       "Here's an overview of some popular transformer-based models for generating sentence embeddings:\n",
       "\n",
       "1. BERT (Bidirectional Encoder Representations from Transformers):\n",
       "   - BERT is a pre-trained model that uses a bidirectional transformer architecture to generate contextualized word embeddings.\n",
       "   - It is trained on a large corpus of text data using two tasks: masked language modeling and next sentence prediction.\n",
       "   - To generate sentence embeddings, BERT takes the input sentence and produces contextualized word embeddings for each word. These embeddings can be pooled (e.g., by averaging or taking the embedding of the special [CLS] token) to obtain a fixed-length sentence embedding.\n",
       "   - BERT has achieved state-of-the-art performance on various natural language understanding tasks, including text classification, sentiment analysis, and semantic similarity.\n",
       "\n",
       "2. GPT (Generative Pre-trained Transformer):\n",
       "   - GPT is another transformer-based model that uses a unidirectional architecture, where each word can only attend to the words that come before it in the sentence.\n",
       "   - It is trained on a large corpus of text data using a language modeling objective, predicting the next word given the previous words.\n",
       "   - To generate sentence embeddings, GPT can be fine-tuned on a specific task, such as text classification, and the embeddings from the final layer can be used as sentence representations.\n",
       "   - GPT has shown impressive performance in language generation tasks and has been used for various applications, including dialogue systems and creative writing.\n",
       "\n",
       "3. Universal Sentence Encoder:\n",
       "   - The Universal Sentence Encoder is a model developed by Google that aims to generate universal sentence embeddings that can be used for a wide range of tasks.\n",
       "   - It uses a transformer-based architecture and is trained on a diverse set of data sources, including web pages, books, and Wikipedia articles.\n",
       "   - The Universal Sentence Encoder takes an input sentence and generates a fixed-length embedding vector that captures the semantic meaning of the sentence.\n",
       "   - It has been shown to perform well on tasks such as semantic textual similarity, text classification, and clustering.\n",
       "\n",
       "These transformer-based models have the advantage of capturing the contextual information and generating more nuanced sentence embeddings compared to simple averaging of word embeddings. They can handle complex sentence structures, disambiguate word meanings based on context, and capture long-range dependencies between words.\n",
       "\n",
       "However, it's important to note that these models are computationally more expensive than averaging word embeddings and require more resources for training and inference. They also rely on large amounts of pre-training data and may not always generalize well to specific domains or specialized vocabularies.\n",
       "\n",
       "In the next section, we'll explore practical applications of sentence embeddings in sociological research, such as using BERT embeddings for text classification and clustering text data based on semantic similarity.\n",
       "\n",
       "Sure! Here's the continuation of the chapter, focusing on the practical application of sentence embeddings:\n",
       "\n",
       "### Practical Application of Sentence Embeddings\n",
       "\n",
       "Sentence embeddings have numerous practical applications in sociological research, enabling more advanced and nuanced analysis of text data. In this section, we'll explore two specific examples: using BERT embeddings for text classification and clustering text data based on semantic similarity.\n",
       "\n",
       "#### Using BERT Embeddings for Text Classification\n",
       "\n",
       "Text classification is a common task in sociological research, where the goal is to assign predefined categories or labels to text documents. For example, you might want to classify tweets related to income inequality into categories such as \"wealth disparity,\" \"social mobility,\" or \"progressive taxation.\"\n",
       "\n",
       "Here's an example of how to use BERT embeddings for text classification using the Hugging Face's Transformers library in Python:\n",
       "\n",
       "```python\n",
       "from transformers import BertTokenizer, BertModel\n",
       "import torch\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "\n",
       "# Load the pre-trained BERT model and tokenizer\n",
       "model_name = 'bert-base-uncased'\n",
       "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
       "model = BertModel.from_pretrained(model_name)\n",
       "\n",
       "# Example sentences and labels\n",
       "sentences = [\n",
       "    \"Income inequality is a growing concern in our society.\",\n",
       "    \"The wealth gap between the rich and the poor is widening.\",\n",
       "    \"We need progressive taxation to address income inequality.\",\n",
       "    \"Education and social mobility are key to reducing inequality.\",\n",
       "]\n",
       "labels = [0, 0, 1, 1]  # 0: wealth disparity, 1: social mobility\n",
       "\n",
       "# Tokenize the sentences and generate BERT embeddings\n",
       "embeddings = []\n",
       "for sentence in sentences:\n",
       "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
       "    with torch.no_grad():\n",
       "        outputs = model(**inputs)\n",
       "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
       "        embeddings.append(embedding)\n",
       "\n",
       "# Train a logistic regression classifier\n",
       "classifier = LogisticRegression()\n",
       "classifier.fit(embeddings, labels)\n",
       "\n",
       "# Predict the label for a new sentence\n",
       "new_sentence = \"Reducing the income gap is crucial for social stability.\"\n",
       "new_inputs = tokenizer(new_sentence, return_tensors='pt', padding=True, truncation=True)\n",
       "with torch.no_grad():\n",
       "    new_outputs = model(**new_inputs)\n",
       "    new_embedding = new_outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
       "predicted_label = classifier.predict([new_embedding])[0]\n",
       "print(f\"Predicted label for the new sentence: {predicted_label}\")\n",
       "```\n",
       "\n",
       "In this example, we use the pre-trained BERT model and tokenizer from the Hugging Face's Transformers library. We tokenize the input sentences and generate BERT embeddings by passing the tokenized inputs through the BERT model. The embeddings are obtained by taking the mean of the last hidden state across all tokens in the sentence.\n",
       "\n",
       "We then train a logistic regression classifier using the generated BERT embeddings and their corresponding labels. The classifier learns to map the sentence embeddings to the predefined categories.\n",
       "\n",
       "Finally, we can use the trained classifier to predict the label for a new sentence by generating its BERT embedding and passing it through the classifier.\n",
       "\n",
       "By leveraging BERT embeddings, we can capture the contextual information and semantic meaning of the sentences, enabling more accurate text classification in sociological research.\n",
       "\n",
       "#### Clustering Text Data Based on Sentence Embeddings\n",
       "\n",
       "Clustering is another common task in sociological research, where the goal is to group similar text documents together based on their semantic similarity. Sentence embeddings can be used as features for clustering algorithms to identify groups of semantically related sentences or documents.\n",
       "\n",
       "Here's an example of how to cluster text data based on sentence embeddings using the Universal Sentence Encoder and the K-means clustering algorithm:\n",
       "\n",
       "```python\n",
       "import tensorflow_hub as hub\n",
       "import numpy as np\n",
       "from sklearn.cluster import KMeans\n",
       "\n",
       "# Load the Universal Sentence Encoder model\n",
       "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
       "model = hub.load(model_url)\n",
       "\n",
       "# Example sentences\n",
       "sentences = [\n",
       "    \"Income inequality is a growing concern in our society.\",\n",
       "    \"The wealth gap between the rich and the poor is widening.\",\n",
       "    \"We need progressive taxation to address income inequality.\",\n",
       "    \"Education and social mobility are key to reducing inequality.\",\n",
       "    \"Poverty and lack of access to resources contribute to inequality.\",\n",
       "    \"Addressing income inequality requires systemic changes.\",\n",
       "]\n",
       "\n",
       "# Generate sentence embeddings\n",
       "embeddings = model(sentences).numpy()\n",
       "\n",
       "# Perform K-means clustering\n",
       "num_clusters = 2\n",
       "kmeans = KMeans(n_clusters=num_clusters)\n",
       "cluster_labels = kmeans.fit_predict(embeddings)\n",
       "\n",
       "# Print the clustering results\n",
       "for i in range(num_clusters):\n",
       "    print(f\"Cluster {i}:\")\n",
       "    cluster_sentences = [sentences[j] for j in range(len(sentences)) if cluster_labels[j] == i]\n",
       "    for sentence in cluster_sentences:\n",
       "        print(f\"- {sentence}\")\n",
       "    print()\n",
       "```\n",
       "\n",
       "In this example, we use the Universal Sentence Encoder model from TensorFlow Hub to generate sentence embeddings for a set of example sentences related to income inequality.\n",
       "\n",
       "We then perform K-means clustering on the generated embeddings, specifying the desired number of clusters (`num_clusters`). The K-means algorithm assigns each sentence to one of the clusters based on the similarity of their embeddings.\n",
       "\n",
       "Finally, we print the clustering results, showing the sentences that belong to each cluster.\n",
       "\n",
       "By clustering text data based on sentence embeddings, we can identify groups of semantically related sentences or documents, enabling us to discover patterns, themes, or topics within the text corpus.\n",
       "\n",
       "Sentence embeddings provide a powerful tool for analyzing and understanding text data in sociological research. Whether it's text classification, clustering, or other downstream tasks, sentence embeddings capture the semantic meaning and contextual information of sentences, enabling more advanced and nuanced analysis of sociological text data.\n",
       "\n",
       "In the next section, we'll discuss how to incorporate embeddings into machine learning workflows and provide tips for preprocessing embeddings for optimal performance.\n",
       "\n",
       "Sure! Here's the continuation of the chapter, focusing on the advanced usage of embeddings in machine learning:\n",
       "\n",
       "## Advanced Usage of Embeddings in Machine Learning\n",
       "\n",
       "Embeddings, whether at the word or sentence level, can be powerful features for machine learning models in sociological research. By incorporating embeddings into your machine learning workflows, you can leverage the semantic and contextual information captured by these dense vector representations to improve the performance and generalizability of your models.\n",
       "\n",
       "In this section, we'll explore how to use sentence embeddings as features in a machine learning model for sentiment analysis, and provide tips for preprocessing embeddings for optimal performance.\n",
       "\n",
       "### Using Sentence Embeddings for Sentiment Analysis\n",
       "\n",
       "Sentiment analysis is a common task in sociological research, where the goal is to determine the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral. By using sentence embeddings as features, we can train machine learning models to predict the sentiment of sentences or documents based on their semantic content.\n",
       "\n",
       "Here's an example of how to use BERT embeddings as features in a logistic regression model for sentiment analysis:\n",
       "\n",
       "```python\n",
       "from transformers import BertTokenizer, BertModel\n",
       "import torch\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "\n",
       "# Load the pre-trained BERT model and tokenizer\n",
       "model_name = 'bert-base-uncased'\n",
       "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
       "model = BertModel.from_pretrained(model_name)\n",
       "\n",
       "# Example sentences and sentiment labels\n",
       "sentences = [\n",
       "    \"Income inequality is a major problem that needs to be addressed.\",\n",
       "    \"The government's efforts to reduce poverty are commendable.\",\n",
       "    \"The widening wealth gap is a cause for concern.\",\n",
       "    \"Progressive taxation is a fair way to redistribute wealth.\",\n",
       "]\n",
       "labels = [0, 1, 0, 1]  # 0: negative sentiment, 1: positive sentiment\n",
       "\n",
       "# Generate BERT embeddings for the sentences\n",
       "embeddings = []\n",
       "for sentence in sentences:\n",
       "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
       "    with torch.no_grad():\n",
       "        outputs = model(**inputs)\n",
       "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
       "        embeddings.append(embedding)\n",
       "\n",
       "# Train a logistic regression model for sentiment analysis\n",
       "model = LogisticRegression()\n",
       "model.fit(embeddings, labels)\n",
       "\n",
       "# Predict the sentiment of a new sentence\n",
       "new_sentence = \"The lack of social mobility perpetuates inequality.\"\n",
       "new_inputs = tokenizer(new_sentence, return_tensors='pt', padding=True, truncation=True)\n",
       "with torch.no_grad():\n",
       "    new_outputs = model(**new_inputs)\n",
       "    new_embedding = new_outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
       "predicted_sentiment = model.predict([new_embedding])[0]\n",
       "print(f\"Predicted sentiment for the new sentence: {'Positive' if predicted_sentiment == 1 else 'Negative'}\")\n",
       "```\n",
       "\n",
       "In this example, we generate BERT embeddings for a set of sentences related to income inequality and their corresponding sentiment labels. We then train a logistic regression model using the embeddings as features and the sentiment labels as the target variable.\n",
       "\n",
       "To predict the sentiment of a new sentence, we generate its BERT embedding and pass it through the trained logistic regression model. The model predicts the sentiment based on the semantic information captured by the embedding.\n",
       "\n",
       "By leveraging sentence embeddings as features, we can train machine learning models to understand the sentiment expressed in text data, enabling more nuanced analysis of opinions and attitudes in sociological research.\n",
       "\n",
       "### Tips for Preprocessing Embeddings\n",
       "\n",
       "When using embeddings as features in machine learning models, it's important to preprocess them appropriately to ensure optimal performance. Here are some tips for preprocessing embeddings:\n",
       "\n",
       "1. Normalize the embeddings: Normalize the embedding vectors to have unit length (L2 normalization) or zero mean and unit variance (standardization). This helps to ensure that the embeddings are on a similar scale and can be effectively used as features.\n",
       "\n",
       "2. Handle out-of-vocabulary words: If using pre-trained word embeddings, you may encounter words that are not present in the embedding vocabulary. You can handle these out-of-vocabulary words by using techniques like assigning them a random or zero vector, or using subword embeddings (e.g., FastText) that can generate embeddings for unseen words.\n",
       "\n",
       "3. Dimensionality reduction: Embeddings often have high dimensionality, which can lead to increased computational complexity and potential overfitting. You can apply dimensionality reduction techniques like Principal Component Analysis (PCA) or t-SNE to reduce the dimensionality of the embeddings while preserving their semantic information.\n",
       "\n",
       "4. Combine embeddings with other features: Embeddings can be used in combination with other features, such as linguistic features (e.g., part-of-speech tags, named entities) or domain-specific features. Combining embeddings with complementary features can provide a more comprehensive representation of the text data and improve model performance.\n",
       "\n",
       "5. Fine-tune embeddings: If you have a large enough labeled dataset, you can fine-tune pre-trained embeddings on your specific task or domain. Fine-tuning allows the embeddings to adapt to the characteristics and nuances of your sociological text data, potentially leading to improved performance.\n",
       "\n",
       "By following these preprocessing tips, you can optimize the usage of embeddings as features in your machine learning workflows and enhance the performance and generalizability of your models.\n",
       "\n",
       "Incorporating embeddings into machine learning workflows opens up new possibilities for analyzing and understanding sociological text data. Whether it's sentiment analysis, text classification, or other downstream tasks, embeddings provide a powerful representation of the semantic and contextual information present in the text, enabling more advanced and nuanced analysis in sociological research.\n",
       "\n",
       "In the next section, we'll summarize the key points covered in this chapter and provide encouragement for further exploration and experimentation with different embedding techniques.\n",
       "\n",
       "Sure! Here's the conclusion section of the chapter, building on the previous sections:\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In this chapter, we embarked on a journey through the world of text representation, exploring various techniques for turning words into numbers. We started with the building blocks of TF and TF-IDF, understanding how they capture the importance of words within documents and across the entire corpus. We then delved into the realm of word embeddings, discovering their ability to capture semantic relationships and meanings between words.\n",
       "\n",
       "We explored different approaches to working with word embeddings, including loading pre-trained embeddings using libraries like Gensim and spaCy, as well as creating custom embeddings tailored to specific sociological text data. We also discussed the limitations of word embeddings and the need for sentence embeddings to capture the meaning and context of entire sentences or longer text segments.\n",
       "\n",
       "We then ventured into the world of sentence embeddings, exploring techniques like averaging word embeddings, using transformer-based models like BERT, and leveraging the Universal Sentence Encoder. We saw how sentence embeddings enable more advanced and nuanced analysis of sociological text data, opening up new possibilities for tasks like text classification, clustering, and sentiment analysis.\n",
       "\n",
       "Throughout the chapter, we provided practical examples and code snippets to demonstrate how these techniques can be applied in sociological research. We showcased how to use BERT embeddings for text classification, cluster text data based on semantic similarity, and incorporate embeddings into machine learning workflows for tasks like sentiment analysis.\n",
       "\n",
       "As we conclude this chapter, we encourage you to explore and experiment with different embedding techniques in your own sociological research. Each technique has its strengths and limitations, and the choice of embedding approach depends on the specific requirements and characteristics of your text data.\n",
       "\n",
       "Don't be afraid to try out different pre-trained embeddings, fine-tune them on your specific domain, or create custom embeddings from scratch. Experiment with different preprocessing techniques, combine embeddings with other features, and evaluate their performance on various downstream tasks.\n",
       "\n",
       "The field of text representation is constantly evolving, with new embedding techniques and models emerging regularly. Stay curious, keep up with the latest advancements, and adapt them to your sociological research needs.\n",
       "\n",
       "Remember, turning words into numbers is just the beginning of the journey. The real power lies in the insights and knowledge you can uncover by leveraging these numerical representations to analyze and understand the rich tapestry of human language and social interactions.\n",
       "\n",
       "So go forth, intrepid computational sociologist, and unlock the secrets hidden within the vast expanse of text data. The possibilities are endless, and the insights you gain have the potential to shape our understanding of society and drive positive change.\n",
       "\n",
       "Happy embedding!\n",
       "\n",
       "Sure! Here's the \"Exercises and Further Reading\" section of the chapter, building on the previous sections:\n",
       "\n",
       "## Exercises and Further Reading\n",
       "\n",
       "To reinforce your understanding of the concepts and techniques covered in this chapter, here are some exercises you can practice:\n",
       "\n",
       "1. Calculate TF-IDF:\n",
       "   - Take a corpus of sociological text data related to a specific topic (e.g., social inequality, gender roles, or political polarization).\n",
       "   - Preprocess the text data by tokenizing, lowercasing, and removing stopwords.\n",
       "   - Calculate the TF-IDF scores for each word in the corpus using scikit-learn's `TfidfVectorizer`.\n",
       "   - Analyze the top-scoring words and discuss their relevance to the chosen sociological topic.\n",
       "\n",
       "2. Train a Word2Vec model:\n",
       "   - Collect a large corpus of text data relevant to your sociological research area.\n",
       "   - Preprocess the text data by tokenizing and lowercasing.\n",
       "   - Train a Word2Vec model using Gensim's `Word2Vec` class, experimenting with different hyperparameters (e.g., vector size, window size, minimum word frequency).\n",
       "   - Explore the trained model by finding similar words, performing word analogies, and visualizing word embeddings using techniques like t-SNE.\n",
       "\n",
       "3. Use BERT for sentence embeddings:\n",
       "   - Select a dataset of sentences or short texts related to a sociological topic of interest.\n",
       "   - Use the Hugging Face's Transformers library to load a pre-trained BERT model and tokenizer.\n",
       "   - Generate BERT embeddings for each sentence in the dataset.\n",
       "   - Apply clustering algorithms (e.g., K-means) to group semantically similar sentences based on their BERT embeddings.\n",
       "   - Analyze the resulting clusters and discuss the themes or patterns observed in each cluster.\n",
       "\n",
       "To further deepen your knowledge and explore advanced topics in text representation and computational social science, here are some recommended resources:\n",
       "\n",
       "- Books:\n",
       "  - \"Natural Language Processing with Python\" by Steven Bird, Ewan Klein, and Edward Loper\n",
       "  - \"Text Mining with R: A Tidy Approach\" by Julia Silge and David Robinson\n",
       "  - \"Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning\" by Benjamin Bengfort, Rebecca Bilbro, and Tony Ojeda\n",
       "\n",
       "- Academic Papers:\n",
       "  - \"Distributed Representations of Words and Phrases and their Compositionality\" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean (2013)\n",
       "  - \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova (2019)\n",
       "  - \"Universal Sentence Encoder\" by Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil (2018)\n",
       "\n",
       "- Online Tutorials and Courses:\n",
       "  - \"Natural Language Processing with Deep Learning\" course on Coursera by deeplearning.ai\n",
       "  - \"Natural Language Processing\" course on Coursera by National Research University Higher School of Economics\n",
       "  - \"Text Mining and Analytics\" course on Coursera by University of Illinois at Urbana-Champaign\n",
       "  - \"Computational Social Science\" course on Coursera by University of California, Davis\n",
       "\n",
       "These exercises and resources will help you gain hands-on experience with the techniques covered in this chapter and provide opportunities for further exploration and learning in the field of computational social science and text representation.\n",
       "\n",
       "Remember, the key to mastering these techniques is practice, experimentation, and continuous learning. Don't hesitate to dive deep into the recommended resources, explore new ideas, and apply them to your own sociological research projects.\n",
       "\n",
       "Happy learning and happy coding!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown('\\n\\n'.join(chapter_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
