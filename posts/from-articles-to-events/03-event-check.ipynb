{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: From Articles to Events, Part III\n",
    "date: 2024-03-11\n",
    "description: Using a LLM to categorize text\n",
    "categories: [OpenAI, Articles to Events]\n",
    "draft: false\n",
    "image: newspaper-protest.webp\n",
    "author-meta: Neal Caren, Associate Professor, Department of Sociology, University of North Carolina, Chapel Hill\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one post in a series where I'm working to expand the working paper \"[Extracting protest events from newspaper articles with ChatGPT](https://osf.io/dvht7)\" I wrote with Andy Andrews and Rashawn Ray. In that paper, we tested whether ChatGPT could replace my undergraduate RAs in extracting details about Black Lives Matter protests from media accounts. This time, I want to expand it to include more articles, movements, and variables.\n",
    "\n",
    "**Earlier Installments** \n",
    "* Part 1: [From Articles to Events](https://nealcaren.github.io/notes/posts/from-articles-to-events/01-downloading-articles.html)\n",
    "* Part 2: [Extracting text from media HTML files](https://nealcaren.github.io/notes/posts/from-articles-to-events/02-extracting-text.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I'm want to check whether the file I downloaded actually contains a media account of a protest that already happened. I'm hoping to filter out a few types of bad texts: files where I downloaded \"You can't see this page without paying.\" instead of the article; articles about future events; and pages that are organizational listings of events rather than media accounts. The plan is to use ChatGPT to categorize the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the article dataset made in [step 2]((https://nealcaren.github.io/notes/posts/from-articles-to-events/02-extracting-text.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('https://github.com/nealcaren/notes/raw/main/posts/from-articles-to-events/article_texts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>site</th>\n",
       "      <th>publisher</th>\n",
       "      <th>file_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>New Braunfels plans annual march, announces cl...</td>\n",
       "      <td>State Alabama Alaska Arizona Arkansas Californ...</td>\n",
       "      <td>https://herald-zeitung.com/community_alert/new...</td>\n",
       "      <td>[Hannah Thompson The Herald-Zeitung, Hannah Th...</td>\n",
       "      <td>None</td>\n",
       "      <td>In observance of Martin Luther King Jr. Day on...</td>\n",
       "      <td>New Braunfels Herald-Zeitung</td>\n",
       "      <td>{}</td>\n",
       "      <td>_HTML/https-herald-zeitung-com-community-alert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>Trump Supporters Plan Protest Outside Fulton C...</td>\n",
       "      <td>Laura Loomer, a staunch supporter of Donald Tr...</td>\n",
       "      <td>https://www.newsweek.com/trump-supporters-plan...</td>\n",
       "      <td>[Nick Mordowanec, Aron Solomon, Dan Perry, Pau...</td>\n",
       "      <td>2023-08-22T21:53:41.000Z</td>\n",
       "      <td>\"The American people recognize that this is a ...</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>{}</td>\n",
       "      <td>_HTML/https-www-newsweek-com-trump-supporters-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Protest held to terminate auditor accused of r...</td>\n",
       "      <td>HOWARD COUNTY, Md. — Members from different or...</td>\n",
       "      <td>https://www.wmar2news.com/local/protest-held-t...</td>\n",
       "      <td>[Ashley Mcdowell]</td>\n",
       "      <td>2023-03-07T03:19:29.618</td>\n",
       "      <td>Members from different organizations in Howard...</td>\n",
       "      <td>WMAR 2 News Baltimore</td>\n",
       "      <td>{}</td>\n",
       "      <td>_HTML/https-www-wmar2news-com-local-protest-he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2127  New Braunfels plans annual march, announces cl...   \n",
       "1651  Trump Supporters Plan Protest Outside Fulton C...   \n",
       "621   Protest held to terminate auditor accused of r...   \n",
       "\n",
       "                                                   text  \\\n",
       "2127  State Alabama Alaska Arizona Arkansas Californ...   \n",
       "1651  Laura Loomer, a staunch supporter of Donald Tr...   \n",
       "621   HOWARD COUNTY, Md. — Members from different or...   \n",
       "\n",
       "                                                    url  \\\n",
       "2127  https://herald-zeitung.com/community_alert/new...   \n",
       "1651  https://www.newsweek.com/trump-supporters-plan...   \n",
       "621   https://www.wmar2news.com/local/protest-held-t...   \n",
       "\n",
       "                                                authors  \\\n",
       "2127  [Hannah Thompson The Herald-Zeitung, Hannah Th...   \n",
       "1651  [Nick Mordowanec, Aron Solomon, Dan Perry, Pau...   \n",
       "621                                   [Ashley Mcdowell]   \n",
       "\n",
       "                          date  \\\n",
       "2127                      None   \n",
       "1651  2023-08-22T21:53:41.000Z   \n",
       "621    2023-03-07T03:19:29.618   \n",
       "\n",
       "                                            description  \\\n",
       "2127  In observance of Martin Luther King Jr. Day on...   \n",
       "1651  \"The American people recognize that this is a ...   \n",
       "621   Members from different organizations in Howard...   \n",
       "\n",
       "                              site publisher  \\\n",
       "2127  New Braunfels Herald-Zeitung        {}   \n",
       "1651                      Newsweek        {}   \n",
       "621          WMAR 2 News Baltimore        {}   \n",
       "\n",
       "                                          file_location  \n",
       "2127  _HTML/https-herald-zeitung-com-community-alert...  \n",
       "1651  _HTML/https-www-newsweek-com-trump-supporters-...  \n",
       "621   _HTML/https-www-wmar2news-com-local-protest-he...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than ask a single question, \"Is this an media article that describes a protest event that has already happened or is ongoing?\" I decided to ask it three separate questions. I *think* this strategy of breaking the question down into its component parts will lead to more accurate answers. \n",
    "\n",
    "To provide information about the questions and the format of a response (`True`/`False`), I use the `pydantic` library to create the structure of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleReview(BaseModel):\n",
    "    discusses_political_protest: bool = Field(\n",
    "        ...,\n",
    "        description=\"Indicates whether the article discusses a political protest. True if it does, False otherwise.\"\n",
    "    )\n",
    "    from_media_source: bool = Field(\n",
    "        ...,\n",
    "        description=\"Determines if the article is from a media source. Respond False if it is a press release or event listing.\"\n",
    "    )\n",
    "    protest_event_future: bool = Field(\n",
    "        ...,\n",
    "        description=\"Is the event planned for the future? Respond false if the event occurred or is currently happening.\"\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a Python function to call the OpenAI ChatGPT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_protest(article_info, client):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that extracts summaries of newspaper articles about political protests as JSON for a database. \",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Extract information about the details about a protest from the following article.\n",
    "      Only use information from the article.\n",
    "\n",
    "      {article_info}\n",
    "      \n",
    "      \"\"\",\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        functions=[\n",
    "            {\n",
    "                \"name\": \"protest_details\",\n",
    "                \"description\": \"Extract insights from media article about protest.\",\n",
    "                \"parameters\": ArticleReview.model_json_schema(),\n",
    "            }\n",
    "        ],\n",
    "        n=1,\n",
    "        messages=messages,\n",
    "    )\n",
    "    \n",
    "    r = json.loads(completion.choices[0].message.function_call.arguments)\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, I include the client as part of the function but this time I wanted to pass it to the function so it is only called once. That should speed things up a tiny bit. Plus, down the road I was thinking about using non-OpenAI models from [anyscale](https://www.anyscale.com) and this might make that easier to add on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    max_retries=3,\n",
    "    timeout=20.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab a sample article, including only the fields I want to process. I also truncate the text length to save $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Faces of protest: Thursday at the Indiana Statehouse',\n",
       " 'text': \"We are The Statehouse File\\n\\nFrom an office in the Press Corps of the Indiana Statehouse, the journalism majors of Franklin College's Pulliam School of Journalism work alongside the pros, digging into the behind-the-scenes stories of Indiana politics. We're a student newsroom, but our work doesn't sit on a professor's desk. We create daily content for this website and professional media outlets around the state.\\n\\nUSE OUR CONTENT FOR FREE: Thanks to a $180,000 grant from Lumina, TheStatehouseFile.com has taken down its reader paywall and is offering its year-round coverage of the Indiana Statehouse to professional media outlets to republish for free. Just retain the author's and The Statehouse File's name, helping our young journalists on their way.\",\n",
       " 'date': None,\n",
       " 'site': 'The Statehouse File'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful = ['title', 'text','date','site']\n",
    "a = df[useful].sample().to_dict(orient='records')[0]\n",
    "a['text'] = a['text'][:2000]\n",
    "a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying it out! I ran these surrounding cells a couple of times to make sure it works for different kinds or articles. Results seem good. A prior version used asked about `protest_event_past` but that seemed to miss a few, so I changed the question around in this version to ask only if the event was in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discusses_political_protest': False,\n",
       " 'from_media_source': True,\n",
       " 'protest_event_future': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_protest(a, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to apply the function to the whole dataframe. I had ChatGPT help me out here because I wanted a function that would (1) make multiple calls to the API at the same time to speed things up, and (2) be able to pick up where I left off in case I closed my laptop during the process. After some negotiation, the solution we agreed upon was one where it created a new feature `api_called` that would start as `false` but then be switched to `true` when the API was successfully called.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be executed by each thread\n",
    "def process_row(index, row):\n",
    "    useful = ['title', 'date','site','text_truncated']\n",
    "    if not row['api_called']:\n",
    "        text = row['text_truncated']\n",
    "        result = is_protest(row[useful], client)\n",
    "        return (index, result)\n",
    "    return (index, None)\n",
    "\n",
    "# Function to execute API calls in parallel and update DataFrame\n",
    "def update_dataframe(df):\n",
    "    # Select rows where API call hasn't been made\n",
    "    rows_to_process = df[~df['api_called']].copy()\n",
    "    \n",
    "    # Use ThreadPoolExecutor to parallelize API calls\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submit tasks\n",
    "        futures = {executor.submit(process_row, index, row): index for index, row in rows_to_process.iterrows()}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                index, api_result = result\n",
    "                # Update DataFrame with the result\n",
    "                for key, value in api_result.items():\n",
    "                    if key not in df.columns:\n",
    "                        df[key] = pd.NA  # Initialize new column with missing values\n",
    "                    df.at[index, key] = value\n",
    "                \n",
    "                # Mark the row as processed\n",
    "                df.at[index, 'api_called'] = True\n",
    "\n",
    "# Truncate 'text' to 2000 characters and initialize 'api_called' column\n",
    "df['text_truncated'] = df['text'].str[:2000]\n",
    "df['api_called'] = False\n",
    "\n",
    "# Update the DataFrame with API call results\n",
    "update_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took 9 minutes to process, which works about to be about .2 second for each record. Using `ThreadPoolExecutor` allowed me to make 5 API calls at a time, so if I hadn't used it, the process would have taken 45 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a new variable for the articles that match all the conditions that I want, and save those as a new JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_conditions_met'] = (\n",
    "    (df['discusses_political_protest'] == True) & \n",
    "    (df['from_media_source'] == True) & \n",
    "    (df['protest_event_future'] == False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4548115623856568"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['all_conditions_met'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1243"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['all_conditions_met'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = df['all_conditions_met']==True\n",
    "df[screen].to_json('protest_articles.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data prep is done! I have about the full text of 1,243 articles that are already coded by CCC for use in testing out the accuracy of LLMs for my use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
