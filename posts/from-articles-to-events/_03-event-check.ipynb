{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: From Articles to Events, PartÂ III\n",
    "date: 2024-02-29\n",
    "description: Using LLM to categorize text\n",
    "categories: [newspaper3k, Articles to Events]\n",
    "draft: false\n",
    "image: newspaper-protest.webp\n",
    "author-meta: Neal Caren, Associate Professor, Department of Sociology, University of North Carolina, Chapel Hill\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one post in a series where I'm working to expand the working paper \"[Extracting protest events from newspaper articles with ChatGPT](https://osf.io/dvht7)\" I wrote with Andy Andrews and Rashawn Ray. In that paper, we tested whether ChatGPT could replace my undergraduate RAs in extracting details about Black Lives Matter protests from media accounts. This time, I want to expand it to include more articles, movements, and variables.\n",
    "\n",
    "**Earlier Installments** \n",
    "* Part 1: [From Articles to Events](https://nealcaren.github.io/notes/posts/from-articles-to-events/01-downloading-articles.html)\n",
    "* \n",
    "\n",
    "In this part, I'm taking the downloaded HTML files and extracting the useful information, such as the article headline and text. Rather than build custom parsers for each webpage, I'm going to use the wonderful [Newspaper3k](https://newspaper.readthedocs.io/en/latest/) library to extract the relevant information from each article. It works on almost every media site, so it's incredibly useful for turning HTML into something useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (10.2.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (5.1.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (2.31.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (5.1.1)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from nltk>=3.2.1->newspaper3k) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/nealcaren/anaconda3/envs/code/lib/python3.10/site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import os\n",
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've used a variant of the function below for more than five years. The documentation for `newspaper` is a little vague on using the library when you already downloaded the HTML, but this version works. There can be more relevant stuff in the `meta_data` field, but it varies by paper and overtime, as they are primarily fields and values to improve the pages Google ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(file_location):\n",
    "    with open(file_location, 'rb') as fh:\n",
    "        html = fh.read()\n",
    "    article = Article(url = file_location)\n",
    "    article.set_html(html)\n",
    "    article.parse()\n",
    "    \n",
    "    article_details = {'title'       : article.title,\n",
    "                       'text'        : article.text,\n",
    "                       'url'         : article.meta_data['og'].get('url', article.url),\n",
    "                       'authors'     : article.authors,\n",
    "                       'date'        : article.publish_date,\n",
    "                       'description' : article.meta_description,\n",
    "                       'site'        : article.meta_data['og'].get('site_name', ''),\n",
    "                       'publisher'   : article.meta_data['publisher']}\n",
    "    \n",
    "\n",
    "    return article_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the `get_article_info` function to a single file is pretty straightforward, but it takes a few seconds, so, ideally, you only do it once per file. Things get more complicated when a few articles, but then add new HTML files to the folder. The set of functions below creates a dataframe to store the results, or loads one if it already exists, and then processes each of the files that we want and that we haven't already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_existing_data(json_file):\n",
    "    \"\"\"Load existing JSON data into a DataFrame.\"\"\"\n",
    "    try:\n",
    "        return pd.read_json(json_file)\n",
    "    except (ValueError, FileNotFoundError):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def is_file_processed(df, file_path):\n",
    "    \"\"\"Check if a file has been processed.\"\"\"\n",
    "    if 'file_location' in df.columns:\n",
    "        return df['file_location'].isin([file_path]).any()\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def update_dataframe(df, file_path):\n",
    "    \"\"\"Update the DataFrame with new article information.\"\"\"\n",
    "    article_info = get_article_info(file_path)  # Assuming this function exists and works as expected\n",
    "    article_info['file_location'] = file_path\n",
    "    \n",
    "    # Check if the DataFrame is empty and initialize columns if necessary\n",
    "    if df.empty:\n",
    "        for key in article_info.keys():\n",
    "            df[key] = pd.Series(dtype='object')\n",
    "    \n",
    "    # Add the new article information as a new row\n",
    "    new_row_index = len(df)\n",
    "    df.loc[new_row_index] = article_info\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_json(df, json_file):\n",
    "    \"\"\"Save the DataFrame to a JSON file.\"\"\"\n",
    "    df.to_json(json_file, orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "def process_files(folder_path, json_file, sources_json):\n",
    "    df = load_existing_data(json_file)\n",
    "    \n",
    "    # Filter the list of files to process based on 'source_1' column in sources_json\n",
    "    sources_df = pd.read_json(sources_json)\n",
    "    urls_to_process  = sources_df['source_1'].tolist()\n",
    "    files_to_process = [slugify(url)+ \".html\" for url in urls_to_process]\n",
    "            \n",
    "    for file in os.listdir(folder_path):\n",
    "        if file in files_to_process:  # Check if the file should be processed\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    "            if not is_file_processed(df, file_path):\n",
    "                try:\n",
    "                    df = update_dataframe(df, file_path)\n",
    "                except Exception as e:  # It's a good practice to catch specific exceptions\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    save_to_json(df, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_files('_HTML', \n",
    "              'article_texts.json', \n",
    "              'ccc_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>site</th>\n",
       "      <th>publisher</th>\n",
       "      <th>file_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>Rally urges Montana leaders to extend meal ass...</td>\n",
       "      <td>Earlier this year, Montana leaders announced t...</td>\n",
       "      <td>https://www.ktvh.com/news/rally-urges-montana-...</td>\n",
       "      <td>[Jonathon Ambarian]</td>\n",
       "      <td>2023-07-11T01:48:51.124</td>\n",
       "      <td>Earlier this year, Montana leaders announced t...</td>\n",
       "      <td>KTVH</td>\n",
       "      <td>{}</td>\n",
       "      <td>_HTML/https-www-ktvh-com-news-rally-urges-mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>NYC students protest gun violence after incide...</td>\n",
       "      <td>While the nation reels from tragedy this week ...</td>\n",
       "      <td>https://www.nydailynews.com/2023/03/30/nyc-stu...</td>\n",
       "      <td>[Cayla Bamberger]</td>\n",
       "      <td>2023-03-30T19:33:59.000Z</td>\n",
       "      <td>While the nation reels from tragedy this week ...</td>\n",
       "      <td>New York Daily News</td>\n",
       "      <td>{}</td>\n",
       "      <td>_HTML/https-www-nydailynews-com-new-york-educa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Mission Hospital nurses to hold rally today fo...</td>\n",
       "      <td>Press release from National Nurses United:\\n\\n...</td>\n",
       "      <td>http://mountainx.com/blogwire/mission-hospital...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Asheville and Western North Carolina News | Lo...</td>\n",
       "      <td>Mountain Xpress</td>\n",
       "      <td>{}</td>\n",
       "      <td>_HTML/https-mountainx-com-blogwire-mission-hos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "1338  Rally urges Montana leaders to extend meal ass...   \n",
       "306   NYC students protest gun violence after incide...   \n",
       "717   Mission Hospital nurses to hold rally today fo...   \n",
       "\n",
       "                                                   text  \\\n",
       "1338  Earlier this year, Montana leaders announced t...   \n",
       "306   While the nation reels from tragedy this week ...   \n",
       "717   Press release from National Nurses United:\\n\\n...   \n",
       "\n",
       "                                                    url              authors  \\\n",
       "1338  https://www.ktvh.com/news/rally-urges-montana-...  [Jonathon Ambarian]   \n",
       "306   https://www.nydailynews.com/2023/03/30/nyc-stu...    [Cayla Bamberger]   \n",
       "717   http://mountainx.com/blogwire/mission-hospital...                   []   \n",
       "\n",
       "                          date  \\\n",
       "1338   2023-07-11T01:48:51.124   \n",
       "306   2023-03-30T19:33:59.000Z   \n",
       "717                       None   \n",
       "\n",
       "                                            description                 site  \\\n",
       "1338  Earlier this year, Montana leaders announced t...                 KTVH   \n",
       "306   While the nation reels from tragedy this week ...  New York Daily News   \n",
       "717   Asheville and Western North Carolina News | Lo...      Mountain Xpress   \n",
       "\n",
       "     publisher                                      file_location  \n",
       "1338        {}  _HTML/https-www-ktvh-com-news-rally-urges-mont...  \n",
       "306         {}  _HTML/https-www-nydailynews-com-new-york-educa...  \n",
       "717         {}  _HTML/https-mountainx-com-blogwire-mission-hos...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_json('article_texts.json')\n",
    "print(len(text_df))\n",
    "text_df.sample(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good. Next up: Are they actually media accounts?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
